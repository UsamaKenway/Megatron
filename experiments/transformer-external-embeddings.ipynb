{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46efb73-49f9-47c0-bbb0-8b45cbd1254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:\\\\MachineLearning\\\\UsamaKenway\\\\DeepSeek-R1-OpenHermes-2.5\\\\datasets\\\\CoT_dataset\\\\dataset_0010.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e24d0b0-1635-40bd-baa9-4e10949e0042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 22)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e79793f5-c98c-4e9c-80ab-e66dab7228ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['instruct_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3122ea5-7961-4004-9e09-1eef1a0c2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75f46ee-72e2-464d-8205-b97d1dc33c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\xformers\\__init__.py\", line 57, in _is_triton_available\n",
      "    import triton  # noqa\n",
      "    ^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\__init__.py\", line 20, in <module>\n",
      "    from .runtime import (\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\runtime\\__init__.py\", line 1, in <module>\n",
      "    from .autotuner import (Autotuner, Config, Heuristics, autotune, heuristics)\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\runtime\\autotuner.py\", line 9, in <module>\n",
      "    from .jit import KernelInterface\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\runtime\\jit.py\", line 12, in <module>\n",
      "    from ..runtime.driver import driver\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\runtime\\driver.py\", line 1, in <module>\n",
      "    from ..backends import backends\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\backends\\__init__.py\", line 50, in <module>\n",
      "    backends = _discover_backends()\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\backends\\__init__.py\", line 43, in _discover_backends\n",
      "    compiler = _load_module(name, os.path.join(root, name, 'compiler.py'))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\backends\\__init__.py\", line 12, in _load_module\n",
      "    spec.loader.exec_module(module)\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\backends\\amd\\compiler.py\", line 2, in <module>\n",
      "    from triton._C.libtriton import ir, passes, llvm, amd\n",
      "ImportError: DLL load failed while importing libtriton: A dynamic link library (DLL) initialization routine failed.\n",
      "Some weights of the model checkpoint at NovaSearch/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model's dimensions: 1024\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "# Load LLaMA tokenizer and MiniLM embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "embedding_model =  SentenceTransformer(\"NovaSearch/stella_en_400M_v5\", trust_remote_code=True, device='cuda').cuda() #= SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"Embedding model's dimensions: {len(embedding_model.encode('ok'))}\")\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    return torch.tensor(embedding_model.encode(texts), dtype=torch.float32)\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Multi-Head Self-Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv_proj(x).reshape(B, T, 3, self.num_heads, self.d_k).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = (attn_weights @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "# Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "        \n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, max_len=100):\n",
    "        self.texts = df['instruct_prompt'].tolist()\n",
    "        # Reshape embeddings to match expected dimensions\n",
    "        self.embeddings = [\n",
    "            torch.nn.functional.pad(\n",
    "                get_embeddings(text).unsqueeze(0), \n",
    "                (0, 0, 0, max_len - 1)\n",
    "            ) if len(get_embeddings(text).unsqueeze(0)) < max_len \n",
    "            else get_embeddings(text).unsqueeze(0)[:max_len]\n",
    "            for text in self.texts\n",
    "        ]\n",
    "        self.tokenized = [\n",
    "            tokenizer(\n",
    "                text, \n",
    "                return_tensors='pt', \n",
    "                padding=\"max_length\", \n",
    "                truncation=True, \n",
    "                max_length=max_len\n",
    "            )[\"input_ids\"].squeeze(0) \n",
    "            for text in self.texts\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.tokenized[idx]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim, num_layers, vocab_size, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_model, num_heads, hidden_dim) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        # Add input projection layer to handle single token embeddings\n",
    "        self.input_projection = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure input has correct shape [batch_size, seq_len, d_model]\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # Project input\n",
    "        x = self.input_projection(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            # x = layer(x)\n",
    "            x = checkpoint.checkpoint(layer, x)  # Apply gradient checkpointing per layer\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def decode(self, logits):\n",
    "        token_ids = torch.argmax(logits, dim=-1)\n",
    "        return [tokenizer.decode(ids, skip_special_tokens=True) for ids in token_ids]\n",
    "\n",
    "# Training setup\n",
    "def train_model(model, train_loader, num_epochs, device, vocab_size):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Reshape outputs and targets for loss calculation\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Add gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffecf439-d949-4c4a-8367-bdbfdeb08bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 37 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.20521261, -0.08252388, -2.8384926 , ..., -1.2719604 ,\n",
       "        0.5373522 , -0.71815664], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "embedding_model.encode(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b50b33df-81b2-44bc-a550-e5da67f617b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, start_epoch, num_epochs, device, vocab_size):\n",
    "    \"\"\"\n",
    "    Memory-optimized training function using Adafactor and gradient accumulation\n",
    "    \"\"\"\n",
    "    # Import Adafactor from transformers\n",
    "    from transformers.optimization import Adafactor\n",
    "    \n",
    "    # Initialize Adafactor optimizer with memory-efficient settings\n",
    "    optimizer = Adafactor(\n",
    "        model.parameters(),\n",
    "        eps=(1e-30, 1e-3),\n",
    "        clip_threshold=1.0,\n",
    "        decay_rate=-0.8,\n",
    "        beta1=None,\n",
    "        weight_decay=0.01,\n",
    "        scale_parameter=True,\n",
    "        relative_step=True,  # Use relative step sizing\n",
    "        warmup_init=True     # Enable warmup\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Gradient accumulation steps\n",
    "    gradient_accumulation_steps = 4\n",
    "    \n",
    "    # Enable gradient checkpointing for memory efficiency\n",
    "    # model.gradient_checkpointing_enable() # im not using it because of error\n",
    "    \n",
    "    # Initialize mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            # Move to device and handle data types\n",
    "            inputs = inputs.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Use automatic mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "                targets = targets.view(-1)\n",
    "                \n",
    "                # Scale loss by gradient accumulation steps\n",
    "                loss = criterion(outputs, targets) / gradient_accumulation_steps\n",
    "            \n",
    "            # Scale gradients and backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Update weights if we've accumulated enough gradients\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                # Unscale gradients for clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Optimizer step with scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Track loss (multiply by accumulation steps to get true loss)\n",
    "            total_loss += loss.item() * gradient_accumulation_steps\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 50 == 0:\n",
    "                avg_loss = total_loss / (batch_idx + 1)\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, \"\n",
    "                      f\"Loss: {loss.item() * gradient_accumulation_steps:.4f}, \"\n",
    "                      f\"Avg Loss: {avg_loss:.4f}\")\n",
    "                \n",
    "                # # Print memory usage if on cuda\n",
    "                # if device.type == 'cuda':\n",
    "                #     print(f\"GPU Memory: \"\n",
    "                #           f\"{torch.cuda.memory_allocated(device) / 1024**2:.1f}MB / \"\n",
    "                #           f\"{torch.cuda.memory_reserved(device) / 1024**2:.1f}MB\")\n",
    "        \n",
    "        # Compute average loss for the epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Optional: Save checkpoint after each epoch\n",
    "        torch.save(model, 'model.pth')\n",
    "        # torch.save({\n",
    "        #     'epoch': epoch,\n",
    "        #     'model_state_dict': model.state_dict(),\n",
    "        #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #     'loss': avg_loss,\n",
    "        # }, f'checkpoint.pt') #f'checkpoint_epoch_{epoch+1}.pt'\n",
    "\n",
    "# Memory optimization settings before training\n",
    "def optimize_memory():\n",
    "    \"\"\"Apply memory optimizations before training\"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    \n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Set memory allocator settings\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_per_process_memory_fraction(0.9)  # Use 90% of available memory\n",
    "        torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac689aaf-2a38-41d3-acf9-b0d24b9a20d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.encode(\"g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dbdf3b6-a23e-4807-ae83-180a2ee0347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, max_len=100):\n",
    "        self.texts = df['instruct_prompt'].tolist()\n",
    "        \n",
    "        # Track progress with tqdm\n",
    "        self.embeddings = []\n",
    "        self.tokenized = []\n",
    "        \n",
    "        for text in tqdm(self.texts, desc=\"Processing Texts\", leave=False):\n",
    "            try:\n",
    "                # Handle embeddings\n",
    "                embedding = get_embeddings(text)\n",
    "                if isinstance(embedding, torch.Tensor):  # Ensure it's a tensor\n",
    "                    embedding = torch.nn.functional.pad(\n",
    "                        embedding.unsqueeze(0), \n",
    "                        (0, 0, 0, max_len - 1)\n",
    "                    ) if len(embedding.unsqueeze(0)) < max_len else embedding.unsqueeze(0)[:max_len]\n",
    "                    self.embeddings.append(embedding)\n",
    "                else:\n",
    "                    raise ValueError(f\"Embedding not tensor for text: {text}\")\n",
    "                \n",
    "                # Handle tokenization\n",
    "                tokenized = tokenizer(\n",
    "                    text, \n",
    "                    return_tensors='pt', \n",
    "                    padding=\"max_length\", \n",
    "                    truncation=True, \n",
    "                    max_length=max_len\n",
    "                )[\"input_ids\"].squeeze(0)\n",
    "                self.tokenized.append(tokenized)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping row due to error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.tokenized[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5fd1eff-a5e5-4a65-be61-a8e7246c940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path, device):\n",
    "    \"\"\"Load model and optimizer states from a checkpoint\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
    "    print(f\"Checkpoint loaded! Resuming from epoch {start_epoch}\")\n",
    "    return model, optimizer, start_epoch\n",
    "\n",
    "def train_model(model, train_loader, num_epochs, device, vocab_size, checkpoint_path=None):\n",
    "    from transformers.optimization import Adafactor\n",
    "    import torch.nn as nn\n",
    "\n",
    "    optimizer = Adafactor(\n",
    "        model.parameters(),\n",
    "        eps=(1e-30, 1e-3),\n",
    "        clip_threshold=1.0,\n",
    "        decay_rate=-0.8,\n",
    "        beta1=None,\n",
    "        weight_decay=0.01,\n",
    "        scale_parameter=True,\n",
    "        relative_step=True,\n",
    "        warmup_init=True\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    gradient_accumulation_steps = 4\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    # Resume training if checkpoint exists\n",
    "    start_epoch = 0\n",
    "    if checkpoint_path:\n",
    "        model, optimizer, start_epoch = load_checkpoint(model, optimizer, checkpoint_path, device)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "\n",
    "            # with torch.cuda.amp.autocast():\n",
    "            #     outputs = model(inputs).view(-1, vocab_size)\n",
    "            #     targets = targets.view(-1)\n",
    "            #     loss = criterion(outputs, targets) / gradient_accumulation_steps\n",
    "            with torch.amp.autocast('cuda'):  # ✅ Fixed autocast\n",
    "                outputs = model(inputs).view(-1, vocab_size)\n",
    "                targets = targets.view(-1)\n",
    "                loss = criterion(outputs, targets) / 4  # Gradient accumulation steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        torch.save(model, 'model.pth')\n",
    "        # # Save checkpoint\n",
    "        # torch.save({\n",
    "        #     'epoch': epoch,\n",
    "        #     'model_state_dict': model.state_dict(),\n",
    "        #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #     'loss': avg_loss,\n",
    "        # }, 'checkpoint.pt')\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37421559-a284-4e13-9d2d-d3d61a8abaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and training\n",
    "# d_model = 384  # MiniLM embedding size\n",
    "# num_heads = 8\n",
    "# hidden_dim = 1536\n",
    "# num_layers = 6\n",
    "# vocab_size = tokenizer.vocab_size\n",
    "# max_len = 500\n",
    "\n",
    "# d_model=1024 #len(embedding_model.encode(\"g\"))\n",
    "# num_heads=32\n",
    "# hidden_dim=4096 #16384\n",
    "# num_layers=24\n",
    "vocab_size=tokenizer.vocab_size\n",
    "max_len = 1000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "d_model = 1024\n",
    "num_heads = 32\n",
    "num_layers = 24\n",
    "hidden_dim = 12288 # hidden_dim = 16384\n",
    "\n",
    "\n",
    "model = Transformer(d_model, num_heads, hidden_dim, num_layers, vocab_size, max_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "981f4160-c716-472c-b1d0-e282b090821a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69378112-7954-4ea0-b604-fd34cf237a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('model.pth', weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa81b48d-f40e-46a1-a1cf-75c5779157dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.optimization import Adafactor\n",
    "\n",
    "# optimizer = Adafactor(\n",
    "#         model.parameters(),\n",
    "#         eps=(1e-30, 1e-3),\n",
    "#         clip_threshold=1.0,\n",
    "#         decay_rate=-0.8,\n",
    "#         beta1=None,\n",
    "#         weight_decay=0.01,\n",
    "#         scale_parameter=True,\n",
    "#         relative_step=True,  # Use relative step sizing\n",
    "#         warmup_init=True     # Enable warmup\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8eb6023-8d84-4160-8db1-adbe735c0b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 2 with loss 6.3275\n"
     ]
    }
   ],
   "source": [
    "# checkpoint = torch.load('checkpoint.pt')\n",
    "\n",
    "# # Restore model and optimizer states\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# # Restore epoch and loss\n",
    "# start_epoch = checkpoint['epoch'] + 1  # Continue from the next epoch\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# print(f\"Resuming training from epoch {start_epoch} with loss {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5a0ebd1-30b5-40bf-9d13-6790e80de4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d00da75-72ad-4423-97b2-a7c590c11864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(df, max_len=max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60a28dfd-6027-4a67-9740-fb1524ec5401",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Save train_dataset to a file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH:/GAMES/train_dataset.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save train_dataset to a file\n",
    "# with open(\"H:/GAMES/train_dataset.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(train_dataset, f)\n",
    "\n",
    "# print(\"Dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90b8d8bb-8ac0-4918-9922-482b9da3fd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shallow size of train_dataset: 0.00 MB\n",
      "Deep size of train_dataset: 59.96 MB\n",
      "Shallow size of train_loader: 0.00 MB\n",
      "Deep size of train_loader: 59.97 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from pympler import asizeof\n",
    "\n",
    "# Check memory size of train_dataset\n",
    "train_dataset_size = sys.getsizeof(train_dataset)  # Shallow size\n",
    "train_dataset_deep_size = asizeof.asizeof(train_dataset)  # Deep size\n",
    "\n",
    "# Check memory size of train_loader\n",
    "train_loader_size = sys.getsizeof(train_loader)  # Shallow size\n",
    "train_loader_deep_size = asizeof.asizeof(train_loader)  # Deep size\n",
    "\n",
    "print(f\"Shallow size of train_dataset: {train_dataset_size / (1024**2):.2f} MB\")\n",
    "print(f\"Deep size of train_dataset: {train_dataset_deep_size / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"Shallow size of train_loader: {train_loader_size / (1024**2):.2f} MB\")\n",
    "print(f\"Deep size of train_loader: {train_loader_deep_size / (1024**2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e58ac09-e359-4c37-bb73-d7a4106114a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b881914-e4d1-419f-9a3d-a62e861b5bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_9048\\1510607220.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_9048\\1510607220.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 10.8133, Avg Loss: 10.8133\n",
      "GPU Memory: 7976.2MB / 14536.0MB\n",
      "Epoch 1, Batch 50, Loss: 10.7034, Avg Loss: 10.7819\n",
      "GPU Memory: 7982.7MB / 15764.0MB\n",
      "Epoch 1, Batch 100, Loss: 10.3118, Avg Loss: 10.6597\n",
      "GPU Memory: 7983.7MB / 15764.0MB\n",
      "Epoch 1, Batch 150, Loss: 9.8308, Avg Loss: 10.4764\n",
      "GPU Memory: 7982.7MB / 15764.0MB\n",
      "Epoch 1, Batch 200, Loss: 9.4193, Avg Loss: 10.2557\n",
      "GPU Memory: 7983.7MB / 15764.0MB\n",
      "Epoch 1, Batch 250, Loss: 9.0225, Avg Loss: 10.0444\n",
      "GPU Memory: 7982.7MB / 15764.0MB\n",
      "Epoch 1, Batch 300, Loss: 8.6639, Avg Loss: 9.8520\n",
      "GPU Memory: 7983.7MB / 15764.0MB\n",
      "Epoch 1, Batch 350, Loss: 8.5891, Avg Loss: 9.6850\n",
      "GPU Memory: 7982.7MB / 15764.0MB\n",
      "Epoch 1, Batch 400, Loss: 8.4636, Avg Loss: 9.5427\n",
      "GPU Memory: 7983.7MB / 15764.0MB\n",
      "Epoch 1, Batch 450, Loss: 8.3328, Avg Loss: 9.4146\n",
      "GPU Memory: 7982.7MB / 15764.0MB\n",
      "Epoch 1, Average Loss: 9.2995\n",
      "Epoch 2, Batch 0, Loss: 8.1870, Avg Loss: 8.1870\n",
      "GPU Memory: 7983.7MB / 15764.0MB\n",
      "Epoch 2, Batch 50, Loss: 8.0836, Avg Loss: 8.0810\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 2, Batch 100, Loss: 7.8422, Avg Loss: 8.0079\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 2, Batch 150, Loss: 7.7717, Avg Loss: 7.9355\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 2, Batch 200, Loss: 7.6720, Avg Loss: 7.8677\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 2, Batch 250, Loss: 7.3837, Avg Loss: 7.8023\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 2, Batch 300, Loss: 7.3974, Avg Loss: 7.7343\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 2, Batch 350, Loss: 7.1971, Avg Loss: 7.6688\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 2, Batch 400, Loss: 6.9778, Avg Loss: 7.6041\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 2, Batch 450, Loss: 7.0342, Avg Loss: 7.5460\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 2, Average Loss: 7.4860\n",
      "Epoch 3, Batch 0, Loss: 6.9850, Avg Loss: 6.9850\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 3, Batch 50, Loss: 6.7653, Avg Loss: 6.8348\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 3, Batch 100, Loss: 6.7814, Avg Loss: 6.8022\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 3, Batch 150, Loss: 6.6822, Avg Loss: 6.7691\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 3, Batch 200, Loss: 6.5064, Avg Loss: 6.7311\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 3, Batch 250, Loss: 6.5967, Avg Loss: 6.6949\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 3, Batch 300, Loss: 6.5693, Avg Loss: 6.6627\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 3, Batch 350, Loss: 6.5303, Avg Loss: 6.6360\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 3, Batch 400, Loss: 6.4012, Avg Loss: 6.6095\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 3, Batch 450, Loss: 6.3691, Avg Loss: 6.5876\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 3, Average Loss: 6.5680\n",
      "Epoch 4, Batch 0, Loss: 6.3142, Avg Loss: 6.3142\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 4, Batch 50, Loss: 6.4466, Avg Loss: 6.3450\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 4, Batch 100, Loss: 6.3271, Avg Loss: 6.3404\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 4, Batch 150, Loss: 6.2526, Avg Loss: 6.3339\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 4, Batch 200, Loss: 6.2521, Avg Loss: 6.3234\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 4, Batch 250, Loss: 6.2589, Avg Loss: 6.3152\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 4, Batch 300, Loss: 6.4225, Avg Loss: 6.3105\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 4, Batch 350, Loss: 6.4250, Avg Loss: 6.3094\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 4, Batch 400, Loss: 6.3845, Avg Loss: 6.3070\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 4, Batch 450, Loss: 6.3039, Avg Loss: 6.3031\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 4, Average Loss: 6.3003\n",
      "Epoch 5, Batch 0, Loss: 6.4272, Avg Loss: 6.4272\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 5, Batch 50, Loss: 6.1792, Avg Loss: 6.2665\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 5, Batch 100, Loss: 6.2280, Avg Loss: 6.2656\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 5, Batch 150, Loss: 6.1587, Avg Loss: 6.2643\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 5, Batch 200, Loss: 6.2036, Avg Loss: 6.2583\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 5, Batch 250, Loss: 6.1700, Avg Loss: 6.2593\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 5, Batch 300, Loss: 6.3998, Avg Loss: 6.2575\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 5, Batch 350, Loss: 6.2618, Avg Loss: 6.2565\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 5, Batch 400, Loss: 6.1358, Avg Loss: 6.2554\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 5, Batch 450, Loss: 6.2730, Avg Loss: 6.2557\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 5, Average Loss: 6.2557\n",
      "Epoch 6, Batch 0, Loss: 6.1414, Avg Loss: 6.1414\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 6, Batch 50, Loss: 6.1956, Avg Loss: 6.2481\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 6, Batch 100, Loss: 6.3602, Avg Loss: 6.2465\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 6, Batch 150, Loss: 6.2794, Avg Loss: 6.2397\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 6, Batch 200, Loss: 6.1913, Avg Loss: 6.2361\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 6, Batch 250, Loss: 6.4317, Avg Loss: 6.2341\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 6, Batch 300, Loss: 6.1568, Avg Loss: 6.2383\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 6, Batch 350, Loss: 6.3719, Avg Loss: 6.2375\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 6, Batch 400, Loss: 5.9312, Avg Loss: 6.2380\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 6, Batch 450, Loss: 6.2959, Avg Loss: 6.2377\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 6, Average Loss: 6.2404\n",
      "Epoch 7, Batch 0, Loss: 6.2685, Avg Loss: 6.2685\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 7, Batch 50, Loss: 6.3484, Avg Loss: 6.2341\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 7, Batch 100, Loss: 6.1438, Avg Loss: 6.2352\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 7, Batch 150, Loss: 6.3332, Avg Loss: 6.2289\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 7, Batch 200, Loss: 6.2065, Avg Loss: 6.2283\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 7, Batch 250, Loss: 6.1184, Avg Loss: 6.2292\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 7, Batch 300, Loss: 6.2679, Avg Loss: 6.2253\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 7, Batch 350, Loss: 6.1670, Avg Loss: 6.2278\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 7, Batch 400, Loss: 6.3086, Avg Loss: 6.2271\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 7, Batch 450, Loss: 6.2232, Avg Loss: 6.2261\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 7, Average Loss: 6.2255\n",
      "Epoch 8, Batch 0, Loss: 6.2179, Avg Loss: 6.2179\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 8, Batch 50, Loss: 6.1766, Avg Loss: 6.2194\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 8, Batch 100, Loss: 6.1112, Avg Loss: 6.2183\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 8, Batch 150, Loss: 6.2246, Avg Loss: 6.2187\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 8, Batch 200, Loss: 6.1139, Avg Loss: 6.2193\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 8, Batch 250, Loss: 6.1409, Avg Loss: 6.2188\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 8, Batch 300, Loss: 6.1286, Avg Loss: 6.2160\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 8, Batch 350, Loss: 6.2867, Avg Loss: 6.2120\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 8, Batch 400, Loss: 6.2024, Avg Loss: 6.2110\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 8, Batch 450, Loss: 6.3076, Avg Loss: 6.2113\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 8, Average Loss: 6.2107\n",
      "Epoch 9, Batch 0, Loss: 6.1715, Avg Loss: 6.1715\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 9, Batch 50, Loss: 6.0247, Avg Loss: 6.1886\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 9, Batch 100, Loss: 6.2392, Avg Loss: 6.1941\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 9, Batch 150, Loss: 6.0555, Avg Loss: 6.1978\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 9, Batch 200, Loss: 6.0877, Avg Loss: 6.1978\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 9, Batch 250, Loss: 6.2149, Avg Loss: 6.1980\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 9, Batch 300, Loss: 6.2446, Avg Loss: 6.1993\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 9, Batch 350, Loss: 6.1458, Avg Loss: 6.1988\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 9, Batch 400, Loss: 6.2158, Avg Loss: 6.1996\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 9, Batch 450, Loss: 6.2026, Avg Loss: 6.1971\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 9, Average Loss: 6.1977\n",
      "Epoch 10, Batch 0, Loss: 6.0364, Avg Loss: 6.0364\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 10, Batch 50, Loss: 6.1221, Avg Loss: 6.1785\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 10, Batch 100, Loss: 6.0934, Avg Loss: 6.1575\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 10, Batch 150, Loss: 6.1332, Avg Loss: 6.1343\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 10, Batch 200, Loss: 6.1302, Avg Loss: 6.1212\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 10, Batch 250, Loss: 6.0246, Avg Loss: 6.1087\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 10, Batch 300, Loss: 5.9390, Avg Loss: 6.0956\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 10, Batch 350, Loss: 5.9909, Avg Loss: 6.0867\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 10, Batch 400, Loss: 5.9948, Avg Loss: 6.0780\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 10, Batch 450, Loss: 5.9300, Avg Loss: 6.0705\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 10, Average Loss: 6.0617\n",
      "Epoch 11, Batch 0, Loss: 6.2726, Avg Loss: 6.2726\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 11, Batch 50, Loss: 5.9006, Avg Loss: 5.9674\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 11, Batch 100, Loss: 5.9417, Avg Loss: 5.9653\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 11, Batch 150, Loss: 6.0109, Avg Loss: 5.9597\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 11, Batch 200, Loss: 5.9449, Avg Loss: 5.9567\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 11, Batch 250, Loss: 5.8051, Avg Loss: 5.9496\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 11, Batch 300, Loss: 5.8285, Avg Loss: 5.9460\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 11, Batch 350, Loss: 5.9601, Avg Loss: 5.9387\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 11, Batch 400, Loss: 5.9423, Avg Loss: 5.9330\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 11, Batch 450, Loss: 6.0341, Avg Loss: 5.9247\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 11, Average Loss: 5.9167\n",
      "Epoch 12, Batch 0, Loss: 5.7798, Avg Loss: 5.7798\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 12, Batch 50, Loss: 5.7524, Avg Loss: 5.8556\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n",
      "Epoch 12, Batch 100, Loss: 5.6733, Avg Loss: 5.8414\n",
      "GPU Memory: 7984.4MB / 15764.0MB\n",
      "Epoch 12, Batch 150, Loss: 5.8674, Avg Loss: 5.8340\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n",
      "Epoch 12, Batch 200, Loss: 5.6653, Avg Loss: 5.8246\n",
      "GPU Memory: 7984.4MB / 15764.0MB\n",
      "Epoch 12, Batch 250, Loss: 5.8781, Avg Loss: 5.8159\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n",
      "Epoch 12, Batch 300, Loss: 5.8356, Avg Loss: 5.8109\n",
      "GPU Memory: 7984.4MB / 15764.0MB\n",
      "Epoch 12, Batch 350, Loss: 5.7877, Avg Loss: 5.8061\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n",
      "Epoch 12, Batch 400, Loss: 5.8241, Avg Loss: 5.8004\n",
      "GPU Memory: 7984.4MB / 15764.0MB\n",
      "Epoch 12, Batch 450, Loss: 5.6495, Avg Loss: 5.7975\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 12, Average Loss: 5.8001\n",
      "Epoch 13, Batch 0, Loss: 5.8076, Avg Loss: 5.8076\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 13, Batch 50, Loss: 5.8578, Avg Loss: 5.7404\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 13, Batch 100, Loss: 5.7000, Avg Loss: 5.7315\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 13, Batch 150, Loss: 5.7346, Avg Loss: 5.7269\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 13, Batch 200, Loss: 5.4898, Avg Loss: 5.7226\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 13, Batch 250, Loss: 5.6800, Avg Loss: 5.7151\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 13, Batch 300, Loss: 5.6506, Avg Loss: 5.7131\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 13, Batch 350, Loss: 5.7820, Avg Loss: 5.7141\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 13, Batch 400, Loss: 5.5647, Avg Loss: 5.7128\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 13, Batch 450, Loss: 5.7461, Avg Loss: 5.7104\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 13, Average Loss: 5.7075\n",
      "Epoch 14, Batch 0, Loss: 5.6918, Avg Loss: 5.6918\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 14, Batch 50, Loss: 5.7018, Avg Loss: 5.6319\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 14, Batch 100, Loss: 5.7101, Avg Loss: 5.6301\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 14, Batch 150, Loss: 5.6828, Avg Loss: 5.6311\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 14, Batch 200, Loss: 5.5509, Avg Loss: 5.6362\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 14, Batch 250, Loss: 5.6267, Avg Loss: 5.6387\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 14, Batch 300, Loss: 5.5145, Avg Loss: 5.6361\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 14, Batch 350, Loss: 5.7010, Avg Loss: 5.6335\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 14, Batch 400, Loss: 5.4023, Avg Loss: 5.6346\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 14, Batch 450, Loss: 5.5767, Avg Loss: 5.6371\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 14, Average Loss: 5.6365\n",
      "Epoch 15, Batch 0, Loss: 5.5824, Avg Loss: 5.5824\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 15, Batch 50, Loss: 5.4826, Avg Loss: 5.5746\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n",
      "Epoch 15, Batch 100, Loss: 5.6394, Avg Loss: 5.5883\n",
      "GPU Memory: 7984.4MB / 15764.0MB\n",
      "Epoch 15, Batch 150, Loss: 5.7604, Avg Loss: 5.5943\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n",
      "Epoch 15, Batch 200, Loss: 5.6762, Avg Loss: 5.5920\n",
      "GPU Memory: 7984.4MB / 15764.0MB\n",
      "Epoch 15, Batch 250, Loss: 5.6383, Avg Loss: 5.5947\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# train_dataset = TextDataset(df, max_len=max_len)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 57\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, num_epochs, device, vocab_size)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Update weights if we've accumulated enough gradients\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m gradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Unscale gradients for clipping\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Clip gradients\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    335\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    336\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 338\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:279\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[1;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m device, per_dtype_grads \u001b[38;5;129;01min\u001b[39;00m per_device_and_dtype_grads\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m grads \u001b[38;5;129;01min\u001b[39;00m per_dtype_grads\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 279\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_amp_foreach_non_finite_check_and_unscale_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m                \u001b[49m\u001b[43mper_device_found_inf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m                \u001b[49m\u001b[43mper_device_inv_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m per_device_found_inf\u001b[38;5;241m.\u001b[39m_per_device_tensors\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader\n",
    "# train_dataset = TextDataset(df, max_len=max_len)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "# Train the model\n",
    "train_model(model, train_loader, num_epochs=40, device=device, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc493d88-4fb4-48a1-a484-129a5c858677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_9048\\922082617.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_9048\\922082617.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 5.6266, Avg Loss: 5.6266\n",
      "GPU Memory: 8631.4MB / 14306.0MB\n",
      "Epoch 1, Batch 50, Loss: 5.4901, Avg Loss: 5.5854\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 100, Loss: 5.5246, Avg Loss: 5.5633\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 150, Loss: 5.4203, Avg Loss: 5.5403\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 200, Loss: 5.3766, Avg Loss: 5.5251\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 250, Loss: 5.3213, Avg Loss: 5.5017\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 300, Loss: 5.3774, Avg Loss: 5.4935\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 350, Loss: 5.5819, Avg Loss: 5.4801\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 400, Loss: 5.5981, Avg Loss: 5.4730\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 450, Loss: 5.2984, Avg Loss: 5.4638\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Average Loss: 5.4629\n",
      "Epoch 2, Batch 0, Loss: 5.4481, Avg Loss: 5.4481\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 50, Loss: 5.5456, Avg Loss: 5.4185\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 100, Loss: 5.3703, Avg Loss: 5.4079\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 150, Loss: 5.5159, Avg Loss: 5.4088\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 200, Loss: 5.5335, Avg Loss: 5.4099\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 250, Loss: 5.4523, Avg Loss: 5.4076\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 300, Loss: 5.4603, Avg Loss: 5.4073\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 350, Loss: 5.4059, Avg Loss: 5.4081\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 400, Loss: 5.1680, Avg Loss: 5.4086\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 450, Loss: 5.4744, Avg Loss: 5.4077\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Average Loss: 5.4050\n",
      "Epoch 3, Batch 0, Loss: 5.3973, Avg Loss: 5.3973\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 50, Loss: 5.5505, Avg Loss: 5.4007\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 100, Loss: 5.2773, Avg Loss: 5.3974\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 150, Loss: 5.5380, Avg Loss: 5.4013\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 200, Loss: 5.5756, Avg Loss: 5.3994\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 250, Loss: 5.4235, Avg Loss: 5.3954\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 300, Loss: 5.4519, Avg Loss: 5.3991\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 350, Loss: 5.4635, Avg Loss: 5.3975\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 400, Loss: 5.3888, Avg Loss: 5.3930\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 450, Loss: 5.3555, Avg Loss: 5.3933\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Average Loss: 5.3926\n",
      "Epoch 4, Batch 0, Loss: 5.4521, Avg Loss: 5.4521\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 50, Loss: 5.2658, Avg Loss: 5.3576\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 100, Loss: 5.3147, Avg Loss: 5.3586\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 150, Loss: 5.2634, Avg Loss: 5.3644\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 200, Loss: 5.2924, Avg Loss: 5.3694\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 250, Loss: 5.4334, Avg Loss: 5.3728\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 300, Loss: 5.3436, Avg Loss: 5.3769\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 350, Loss: 5.5299, Avg Loss: 5.3773\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 400, Loss: 5.2929, Avg Loss: 5.3794\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 450, Loss: 5.3592, Avg Loss: 5.3800\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Average Loss: 5.3803\n",
      "Epoch 5, Batch 0, Loss: 5.4729, Avg Loss: 5.4729\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 50, Loss: 5.5406, Avg Loss: 5.3881\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 100, Loss: 5.3339, Avg Loss: 5.3668\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 150, Loss: 5.3596, Avg Loss: 5.3627\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 200, Loss: 5.4396, Avg Loss: 5.3700\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 250, Loss: 5.3595, Avg Loss: 5.3713\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 300, Loss: 5.2610, Avg Loss: 5.3675\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 350, Loss: 5.3728, Avg Loss: 5.3688\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 400, Loss: 5.1871, Avg Loss: 5.3665\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 450, Loss: 5.3942, Avg Loss: 5.3660\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Average Loss: 5.3678\n",
      "Epoch 6, Batch 0, Loss: 5.2546, Avg Loss: 5.2546\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 50, Loss: 5.2336, Avg Loss: 5.3736\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 100, Loss: 5.2440, Avg Loss: 5.3587\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 150, Loss: 5.3226, Avg Loss: 5.3466\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 200, Loss: 5.2809, Avg Loss: 5.3506\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 250, Loss: 5.4122, Avg Loss: 5.3559\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 300, Loss: 5.5601, Avg Loss: 5.3542\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 350, Loss: 5.3310, Avg Loss: 5.3520\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 400, Loss: 5.6167, Avg Loss: 5.3562\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 450, Loss: 5.5078, Avg Loss: 5.3575\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Average Loss: 5.3563\n",
      "Epoch 7, Batch 0, Loss: 5.3190, Avg Loss: 5.3190\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 50, Loss: 5.3303, Avg Loss: 5.3266\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 100, Loss: 5.3954, Avg Loss: 5.3289\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 150, Loss: 5.4229, Avg Loss: 5.3409\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 200, Loss: 5.3518, Avg Loss: 5.3388\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 250, Loss: 5.2522, Avg Loss: 5.3491\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 300, Loss: 5.3326, Avg Loss: 5.3491\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 350, Loss: 5.3504, Avg Loss: 5.3521\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 400, Loss: 5.3784, Avg Loss: 5.3524\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 450, Loss: 5.3417, Avg Loss: 5.3506\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Average Loss: 5.3481\n",
      "Epoch 8, Batch 0, Loss: 5.4705, Avg Loss: 5.4705\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 50, Loss: 5.3743, Avg Loss: 5.3348\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 100, Loss: 5.2332, Avg Loss: 5.3423\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 150, Loss: 5.2531, Avg Loss: 5.3394\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 200, Loss: 5.4626, Avg Loss: 5.3391\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 250, Loss: 5.3565, Avg Loss: 5.3337\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 300, Loss: 5.1854, Avg Loss: 5.3338\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 350, Loss: 5.2967, Avg Loss: 5.3391\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 400, Loss: 5.3119, Avg Loss: 5.3378\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 450, Loss: 5.3591, Avg Loss: 5.3375\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Average Loss: 5.3378\n",
      "Epoch 9, Batch 0, Loss: 5.4094, Avg Loss: 5.4094\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 50, Loss: 5.2537, Avg Loss: 5.3413\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 100, Loss: 5.4030, Avg Loss: 5.3549\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 150, Loss: 5.2009, Avg Loss: 5.3473\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 200, Loss: 5.2985, Avg Loss: 5.3430\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 250, Loss: 5.4570, Avg Loss: 5.3347\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 300, Loss: 5.4300, Avg Loss: 5.3304\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 350, Loss: 5.2347, Avg Loss: 5.3288\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 400, Loss: 5.4928, Avg Loss: 5.3304\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 450, Loss: 5.3353, Avg Loss: 5.3306\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Average Loss: 5.3316\n",
      "Epoch 10, Batch 0, Loss: 5.2470, Avg Loss: 5.2470\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 50, Loss: 5.5211, Avg Loss: 5.3190\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 100, Loss: 5.1735, Avg Loss: 5.3211\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 150, Loss: 5.2656, Avg Loss: 5.3141\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 200, Loss: 5.3045, Avg Loss: 5.3147\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 250, Loss: 5.3437, Avg Loss: 5.3192\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 300, Loss: 5.3922, Avg Loss: 5.3259\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 350, Loss: 5.3349, Avg Loss: 5.3243\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 400, Loss: 5.3638, Avg Loss: 5.3246\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 450, Loss: 5.2150, Avg Loss: 5.3242\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Average Loss: 5.3231\n",
      "Epoch 11, Batch 0, Loss: 5.2399, Avg Loss: 5.2399\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 50, Loss: 5.3217, Avg Loss: 5.3031\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 100, Loss: 5.2073, Avg Loss: 5.2972\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 150, Loss: 5.4976, Avg Loss: 5.2980\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 200, Loss: 5.1195, Avg Loss: 5.3062\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 250, Loss: 5.4423, Avg Loss: 5.3062\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 300, Loss: 5.4149, Avg Loss: 5.3047\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 350, Loss: 5.4885, Avg Loss: 5.3085\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 400, Loss: 5.3416, Avg Loss: 5.3122\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 450, Loss: 5.6231, Avg Loss: 5.3134\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Average Loss: 5.3153\n",
      "Epoch 12, Batch 0, Loss: 5.4202, Avg Loss: 5.4202\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 50, Loss: 5.2064, Avg Loss: 5.3115\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 100, Loss: 5.4247, Avg Loss: 5.3080\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 150, Loss: 5.1822, Avg Loss: 5.3087\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 200, Loss: 5.0620, Avg Loss: 5.3105\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 250, Loss: 5.3279, Avg Loss: 5.3098\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 300, Loss: 5.2995, Avg Loss: 5.3068\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 350, Loss: 5.3284, Avg Loss: 5.3096\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 400, Loss: 5.1310, Avg Loss: 5.3114\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 450, Loss: 5.2711, Avg Loss: 5.3103\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Average Loss: 5.3106\n",
      "Epoch 13, Batch 0, Loss: 5.1276, Avg Loss: 5.1276\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 50, Loss: 5.3505, Avg Loss: 5.2738\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 100, Loss: 5.1697, Avg Loss: 5.2783\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 150, Loss: 5.4046, Avg Loss: 5.2851\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 200, Loss: 5.3624, Avg Loss: 5.2948\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 250, Loss: 5.3361, Avg Loss: 5.2956\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 300, Loss: 5.2498, Avg Loss: 5.2936\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 350, Loss: 5.2877, Avg Loss: 5.2976\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 400, Loss: 5.2379, Avg Loss: 5.2969\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 450, Loss: 5.5171, Avg Loss: 5.2997\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Average Loss: 5.3013\n",
      "Epoch 14, Batch 0, Loss: 5.3012, Avg Loss: 5.3012\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 50, Loss: 5.2636, Avg Loss: 5.2866\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 100, Loss: 5.2462, Avg Loss: 5.2801\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 150, Loss: 5.2985, Avg Loss: 5.2813\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 200, Loss: 5.2467, Avg Loss: 5.2873\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 250, Loss: 5.3876, Avg Loss: 5.2831\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 300, Loss: 5.2138, Avg Loss: 5.2875\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 350, Loss: 5.4675, Avg Loss: 5.2861\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 400, Loss: 5.2640, Avg Loss: 5.2869\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 450, Loss: 5.3585, Avg Loss: 5.2896\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Average Loss: 5.2927\n",
      "Epoch 15, Batch 0, Loss: 5.2432, Avg Loss: 5.2432\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 50, Loss: 5.3250, Avg Loss: 5.2932\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 100, Loss: 5.0389, Avg Loss: 5.2774\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 150, Loss: 5.1852, Avg Loss: 5.2794\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 200, Loss: 5.3409, Avg Loss: 5.2836\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 250, Loss: 5.3428, Avg Loss: 5.2813\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 300, Loss: 5.2764, Avg Loss: 5.2826\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 350, Loss: 5.3487, Avg Loss: 5.2827\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 400, Loss: 5.3302, Avg Loss: 5.2846\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 450, Loss: 5.2223, Avg Loss: 5.2827\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Average Loss: 5.2831\n",
      "Epoch 16, Batch 0, Loss: 5.3123, Avg Loss: 5.3123\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 50, Loss: 5.3553, Avg Loss: 5.2616\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 100, Loss: 5.3172, Avg Loss: 5.2745\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 150, Loss: 5.2958, Avg Loss: 5.2672\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 200, Loss: 5.1123, Avg Loss: 5.2667\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 250, Loss: 5.3408, Avg Loss: 5.2708\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 300, Loss: 5.2377, Avg Loss: 5.2726\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 350, Loss: 5.1596, Avg Loss: 5.2735\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 400, Loss: 5.3046, Avg Loss: 5.2719\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 450, Loss: 5.2932, Avg Loss: 5.2710\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Average Loss: 5.2716\n",
      "Epoch 17, Batch 0, Loss: 5.1533, Avg Loss: 5.1533\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 50, Loss: 5.1028, Avg Loss: 5.2337\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 100, Loss: 5.2067, Avg Loss: 5.2456\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 150, Loss: 5.1904, Avg Loss: 5.2493\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 200, Loss: 5.3926, Avg Loss: 5.2494\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 250, Loss: 5.1221, Avg Loss: 5.2469\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 300, Loss: 5.1401, Avg Loss: 5.2511\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 350, Loss: 5.2157, Avg Loss: 5.2524\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 400, Loss: 5.2486, Avg Loss: 5.2578\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 450, Loss: 5.3186, Avg Loss: 5.2562\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Average Loss: 5.2595\n",
      "Epoch 18, Batch 0, Loss: 5.2246, Avg Loss: 5.2246\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 50, Loss: 5.1634, Avg Loss: 5.2381\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 100, Loss: 5.1149, Avg Loss: 5.2343\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 150, Loss: 5.3339, Avg Loss: 5.2297\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 200, Loss: 5.1383, Avg Loss: 5.2281\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 250, Loss: 5.2773, Avg Loss: 5.2274\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 300, Loss: 5.2613, Avg Loss: 5.2331\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 350, Loss: 5.2219, Avg Loss: 5.2382\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 400, Loss: 5.2588, Avg Loss: 5.2410\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 450, Loss: 5.3482, Avg Loss: 5.2423\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Average Loss: 5.2462\n",
      "Epoch 19, Batch 0, Loss: 5.1132, Avg Loss: 5.1132\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 50, Loss: 5.1281, Avg Loss: 5.2381\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 100, Loss: 5.2954, Avg Loss: 5.2289\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 150, Loss: 5.2543, Avg Loss: 5.2223\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 200, Loss: 5.4723, Avg Loss: 5.2249\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 250, Loss: 5.3463, Avg Loss: 5.2263\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 300, Loss: 5.0357, Avg Loss: 5.2240\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 350, Loss: 5.2238, Avg Loss: 5.2263\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 400, Loss: 5.1331, Avg Loss: 5.2266\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 450, Loss: 5.1383, Avg Loss: 5.2308\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Average Loss: 5.2302\n",
      "Epoch 20, Batch 0, Loss: 5.1448, Avg Loss: 5.1448\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 50, Loss: 5.0462, Avg Loss: 5.1855\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 100, Loss: 5.2383, Avg Loss: 5.1872\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 150, Loss: 5.2121, Avg Loss: 5.1884\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 200, Loss: 5.1857, Avg Loss: 5.1950\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 250, Loss: 5.1351, Avg Loss: 5.1993\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 300, Loss: 5.2266, Avg Loss: 5.1985\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 350, Loss: 5.1598, Avg Loss: 5.2040\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 400, Loss: 5.3536, Avg Loss: 5.2102\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 450, Loss: 5.2389, Avg Loss: 5.2124\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Average Loss: 5.2141\n",
      "Epoch 21, Batch 0, Loss: 4.9394, Avg Loss: 4.9394\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 50, Loss: 5.2172, Avg Loss: 5.1740\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 100, Loss: 5.3049, Avg Loss: 5.2136\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 150, Loss: 5.3085, Avg Loss: 5.2148\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 200, Loss: 5.1941, Avg Loss: 5.2109\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 250, Loss: 5.2578, Avg Loss: 5.2104\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 300, Loss: 5.1019, Avg Loss: 5.2119\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 350, Loss: 5.1845, Avg Loss: 5.2123\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 400, Loss: 5.1299, Avg Loss: 5.2107\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 450, Loss: 5.3823, Avg Loss: 5.2085\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Average Loss: 5.2099\n",
      "Epoch 22, Batch 0, Loss: 5.2120, Avg Loss: 5.2120\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 50, Loss: 5.0901, Avg Loss: 5.1485\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 100, Loss: 5.3106, Avg Loss: 5.1630\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 150, Loss: 5.0873, Avg Loss: 5.1688\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 200, Loss: 5.2074, Avg Loss: 5.1645\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 250, Loss: 5.3421, Avg Loss: 5.1703\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 300, Loss: 5.1938, Avg Loss: 5.1754\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 350, Loss: 5.0945, Avg Loss: 5.1773\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 400, Loss: 5.0214, Avg Loss: 5.1784\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 450, Loss: 5.2497, Avg Loss: 5.1782\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Average Loss: 5.1813\n",
      "Epoch 23, Batch 0, Loss: 5.1461, Avg Loss: 5.1461\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 50, Loss: 5.1967, Avg Loss: 5.1391\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 100, Loss: 5.3575, Avg Loss: 5.1425\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 150, Loss: 5.0675, Avg Loss: 5.1491\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 200, Loss: 5.1716, Avg Loss: 5.1517\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 250, Loss: 4.9789, Avg Loss: 5.1580\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 300, Loss: 5.2615, Avg Loss: 5.1596\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 350, Loss: 5.1706, Avg Loss: 5.1590\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 400, Loss: 5.3043, Avg Loss: 5.1594\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 450, Loss: 5.2018, Avg Loss: 5.1624\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Average Loss: 5.1642\n",
      "Epoch 24, Batch 0, Loss: 5.1173, Avg Loss: 5.1173\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 50, Loss: 5.1053, Avg Loss: 5.1156\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 100, Loss: 5.1159, Avg Loss: 5.1234\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 150, Loss: 5.2193, Avg Loss: 5.1357\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 200, Loss: 5.1066, Avg Loss: 5.1383\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 250, Loss: 5.0669, Avg Loss: 5.1409\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 300, Loss: 5.0472, Avg Loss: 5.1402\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 350, Loss: 5.3034, Avg Loss: 5.1407\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 400, Loss: 5.0652, Avg Loss: 5.1432\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 450, Loss: 5.1257, Avg Loss: 5.1415\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Average Loss: 5.1421\n",
      "Epoch 25, Batch 0, Loss: 5.0435, Avg Loss: 5.0435\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 50, Loss: 4.9617, Avg Loss: 5.0556\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 100, Loss: 4.9714, Avg Loss: 5.0784\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 150, Loss: 5.0251, Avg Loss: 5.0865\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 200, Loss: 4.9314, Avg Loss: 5.0920\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 250, Loss: 5.1591, Avg Loss: 5.0989\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 300, Loss: 5.1092, Avg Loss: 5.1021\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 350, Loss: 4.8972, Avg Loss: 5.1060\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 400, Loss: 5.1277, Avg Loss: 5.1083\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 450, Loss: 5.1782, Avg Loss: 5.1129\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Average Loss: 5.1149\n",
      "Epoch 26, Batch 0, Loss: 5.0674, Avg Loss: 5.0674\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 50, Loss: 4.9675, Avg Loss: 5.0764\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 100, Loss: 5.0918, Avg Loss: 5.0847\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 150, Loss: 5.1566, Avg Loss: 5.0899\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 200, Loss: 5.0443, Avg Loss: 5.0884\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 250, Loss: 4.8560, Avg Loss: 5.0890\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 300, Loss: 5.0392, Avg Loss: 5.0917\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 350, Loss: 5.2164, Avg Loss: 5.0947\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 400, Loss: 4.9775, Avg Loss: 5.0954\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 450, Loss: 5.2367, Avg Loss: 5.0954\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Average Loss: 5.0952\n",
      "Epoch 27, Batch 0, Loss: 5.1837, Avg Loss: 5.1837\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 50, Loss: 5.0076, Avg Loss: 5.0305\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 100, Loss: 5.1253, Avg Loss: 5.0433\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 150, Loss: 5.0505, Avg Loss: 5.0477\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 200, Loss: 5.0537, Avg Loss: 5.0504\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 250, Loss: 4.9945, Avg Loss: 5.0484\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 300, Loss: 5.0774, Avg Loss: 5.0533\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 350, Loss: 5.1318, Avg Loss: 5.0580\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 400, Loss: 4.9933, Avg Loss: 5.0578\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 450, Loss: 5.1714, Avg Loss: 5.0615\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Average Loss: 5.0645\n",
      "Epoch 28, Batch 0, Loss: 5.0441, Avg Loss: 5.0441\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 50, Loss: 5.1852, Avg Loss: 5.0067\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 100, Loss: 5.0519, Avg Loss: 5.0108\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 150, Loss: 5.1255, Avg Loss: 5.0198\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 200, Loss: 4.9119, Avg Loss: 5.0213\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 250, Loss: 5.0064, Avg Loss: 5.0233\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 300, Loss: 5.0854, Avg Loss: 5.0266\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 350, Loss: 5.1174, Avg Loss: 5.0275\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 400, Loss: 5.1523, Avg Loss: 5.0308\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 450, Loss: 5.0024, Avg Loss: 5.0312\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Average Loss: 5.0332\n",
      "Epoch 29, Batch 0, Loss: 4.9776, Avg Loss: 4.9776\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 50, Loss: 4.9373, Avg Loss: 4.9610\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 100, Loss: 4.9587, Avg Loss: 4.9691\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 150, Loss: 4.9100, Avg Loss: 4.9739\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 200, Loss: 5.0006, Avg Loss: 4.9749\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 250, Loss: 4.9311, Avg Loss: 4.9802\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 300, Loss: 5.0133, Avg Loss: 4.9857\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 350, Loss: 5.0282, Avg Loss: 4.9911\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 400, Loss: 4.8478, Avg Loss: 4.9938\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 450, Loss: 5.0066, Avg Loss: 4.9973\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Average Loss: 4.9994\n",
      "Epoch 30, Batch 0, Loss: 4.9049, Avg Loss: 4.9049\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 50, Loss: 4.9403, Avg Loss: 4.9466\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 100, Loss: 4.8931, Avg Loss: 4.9472\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 150, Loss: 4.9055, Avg Loss: 4.9472\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 200, Loss: 4.9627, Avg Loss: 4.9452\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 250, Loss: 4.8901, Avg Loss: 4.9518\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 300, Loss: 4.9784, Avg Loss: 4.9542\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 350, Loss: 5.0649, Avg Loss: 4.9576\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 400, Loss: 5.0492, Avg Loss: 4.9603\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 450, Loss: 5.0143, Avg Loss: 4.9615\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Average Loss: 4.9611\n",
      "Epoch 31, Batch 0, Loss: 4.9728, Avg Loss: 4.9728\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 50, Loss: 4.9152, Avg Loss: 4.8723\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 100, Loss: 4.9302, Avg Loss: 4.8768\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 150, Loss: 4.9198, Avg Loss: 4.8883\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 200, Loss: 5.0329, Avg Loss: 4.8923\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 250, Loss: 4.8904, Avg Loss: 4.9038\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 300, Loss: 4.9099, Avg Loss: 4.9073\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 350, Loss: 4.9166, Avg Loss: 4.9108\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 400, Loss: 4.9512, Avg Loss: 4.9157\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 450, Loss: 4.8886, Avg Loss: 4.9193\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Average Loss: 4.9204\n",
      "Epoch 32, Batch 0, Loss: 4.9394, Avg Loss: 4.9394\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 50, Loss: 4.7939, Avg Loss: 4.8271\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 100, Loss: 4.8916, Avg Loss: 4.8297\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 150, Loss: 4.9483, Avg Loss: 4.8436\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 200, Loss: 4.8274, Avg Loss: 4.8519\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 250, Loss: 4.8443, Avg Loss: 4.8586\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 300, Loss: 4.8420, Avg Loss: 4.8628\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 350, Loss: 4.9583, Avg Loss: 4.8642\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 400, Loss: 4.8700, Avg Loss: 4.8708\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 450, Loss: 4.8753, Avg Loss: 4.8758\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Average Loss: 4.8772\n",
      "Epoch 33, Batch 0, Loss: 4.7363, Avg Loss: 4.7363\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 50, Loss: 4.8701, Avg Loss: 4.7949\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 100, Loss: 4.8025, Avg Loss: 4.8001\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 150, Loss: 4.9044, Avg Loss: 4.8036\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 200, Loss: 4.8342, Avg Loss: 4.8055\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 250, Loss: 4.9328, Avg Loss: 4.8101\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 300, Loss: 4.8488, Avg Loss: 4.8172\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 350, Loss: 4.8615, Avg Loss: 4.8188\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 400, Loss: 4.6919, Avg Loss: 4.8210\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 450, Loss: 4.9507, Avg Loss: 4.8236\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Average Loss: 4.8256\n",
      "Epoch 34, Batch 0, Loss: 4.7847, Avg Loss: 4.7847\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 34, Batch 50, Loss: 4.8150, Avg Loss: 4.7225\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 34, Batch 100, Loss: 4.7865, Avg Loss: 4.7308\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 68\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, num_epochs, device, vocab_size)\u001b[0m\n\u001b[0;32m     65\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Track loss (multiply by accumulation steps to get true loss)\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m gradient_accumulation_steps\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Print progress\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader,start_epoch=50, num_epochs=40, device=device, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e019f613-7e88-447f-8cf2-8c60d9f037cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (pos_encoding): PositionalEncoding()\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=12288, bias=True)\n",
       "        (fc2): Linear(in_features=12288, out_features=1024, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=1024, out_features=32000, bias=True)\n",
       "  (input_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b91fb5f-85ef-4e9e-8f4f-d792523f1003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_9048\\4040975594.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_9048\\4040975594.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, Batch 0, Loss: 4.7486, Avg Loss: 4.7486\n",
      "Epoch 51, Batch 50, Loss: 4.7525, Avg Loss: 4.6951\n",
      "Epoch 51, Batch 100, Loss: 4.6004, Avg Loss: 4.6840\n",
      "Epoch 51, Batch 150, Loss: 4.4614, Avg Loss: 4.6691\n",
      "Epoch 51, Batch 200, Loss: 4.7306, Avg Loss: 4.6544\n",
      "Epoch 51, Batch 250, Loss: 4.5479, Avg Loss: 4.6441\n",
      "Epoch 51, Batch 300, Loss: 4.4485, Avg Loss: 4.6309\n",
      "Epoch 51, Batch 350, Loss: 4.5122, Avg Loss: 4.6185\n",
      "Epoch 51, Batch 400, Loss: 4.5297, Avg Loss: 4.6066\n",
      "Epoch 51, Batch 450, Loss: 4.5123, Avg Loss: 4.5976\n",
      "Epoch 51, Average Loss: 4.5903\n",
      "Epoch 52, Batch 0, Loss: 4.5649, Avg Loss: 4.5649\n",
      "Epoch 52, Batch 50, Loss: 4.5381, Avg Loss: 4.5005\n",
      "Epoch 52, Batch 100, Loss: 4.5162, Avg Loss: 4.4990\n",
      "Epoch 52, Batch 150, Loss: 4.3415, Avg Loss: 4.4908\n",
      "Epoch 52, Batch 200, Loss: 4.4661, Avg Loss: 4.4884\n",
      "Epoch 52, Batch 250, Loss: 4.3692, Avg Loss: 4.4885\n",
      "Epoch 52, Batch 300, Loss: 4.3686, Avg Loss: 4.4830\n",
      "Epoch 52, Batch 350, Loss: 4.4119, Avg Loss: 4.4837\n",
      "Epoch 52, Batch 400, Loss: 4.3464, Avg Loss: 4.4800\n",
      "Epoch 52, Batch 450, Loss: 4.5772, Avg Loss: 4.4768\n",
      "Epoch 52, Average Loss: 4.4763\n",
      "Epoch 53, Batch 0, Loss: 4.2755, Avg Loss: 4.2755\n",
      "Epoch 53, Batch 50, Loss: 4.3661, Avg Loss: 4.4312\n",
      "Epoch 53, Batch 100, Loss: 4.4672, Avg Loss: 4.4362\n",
      "Epoch 53, Batch 150, Loss: 4.3086, Avg Loss: 4.4340\n",
      "Epoch 53, Batch 200, Loss: 4.4386, Avg Loss: 4.4343\n",
      "Epoch 53, Batch 250, Loss: 4.4976, Avg Loss: 4.4315\n",
      "Epoch 53, Batch 300, Loss: 4.4905, Avg Loss: 4.4349\n",
      "Epoch 53, Batch 350, Loss: 4.4055, Avg Loss: 4.4349\n",
      "Epoch 53, Batch 400, Loss: 4.4448, Avg Loss: 4.4339\n",
      "Epoch 53, Batch 450, Loss: 4.3004, Avg Loss: 4.4323\n",
      "Epoch 53, Average Loss: 4.4302\n",
      "Epoch 54, Batch 0, Loss: 4.3795, Avg Loss: 4.3795\n",
      "Epoch 54, Batch 50, Loss: 4.2740, Avg Loss: 4.3780\n",
      "Epoch 54, Batch 100, Loss: 4.3755, Avg Loss: 4.3750\n",
      "Epoch 54, Batch 150, Loss: 4.4323, Avg Loss: 4.3794\n",
      "Epoch 54, Batch 200, Loss: 4.3900, Avg Loss: 4.3833\n",
      "Epoch 54, Batch 250, Loss: 4.4325, Avg Loss: 4.3857\n",
      "Epoch 54, Batch 300, Loss: 4.4801, Avg Loss: 4.3875\n",
      "Epoch 54, Batch 350, Loss: 4.4198, Avg Loss: 4.3870\n",
      "Epoch 54, Batch 400, Loss: 4.4357, Avg Loss: 4.3887\n",
      "Epoch 54, Batch 450, Loss: 4.4863, Avg Loss: 4.3891\n",
      "Epoch 54, Average Loss: 4.3920\n",
      "Epoch 55, Batch 0, Loss: 4.3689, Avg Loss: 4.3689\n",
      "Epoch 55, Batch 50, Loss: 4.2552, Avg Loss: 4.3550\n",
      "Epoch 55, Batch 100, Loss: 4.3968, Avg Loss: 4.3531\n",
      "Epoch 55, Batch 150, Loss: 4.3104, Avg Loss: 4.3507\n",
      "Epoch 55, Batch 200, Loss: 4.2267, Avg Loss: 4.3496\n",
      "Epoch 55, Batch 250, Loss: 4.2975, Avg Loss: 4.3520\n",
      "Epoch 55, Batch 300, Loss: 4.3545, Avg Loss: 4.3523\n",
      "Epoch 55, Batch 350, Loss: 4.2911, Avg Loss: 4.3502\n",
      "Epoch 55, Batch 400, Loss: 4.1901, Avg Loss: 4.3511\n",
      "Epoch 55, Batch 450, Loss: 4.2894, Avg Loss: 4.3505\n",
      "Epoch 55, Average Loss: 4.3509\n",
      "Epoch 56, Batch 0, Loss: 4.2485, Avg Loss: 4.2485\n",
      "Epoch 56, Batch 50, Loss: 4.1655, Avg Loss: 4.2876\n",
      "Epoch 56, Batch 100, Loss: 4.3092, Avg Loss: 4.2841\n",
      "Epoch 56, Batch 150, Loss: 4.4027, Avg Loss: 4.2917\n",
      "Epoch 56, Batch 200, Loss: 4.3579, Avg Loss: 4.2948\n",
      "Epoch 56, Batch 250, Loss: 4.2373, Avg Loss: 4.2943\n",
      "Epoch 56, Batch 300, Loss: 4.3287, Avg Loss: 4.2941\n",
      "Epoch 56, Batch 350, Loss: 4.4091, Avg Loss: 4.2975\n",
      "Epoch 56, Batch 400, Loss: 4.2472, Avg Loss: 4.2992\n",
      "Epoch 56, Batch 450, Loss: 4.2418, Avg Loss: 4.3009\n",
      "Epoch 56, Average Loss: 4.3041\n",
      "Epoch 57, Batch 0, Loss: 4.3168, Avg Loss: 4.3168\n",
      "Epoch 57, Batch 50, Loss: 4.2618, Avg Loss: 4.2244\n",
      "Epoch 57, Batch 100, Loss: 4.1815, Avg Loss: 4.2269\n",
      "Epoch 57, Batch 150, Loss: 4.3070, Avg Loss: 4.2321\n",
      "Epoch 57, Batch 200, Loss: 4.2101, Avg Loss: 4.2354\n",
      "Epoch 57, Batch 250, Loss: 4.2815, Avg Loss: 4.2375\n",
      "Epoch 57, Batch 300, Loss: 4.2849, Avg Loss: 4.2429\n",
      "Epoch 57, Batch 350, Loss: 4.2973, Avg Loss: 4.2437\n",
      "Epoch 57, Batch 400, Loss: 4.2713, Avg Loss: 4.2469\n",
      "Epoch 57, Batch 450, Loss: 4.2316, Avg Loss: 4.2482\n",
      "Epoch 57, Average Loss: 4.2505\n",
      "Epoch 58, Batch 0, Loss: 4.2532, Avg Loss: 4.2532\n",
      "Epoch 58, Batch 50, Loss: 4.2468, Avg Loss: 4.1636\n",
      "Epoch 58, Batch 100, Loss: 4.0985, Avg Loss: 4.1612\n",
      "Epoch 58, Batch 150, Loss: 4.2439, Avg Loss: 4.1662\n",
      "Epoch 58, Batch 200, Loss: 4.1948, Avg Loss: 4.1716\n",
      "Epoch 58, Batch 250, Loss: 4.1643, Avg Loss: 4.1773\n",
      "Epoch 58, Batch 300, Loss: 4.2514, Avg Loss: 4.1801\n",
      "Epoch 58, Batch 350, Loss: 4.1672, Avg Loss: 4.1834\n",
      "Epoch 58, Batch 400, Loss: 4.2062, Avg Loss: 4.1846\n",
      "Epoch 58, Batch 450, Loss: 4.1818, Avg Loss: 4.1879\n",
      "Epoch 58, Average Loss: 4.1910\n",
      "Epoch 59, Batch 0, Loss: 4.1025, Avg Loss: 4.1025\n",
      "Epoch 59, Batch 50, Loss: 4.0895, Avg Loss: 4.0885\n",
      "Epoch 59, Batch 100, Loss: 4.1193, Avg Loss: 4.0920\n",
      "Epoch 59, Batch 150, Loss: 4.2154, Avg Loss: 4.1008\n",
      "Epoch 59, Batch 200, Loss: 4.1698, Avg Loss: 4.1063\n",
      "Epoch 59, Batch 250, Loss: 4.1998, Avg Loss: 4.1104\n",
      "Epoch 59, Batch 300, Loss: 4.2177, Avg Loss: 4.1133\n",
      "Epoch 59, Batch 350, Loss: 4.0786, Avg Loss: 4.1165\n",
      "Epoch 59, Batch 400, Loss: 4.1342, Avg Loss: 4.1212\n",
      "Epoch 59, Batch 450, Loss: 4.1166, Avg Loss: 4.1237\n",
      "Epoch 59, Average Loss: 4.1248\n",
      "Epoch 60, Batch 0, Loss: 3.9638, Avg Loss: 3.9638\n",
      "Epoch 60, Batch 50, Loss: 3.8785, Avg Loss: 4.0015\n",
      "Epoch 60, Batch 100, Loss: 4.0552, Avg Loss: 4.0340\n",
      "Epoch 60, Batch 150, Loss: 4.0776, Avg Loss: 4.0285\n",
      "Epoch 60, Batch 200, Loss: 3.9226, Avg Loss: 4.0331\n",
      "Epoch 60, Batch 250, Loss: 4.1046, Avg Loss: 4.0388\n",
      "Epoch 60, Batch 300, Loss: 4.0858, Avg Loss: 4.0406\n",
      "Epoch 60, Batch 350, Loss: 4.1397, Avg Loss: 4.0458\n",
      "Epoch 60, Batch 400, Loss: 3.9931, Avg Loss: 4.0483\n",
      "Epoch 60, Batch 450, Loss: 4.1082, Avg Loss: 4.0517\n",
      "Epoch 60, Average Loss: 4.0534\n",
      "Epoch 61, Batch 0, Loss: 3.8498, Avg Loss: 3.8498\n",
      "Epoch 61, Batch 50, Loss: 3.7094, Avg Loss: 3.9251\n",
      "Epoch 61, Batch 100, Loss: 4.0376, Avg Loss: 3.9312\n",
      "Epoch 61, Batch 150, Loss: 3.9056, Avg Loss: 3.9424\n",
      "Epoch 61, Batch 200, Loss: 3.9751, Avg Loss: 3.9480\n",
      "Epoch 61, Batch 250, Loss: 4.0140, Avg Loss: 3.9552\n",
      "Epoch 61, Batch 300, Loss: 4.1570, Avg Loss: 3.9611\n",
      "Epoch 61, Batch 350, Loss: 4.0894, Avg Loss: 3.9682\n",
      "Epoch 61, Batch 400, Loss: 4.1825, Avg Loss: 3.9715\n",
      "Epoch 61, Batch 450, Loss: 4.0279, Avg Loss: 3.9737\n",
      "Epoch 61, Average Loss: 3.9757\n",
      "Epoch 62, Batch 0, Loss: 3.8241, Avg Loss: 3.8241\n",
      "Epoch 62, Batch 50, Loss: 3.9162, Avg Loss: 3.8363\n",
      "Epoch 62, Batch 100, Loss: 3.8819, Avg Loss: 3.8420\n",
      "Epoch 62, Batch 150, Loss: 3.8455, Avg Loss: 3.8444\n",
      "Epoch 62, Batch 200, Loss: 3.7999, Avg Loss: 3.8559\n",
      "Epoch 62, Batch 250, Loss: 3.8995, Avg Loss: 3.8621\n",
      "Epoch 62, Batch 300, Loss: 3.8247, Avg Loss: 3.8702\n",
      "Epoch 62, Batch 350, Loss: 3.8740, Avg Loss: 3.8764\n",
      "Epoch 62, Batch 400, Loss: 3.9642, Avg Loss: 3.8818\n",
      "Epoch 62, Batch 450, Loss: 4.1037, Avg Loss: 3.8889\n",
      "Epoch 62, Average Loss: 3.8924\n",
      "Epoch 63, Batch 0, Loss: 3.7414, Avg Loss: 3.7414\n",
      "Epoch 63, Batch 50, Loss: 3.7199, Avg Loss: 3.7485\n",
      "Epoch 63, Batch 100, Loss: 3.6527, Avg Loss: 3.7537\n",
      "Epoch 63, Batch 150, Loss: 3.7272, Avg Loss: 3.7644\n",
      "Epoch 63, Batch 200, Loss: 3.8227, Avg Loss: 3.7688\n",
      "Epoch 63, Batch 250, Loss: 3.8004, Avg Loss: 3.7768\n",
      "Epoch 63, Batch 300, Loss: 3.7176, Avg Loss: 3.7862\n",
      "Epoch 63, Batch 350, Loss: 3.7722, Avg Loss: 3.7919\n",
      "Epoch 63, Batch 400, Loss: 3.8856, Avg Loss: 3.7969\n",
      "Epoch 63, Batch 450, Loss: 3.8787, Avg Loss: 3.7990\n",
      "Epoch 63, Average Loss: 3.8051\n",
      "Epoch 64, Batch 0, Loss: 3.6048, Avg Loss: 3.6048\n",
      "Epoch 64, Batch 50, Loss: 3.5586, Avg Loss: 3.6292\n",
      "Epoch 64, Batch 100, Loss: 3.6008, Avg Loss: 3.6489\n",
      "Epoch 64, Batch 150, Loss: 3.6914, Avg Loss: 3.6657\n",
      "Epoch 64, Batch 200, Loss: 3.7632, Avg Loss: 3.6740\n",
      "Epoch 64, Batch 250, Loss: 3.7837, Avg Loss: 3.6787\n",
      "Epoch 64, Batch 300, Loss: 3.7178, Avg Loss: 3.6856\n",
      "Epoch 64, Batch 350, Loss: 3.7511, Avg Loss: 3.6942\n",
      "Epoch 64, Batch 400, Loss: 3.7001, Avg Loss: 3.7010\n",
      "Epoch 64, Batch 450, Loss: 3.7232, Avg Loss: 3.7065\n",
      "Epoch 64, Average Loss: 3.7132\n",
      "Epoch 65, Batch 0, Loss: 3.4180, Avg Loss: 3.4180\n",
      "Epoch 65, Batch 50, Loss: 3.5697, Avg Loss: 3.5498\n",
      "Epoch 65, Batch 100, Loss: 3.5405, Avg Loss: 3.5631\n",
      "Epoch 65, Batch 150, Loss: 3.6270, Avg Loss: 3.5651\n",
      "Epoch 65, Batch 200, Loss: 3.5903, Avg Loss: 3.5768\n",
      "Epoch 65, Batch 250, Loss: 3.5477, Avg Loss: 3.5848\n",
      "Epoch 65, Batch 300, Loss: 3.7680, Avg Loss: 3.5898\n",
      "Epoch 65, Batch 350, Loss: 3.5562, Avg Loss: 3.5989\n",
      "Epoch 65, Batch 400, Loss: 3.6699, Avg Loss: 3.6038\n",
      "Epoch 65, Batch 450, Loss: 3.7888, Avg Loss: 3.6103\n",
      "Epoch 65, Average Loss: 3.6175\n",
      "Epoch 66, Batch 0, Loss: 3.5700, Avg Loss: 3.5700\n",
      "Epoch 66, Batch 50, Loss: 3.2615, Avg Loss: 3.4485\n",
      "Epoch 66, Batch 100, Loss: 3.4807, Avg Loss: 3.4537\n",
      "Epoch 66, Batch 150, Loss: 3.5317, Avg Loss: 3.4663\n",
      "Epoch 66, Batch 200, Loss: 3.3287, Avg Loss: 3.4722\n",
      "Epoch 66, Batch 250, Loss: 3.5320, Avg Loss: 3.4810\n",
      "Epoch 66, Batch 300, Loss: 3.4863, Avg Loss: 3.4872\n",
      "Epoch 66, Batch 350, Loss: 3.5926, Avg Loss: 3.4970\n",
      "Epoch 66, Batch 400, Loss: 3.3860, Avg Loss: 3.5027\n",
      "Epoch 66, Batch 450, Loss: 3.7668, Avg Loss: 3.5089\n",
      "Epoch 66, Average Loss: 3.5151\n",
      "Epoch 67, Batch 0, Loss: 3.5376, Avg Loss: 3.5376\n",
      "Epoch 67, Batch 50, Loss: 3.4200, Avg Loss: 3.3182\n",
      "Epoch 67, Batch 100, Loss: 3.2757, Avg Loss: 3.3265\n",
      "Epoch 67, Batch 150, Loss: 3.4336, Avg Loss: 3.3442\n",
      "Epoch 67, Batch 200, Loss: 3.4751, Avg Loss: 3.3604\n",
      "Epoch 67, Batch 250, Loss: 3.2494, Avg Loss: 3.3709\n",
      "Epoch 67, Batch 300, Loss: 3.5244, Avg Loss: 3.3807\n",
      "Epoch 67, Batch 350, Loss: 3.5349, Avg Loss: 3.3905\n",
      "Epoch 67, Batch 400, Loss: 3.4673, Avg Loss: 3.4004\n",
      "Epoch 67, Batch 450, Loss: 3.4838, Avg Loss: 3.4068\n",
      "Epoch 67, Average Loss: 3.4143\n",
      "Epoch 68, Batch 0, Loss: 3.1165, Avg Loss: 3.1165\n",
      "Epoch 68, Batch 50, Loss: 3.3049, Avg Loss: 3.2088\n",
      "Epoch 68, Batch 100, Loss: 3.1595, Avg Loss: 3.2185\n",
      "Epoch 68, Batch 150, Loss: 3.1910, Avg Loss: 3.2378\n",
      "Epoch 68, Batch 200, Loss: 3.1755, Avg Loss: 3.2477\n",
      "Epoch 68, Batch 250, Loss: 3.3967, Avg Loss: 3.2562\n",
      "Epoch 68, Batch 300, Loss: 3.3315, Avg Loss: 3.2750\n",
      "Epoch 68, Batch 350, Loss: 3.4384, Avg Loss: 3.2871\n",
      "Epoch 68, Batch 400, Loss: 3.4847, Avg Loss: 3.2977\n",
      "Epoch 68, Batch 450, Loss: 3.1986, Avg Loss: 3.3064\n",
      "Epoch 68, Average Loss: 3.3140\n",
      "Epoch 69, Batch 0, Loss: 3.1797, Avg Loss: 3.1797\n",
      "Epoch 69, Batch 50, Loss: 3.0759, Avg Loss: 3.0909\n",
      "Epoch 69, Batch 100, Loss: 3.3069, Avg Loss: 3.1115\n",
      "Epoch 69, Batch 150, Loss: 3.1447, Avg Loss: 3.1281\n",
      "Epoch 69, Batch 200, Loss: 3.1034, Avg Loss: 3.1385\n",
      "Epoch 69, Batch 250, Loss: 3.2208, Avg Loss: 3.1601\n",
      "Epoch 69, Batch 300, Loss: 3.4617, Avg Loss: 3.1676\n",
      "Epoch 69, Batch 350, Loss: 3.1349, Avg Loss: 3.1765\n",
      "Epoch 69, Batch 400, Loss: 3.2795, Avg Loss: 3.1876\n",
      "Epoch 69, Batch 450, Loss: 3.1998, Avg Loss: 3.1973\n",
      "Epoch 69, Average Loss: 3.2075\n",
      "Epoch 70, Batch 0, Loss: 2.8829, Avg Loss: 2.8829\n",
      "Epoch 70, Batch 50, Loss: 2.9200, Avg Loss: 2.9536\n",
      "Epoch 70, Batch 100, Loss: 3.1669, Avg Loss: 2.9814\n",
      "Epoch 70, Batch 150, Loss: 3.0291, Avg Loss: 3.0041\n",
      "Epoch 70, Batch 200, Loss: 2.9120, Avg Loss: 3.0228\n",
      "Epoch 70, Batch 250, Loss: 3.0564, Avg Loss: 3.0385\n",
      "Epoch 70, Batch 300, Loss: 3.0851, Avg Loss: 3.0501\n",
      "Epoch 70, Batch 350, Loss: 3.1675, Avg Loss: 3.0627\n",
      "Epoch 70, Batch 400, Loss: 3.5241, Avg Loss: 3.0771\n",
      "Epoch 70, Batch 450, Loss: 3.1404, Avg Loss: 3.0900\n",
      "Epoch 70, Average Loss: 3.1002\n",
      "Epoch 71, Batch 0, Loss: 3.1530, Avg Loss: 3.1530\n",
      "Epoch 71, Batch 50, Loss: 2.9863, Avg Loss: 2.8786\n",
      "Epoch 71, Batch 100, Loss: 2.9179, Avg Loss: 2.8926\n",
      "Epoch 71, Batch 150, Loss: 2.8391, Avg Loss: 2.9040\n",
      "Epoch 71, Batch 200, Loss: 3.0684, Avg Loss: 2.9199\n",
      "Epoch 71, Batch 250, Loss: 3.1543, Avg Loss: 2.9376\n",
      "Epoch 71, Batch 300, Loss: 3.1143, Avg Loss: 2.9493\n",
      "Epoch 71, Batch 350, Loss: 3.1770, Avg Loss: 2.9589\n",
      "Epoch 71, Batch 400, Loss: 3.1018, Avg Loss: 2.9745\n",
      "Epoch 71, Batch 450, Loss: 3.2282, Avg Loss: 2.9831\n",
      "Epoch 71, Average Loss: 2.9977\n",
      "Epoch 72, Batch 0, Loss: 2.5685, Avg Loss: 2.5685\n",
      "Epoch 72, Batch 50, Loss: 2.9096, Avg Loss: 2.7475\n",
      "Epoch 72, Batch 100, Loss: 2.9850, Avg Loss: 2.7718\n",
      "Epoch 72, Batch 150, Loss: 2.8264, Avg Loss: 2.7851\n",
      "Epoch 72, Batch 200, Loss: 2.7935, Avg Loss: 2.8054\n",
      "Epoch 72, Batch 250, Loss: 2.9369, Avg Loss: 2.8234\n",
      "Epoch 72, Batch 300, Loss: 2.7630, Avg Loss: 2.8321\n",
      "Epoch 72, Batch 350, Loss: 2.7948, Avg Loss: 2.8459\n",
      "Epoch 72, Batch 400, Loss: 2.9082, Avg Loss: 2.8591\n",
      "Epoch 72, Batch 450, Loss: 3.0719, Avg Loss: 2.8761\n",
      "Epoch 72, Average Loss: 2.8879\n",
      "Epoch 73, Batch 0, Loss: 2.4894, Avg Loss: 2.4894\n",
      "Epoch 73, Batch 50, Loss: 2.6778, Avg Loss: 2.6381\n",
      "Epoch 73, Batch 100, Loss: 2.8622, Avg Loss: 2.6743\n",
      "Epoch 73, Batch 150, Loss: 2.8995, Avg Loss: 2.6995\n",
      "Epoch 73, Batch 200, Loss: 2.6582, Avg Loss: 2.7287\n",
      "Epoch 73, Batch 250, Loss: 2.8363, Avg Loss: 2.7392\n",
      "Epoch 73, Batch 300, Loss: 2.9766, Avg Loss: 2.7472\n",
      "Epoch 73, Batch 350, Loss: 2.6369, Avg Loss: 2.7605\n",
      "Epoch 73, Batch 400, Loss: 2.9452, Avg Loss: 2.7698\n",
      "Epoch 73, Batch 450, Loss: 3.0436, Avg Loss: 2.7773\n",
      "Epoch 73, Average Loss: 2.7895\n",
      "Epoch 74, Batch 0, Loss: 2.5854, Avg Loss: 2.5854\n",
      "Epoch 74, Batch 50, Loss: 2.6777, Avg Loss: 2.5829\n",
      "Epoch 74, Batch 100, Loss: 2.7336, Avg Loss: 2.5903\n",
      "Epoch 74, Batch 150, Loss: 2.6096, Avg Loss: 2.6053\n",
      "Epoch 74, Batch 200, Loss: 2.7201, Avg Loss: 2.6117\n",
      "Epoch 74, Batch 250, Loss: 2.6748, Avg Loss: 2.6286\n",
      "Epoch 74, Batch 300, Loss: 2.7825, Avg Loss: 2.6397\n",
      "Epoch 74, Batch 350, Loss: 2.5050, Avg Loss: 2.6537\n",
      "Epoch 74, Batch 400, Loss: 3.0655, Avg Loss: 2.6679\n",
      "Epoch 74, Batch 450, Loss: 2.9704, Avg Loss: 2.6833\n",
      "Epoch 74, Average Loss: 2.6962\n",
      "Epoch 75, Batch 0, Loss: 2.4171, Avg Loss: 2.4171\n",
      "Epoch 75, Batch 50, Loss: 2.3603, Avg Loss: 2.4002\n",
      "Epoch 75, Batch 100, Loss: 2.6262, Avg Loss: 2.4580\n",
      "Epoch 75, Batch 150, Loss: 2.5357, Avg Loss: 2.4673\n",
      "Epoch 75, Batch 200, Loss: 2.3598, Avg Loss: 2.4906\n",
      "Epoch 75, Batch 250, Loss: 2.7177, Avg Loss: 2.5167\n",
      "Epoch 75, Batch 300, Loss: 2.6943, Avg Loss: 2.5283\n",
      "Epoch 75, Batch 350, Loss: 2.4581, Avg Loss: 2.5433\n",
      "Epoch 75, Batch 400, Loss: 2.5312, Avg Loss: 2.5536\n",
      "Epoch 75, Batch 450, Loss: 2.8008, Avg Loss: 2.5698\n",
      "Epoch 75, Average Loss: 2.5809\n",
      "Epoch 76, Batch 0, Loss: 2.4042, Avg Loss: 2.4042\n",
      "Epoch 76, Batch 50, Loss: 2.0871, Avg Loss: 2.3298\n",
      "Epoch 76, Batch 100, Loss: 2.5630, Avg Loss: 2.3728\n",
      "Epoch 76, Batch 150, Loss: 2.3463, Avg Loss: 2.3928\n",
      "Epoch 76, Batch 200, Loss: 2.6161, Avg Loss: 2.4112\n",
      "Epoch 76, Batch 250, Loss: 2.7157, Avg Loss: 2.4145\n",
      "Epoch 76, Batch 300, Loss: 2.3660, Avg Loss: 2.4262\n",
      "Epoch 76, Batch 350, Loss: 2.3814, Avg Loss: 2.4347\n",
      "Epoch 76, Batch 400, Loss: 3.0128, Avg Loss: 2.4510\n",
      "Epoch 76, Batch 450, Loss: 2.8102, Avg Loss: 2.4678\n",
      "Epoch 76, Average Loss: 2.4796\n",
      "Epoch 77, Batch 0, Loss: 2.1254, Avg Loss: 2.1254\n",
      "Epoch 77, Batch 50, Loss: 2.3329, Avg Loss: 2.2604\n",
      "Epoch 77, Batch 100, Loss: 2.4762, Avg Loss: 2.2651\n",
      "Epoch 77, Batch 150, Loss: 2.1504, Avg Loss: 2.2850\n",
      "Epoch 77, Batch 200, Loss: 2.5365, Avg Loss: 2.2985\n",
      "Epoch 77, Batch 250, Loss: 2.1828, Avg Loss: 2.3078\n",
      "Epoch 77, Batch 300, Loss: 2.4280, Avg Loss: 2.3180\n",
      "Epoch 77, Batch 350, Loss: 2.6557, Avg Loss: 2.3326\n",
      "Epoch 77, Batch 400, Loss: 2.6976, Avg Loss: 2.3496\n",
      "Epoch 77, Batch 450, Loss: 2.3720, Avg Loss: 2.3642\n",
      "Epoch 77, Average Loss: 2.3823\n",
      "Epoch 78, Batch 0, Loss: 2.1748, Avg Loss: 2.1748\n",
      "Epoch 78, Batch 50, Loss: 1.9860, Avg Loss: 2.1288\n",
      "Epoch 78, Batch 100, Loss: 2.0904, Avg Loss: 2.1427\n",
      "Epoch 78, Batch 150, Loss: 2.3494, Avg Loss: 2.1598\n",
      "Epoch 78, Batch 200, Loss: 2.4589, Avg Loss: 2.1814\n",
      "Epoch 78, Batch 250, Loss: 2.3335, Avg Loss: 2.1951\n",
      "Epoch 78, Batch 300, Loss: 2.5100, Avg Loss: 2.2125\n",
      "Epoch 78, Batch 350, Loss: 2.2169, Avg Loss: 2.2232\n",
      "Epoch 78, Batch 400, Loss: 2.1742, Avg Loss: 2.2394\n",
      "Epoch 78, Batch 450, Loss: 2.5630, Avg Loss: 2.2587\n",
      "Epoch 78, Average Loss: 2.2738\n",
      "Epoch 79, Batch 0, Loss: 2.0944, Avg Loss: 2.0944\n",
      "Epoch 79, Batch 50, Loss: 1.9614, Avg Loss: 2.0128\n",
      "Epoch 79, Batch 100, Loss: 1.9169, Avg Loss: 2.0300\n",
      "Epoch 79, Batch 150, Loss: 2.1369, Avg Loss: 2.0538\n",
      "Epoch 79, Batch 200, Loss: 2.2940, Avg Loss: 2.0771\n",
      "Epoch 79, Batch 250, Loss: 2.3898, Avg Loss: 2.1121\n",
      "Epoch 79, Batch 300, Loss: 2.1544, Avg Loss: 2.1326\n",
      "Epoch 79, Batch 350, Loss: 2.2661, Avg Loss: 2.1455\n",
      "Epoch 79, Batch 400, Loss: 2.6220, Avg Loss: 2.1566\n",
      "Epoch 79, Batch 450, Loss: 2.2712, Avg Loss: 2.1721\n",
      "Epoch 79, Average Loss: 2.1860\n",
      "Epoch 80, Batch 0, Loss: 2.1220, Avg Loss: 2.1220\n",
      "Epoch 80, Batch 50, Loss: 2.1714, Avg Loss: 1.9001\n",
      "Epoch 80, Batch 100, Loss: 2.2095, Avg Loss: 1.9328\n",
      "Epoch 80, Batch 150, Loss: 1.6439, Avg Loss: 1.9533\n",
      "Epoch 80, Batch 200, Loss: 2.1260, Avg Loss: 1.9733\n",
      "Epoch 80, Batch 250, Loss: 2.1751, Avg Loss: 1.9937\n",
      "Epoch 80, Batch 300, Loss: 1.9414, Avg Loss: 2.0134\n",
      "Epoch 80, Batch 350, Loss: 2.0647, Avg Loss: 2.0347\n",
      "Epoch 80, Batch 400, Loss: 2.2174, Avg Loss: 2.0448\n",
      "Epoch 80, Batch 450, Loss: 2.5356, Avg Loss: 2.0661\n",
      "Epoch 80, Average Loss: 2.0821\n",
      "Epoch 81, Batch 0, Loss: 2.0813, Avg Loss: 2.0813\n",
      "Epoch 81, Batch 50, Loss: 1.8433, Avg Loss: 1.8329\n",
      "Epoch 81, Batch 100, Loss: 1.6190, Avg Loss: 1.8419\n",
      "Epoch 81, Batch 150, Loss: 2.1294, Avg Loss: 1.8522\n",
      "Epoch 81, Batch 200, Loss: 1.8419, Avg Loss: 1.8807\n",
      "Epoch 81, Batch 250, Loss: 2.0744, Avg Loss: 1.8954\n",
      "Epoch 81, Batch 300, Loss: 2.0849, Avg Loss: 1.9227\n",
      "Epoch 81, Batch 350, Loss: 2.0733, Avg Loss: 1.9446\n",
      "Epoch 81, Batch 400, Loss: 1.7596, Avg Loss: 1.9595\n",
      "Epoch 81, Batch 450, Loss: 2.1727, Avg Loss: 1.9752\n",
      "Epoch 81, Average Loss: 1.9862\n",
      "Epoch 82, Batch 0, Loss: 1.7108, Avg Loss: 1.7108\n",
      "Epoch 82, Batch 50, Loss: 1.9356, Avg Loss: 1.7310\n",
      "Epoch 82, Batch 100, Loss: 1.8314, Avg Loss: 1.7623\n",
      "Epoch 82, Batch 150, Loss: 1.6928, Avg Loss: 1.7716\n",
      "Epoch 82, Batch 200, Loss: 1.8932, Avg Loss: 1.7906\n",
      "Epoch 82, Batch 250, Loss: 1.9140, Avg Loss: 1.8161\n",
      "Epoch 82, Batch 300, Loss: 1.9277, Avg Loss: 1.8354\n",
      "Epoch 82, Batch 350, Loss: 1.8324, Avg Loss: 1.8490\n",
      "Epoch 82, Batch 400, Loss: 1.7840, Avg Loss: 1.8663\n",
      "Epoch 82, Batch 450, Loss: 1.7528, Avg Loss: 1.8798\n",
      "Epoch 82, Average Loss: 1.8983\n",
      "Epoch 83, Batch 0, Loss: 1.7444, Avg Loss: 1.7444\n",
      "Epoch 83, Batch 50, Loss: 1.7122, Avg Loss: 1.6448\n",
      "Epoch 83, Batch 100, Loss: 1.5451, Avg Loss: 1.6820\n",
      "Epoch 83, Batch 150, Loss: 1.4099, Avg Loss: 1.6999\n",
      "Epoch 83, Batch 200, Loss: 1.7917, Avg Loss: 1.7076\n",
      "Epoch 83, Batch 250, Loss: 1.6377, Avg Loss: 1.7370\n",
      "Epoch 83, Batch 300, Loss: 1.9876, Avg Loss: 1.7472\n",
      "Epoch 83, Batch 350, Loss: 1.6940, Avg Loss: 1.7659\n",
      "Epoch 83, Batch 400, Loss: 1.8544, Avg Loss: 1.7762\n",
      "Epoch 83, Batch 450, Loss: 1.7780, Avg Loss: 1.7918\n",
      "Epoch 83, Average Loss: 1.8091\n",
      "Epoch 84, Batch 0, Loss: 1.4332, Avg Loss: 1.4332\n",
      "Epoch 84, Batch 50, Loss: 1.5972, Avg Loss: 1.5483\n",
      "Epoch 84, Batch 100, Loss: 1.9641, Avg Loss: 1.5771\n",
      "Epoch 84, Batch 150, Loss: 1.7613, Avg Loss: 1.5966\n",
      "Epoch 84, Batch 200, Loss: 1.8563, Avg Loss: 1.6172\n",
      "Epoch 84, Batch 250, Loss: 1.8085, Avg Loss: 1.6313\n",
      "Epoch 84, Batch 300, Loss: 1.4218, Avg Loss: 1.6451\n",
      "Epoch 84, Batch 350, Loss: 1.5956, Avg Loss: 1.6571\n",
      "Epoch 84, Batch 400, Loss: 1.7030, Avg Loss: 1.6780\n",
      "Epoch 84, Batch 450, Loss: 1.9891, Avg Loss: 1.6925\n",
      "Epoch 84, Average Loss: 1.7124\n",
      "Epoch 85, Batch 0, Loss: 1.5980, Avg Loss: 1.5980\n",
      "Epoch 85, Batch 50, Loss: 1.8241, Avg Loss: 1.5381\n",
      "Epoch 85, Batch 100, Loss: 1.7756, Avg Loss: 1.5410\n",
      "Epoch 85, Batch 150, Loss: 1.3964, Avg Loss: 1.5426\n",
      "Epoch 85, Batch 200, Loss: 1.8108, Avg Loss: 1.5514\n",
      "Epoch 85, Batch 250, Loss: 1.7894, Avg Loss: 1.5803\n",
      "Epoch 85, Batch 300, Loss: 1.5096, Avg Loss: 1.6091\n",
      "Epoch 85, Batch 350, Loss: 1.5927, Avg Loss: 1.6205\n",
      "Epoch 85, Batch 400, Loss: 1.4243, Avg Loss: 1.6313\n",
      "Epoch 85, Batch 450, Loss: 1.5977, Avg Loss: 1.6421\n",
      "Epoch 85, Average Loss: 1.6506\n",
      "Epoch 86, Batch 0, Loss: 1.3677, Avg Loss: 1.3677\n",
      "Epoch 86, Batch 50, Loss: 1.9240, Avg Loss: 1.4411\n",
      "Epoch 86, Batch 100, Loss: 1.4685, Avg Loss: 1.4373\n",
      "Epoch 86, Batch 150, Loss: 1.6447, Avg Loss: 1.4578\n",
      "Epoch 86, Batch 200, Loss: 1.5405, Avg Loss: 1.4786\n",
      "Epoch 86, Batch 250, Loss: 1.4534, Avg Loss: 1.4948\n",
      "Epoch 86, Batch 300, Loss: 1.3931, Avg Loss: 1.5110\n",
      "Epoch 86, Batch 350, Loss: 1.5065, Avg Loss: 1.5236\n",
      "Epoch 86, Batch 400, Loss: 1.6396, Avg Loss: 1.5399\n",
      "Epoch 86, Batch 450, Loss: 1.9665, Avg Loss: 1.5528\n",
      "Epoch 86, Average Loss: 1.5729\n",
      "Epoch 87, Batch 0, Loss: 1.4800, Avg Loss: 1.4800\n",
      "Epoch 87, Batch 50, Loss: 1.0925, Avg Loss: 1.3437\n",
      "Epoch 87, Batch 100, Loss: 1.3328, Avg Loss: 1.3653\n",
      "Epoch 87, Batch 150, Loss: 1.9216, Avg Loss: 1.3983\n",
      "Epoch 87, Batch 200, Loss: 1.2958, Avg Loss: 1.4090\n",
      "Epoch 87, Batch 250, Loss: 1.6872, Avg Loss: 1.4292\n",
      "Epoch 87, Batch 300, Loss: 1.3189, Avg Loss: 1.4423\n",
      "Epoch 87, Batch 350, Loss: 1.7398, Avg Loss: 1.4538\n",
      "Epoch 87, Batch 400, Loss: 1.7539, Avg Loss: 1.4624\n",
      "Epoch 87, Batch 450, Loss: 1.5920, Avg Loss: 1.4731\n",
      "Epoch 87, Average Loss: 1.4841\n",
      "Epoch 88, Batch 0, Loss: 1.0292, Avg Loss: 1.0292\n",
      "Epoch 88, Batch 50, Loss: 1.3746, Avg Loss: 1.2774\n",
      "Epoch 88, Batch 100, Loss: 1.1730, Avg Loss: 1.2751\n",
      "Epoch 88, Batch 150, Loss: 1.8337, Avg Loss: 1.3028\n",
      "Epoch 88, Batch 200, Loss: 1.4836, Avg Loss: 1.3242\n",
      "Epoch 88, Batch 250, Loss: 1.3642, Avg Loss: 1.3394\n",
      "Epoch 88, Batch 300, Loss: 1.5948, Avg Loss: 1.3747\n",
      "Epoch 88, Batch 350, Loss: 1.3232, Avg Loss: 1.3936\n",
      "Epoch 88, Batch 400, Loss: 1.4987, Avg Loss: 1.4042\n",
      "Epoch 88, Batch 450, Loss: 1.5994, Avg Loss: 1.4156\n",
      "Epoch 88, Average Loss: 1.4282\n",
      "Epoch 89, Batch 0, Loss: 1.1985, Avg Loss: 1.1985\n",
      "Epoch 89, Batch 50, Loss: 1.0366, Avg Loss: 1.1582\n",
      "Epoch 89, Batch 100, Loss: 1.3859, Avg Loss: 1.1895\n",
      "Epoch 89, Batch 150, Loss: 1.4903, Avg Loss: 1.2151\n",
      "Epoch 89, Batch 200, Loss: 1.3590, Avg Loss: 1.2384\n",
      "Epoch 89, Batch 250, Loss: 1.3178, Avg Loss: 1.2554\n",
      "Epoch 89, Batch 300, Loss: 1.3868, Avg Loss: 1.2762\n",
      "Epoch 89, Batch 350, Loss: 1.3844, Avg Loss: 1.2954\n",
      "Epoch 89, Batch 400, Loss: 1.2968, Avg Loss: 1.3095\n",
      "Epoch 89, Batch 450, Loss: 1.6104, Avg Loss: 1.3264\n",
      "Epoch 89, Average Loss: 1.3416\n",
      "Epoch 90, Batch 0, Loss: 1.0179, Avg Loss: 1.0179\n",
      "Epoch 90, Batch 50, Loss: 0.8570, Avg Loss: 1.1649\n",
      "Epoch 90, Batch 100, Loss: 1.0872, Avg Loss: 1.1844\n",
      "Epoch 90, Batch 150, Loss: 1.1866, Avg Loss: 1.1857\n",
      "Epoch 90, Batch 200, Loss: 1.3024, Avg Loss: 1.2087\n",
      "Epoch 90, Batch 250, Loss: 1.2455, Avg Loss: 1.2250\n",
      "Epoch 90, Batch 300, Loss: 1.3273, Avg Loss: 1.2371\n",
      "Epoch 90, Batch 350, Loss: 1.6967, Avg Loss: 1.2485\n",
      "Epoch 90, Batch 400, Loss: 1.2924, Avg Loss: 1.2677\n",
      "Epoch 90, Batch 450, Loss: 1.1863, Avg Loss: 1.2786\n",
      "Epoch 90, Average Loss: 1.2919\n",
      "Epoch 91, Batch 0, Loss: 0.9594, Avg Loss: 0.9594\n",
      "Epoch 91, Batch 50, Loss: 1.0225, Avg Loss: 1.0784\n",
      "Epoch 91, Batch 100, Loss: 1.1875, Avg Loss: 1.1229\n",
      "Epoch 91, Batch 150, Loss: 0.9889, Avg Loss: 1.1355\n",
      "Epoch 91, Batch 200, Loss: 1.2953, Avg Loss: 1.1462\n",
      "Epoch 91, Batch 250, Loss: 1.2959, Avg Loss: 1.1637\n",
      "Epoch 91, Batch 300, Loss: 1.1574, Avg Loss: 1.1786\n",
      "Epoch 91, Batch 350, Loss: 1.4129, Avg Loss: 1.1943\n",
      "Epoch 91, Batch 400, Loss: 1.4887, Avg Loss: 1.2116\n",
      "Epoch 91, Batch 450, Loss: 1.3428, Avg Loss: 1.2184\n",
      "Epoch 91, Average Loss: 1.2272\n",
      "Epoch 92, Batch 0, Loss: 1.0322, Avg Loss: 1.0322\n",
      "Epoch 92, Batch 50, Loss: 1.1602, Avg Loss: 1.0256\n",
      "Epoch 92, Batch 100, Loss: 0.9893, Avg Loss: 1.0615\n",
      "Epoch 92, Batch 150, Loss: 1.1212, Avg Loss: 1.0846\n",
      "Epoch 92, Batch 200, Loss: 1.1512, Avg Loss: 1.1032\n",
      "Epoch 92, Batch 250, Loss: 0.9470, Avg Loss: 1.1172\n",
      "Epoch 92, Batch 300, Loss: 1.4704, Avg Loss: 1.1311\n",
      "Epoch 92, Batch 350, Loss: 1.3173, Avg Loss: 1.1480\n",
      "Epoch 92, Batch 400, Loss: 1.3013, Avg Loss: 1.1541\n",
      "Epoch 92, Batch 450, Loss: 1.3632, Avg Loss: 1.1663\n",
      "Epoch 92, Average Loss: 1.1797\n",
      "Epoch 93, Batch 0, Loss: 1.0807, Avg Loss: 1.0807\n",
      "Epoch 93, Batch 50, Loss: 1.0200, Avg Loss: 1.0059\n",
      "Epoch 93, Batch 100, Loss: 1.4355, Avg Loss: 1.0173\n",
      "Epoch 93, Batch 150, Loss: 1.1020, Avg Loss: 1.0198\n",
      "Epoch 93, Batch 200, Loss: 1.2131, Avg Loss: 1.0339\n",
      "Epoch 93, Batch 250, Loss: 1.0233, Avg Loss: 1.0464\n",
      "Epoch 93, Batch 300, Loss: 1.2085, Avg Loss: 1.0646\n",
      "Epoch 93, Batch 350, Loss: 1.3157, Avg Loss: 1.0800\n",
      "Epoch 93, Batch 400, Loss: 0.9428, Avg Loss: 1.0909\n",
      "Epoch 93, Batch 450, Loss: 1.2105, Avg Loss: 1.1029\n",
      "Epoch 93, Average Loss: 1.1153\n",
      "Epoch 94, Batch 0, Loss: 0.8930, Avg Loss: 0.8930\n",
      "Epoch 94, Batch 50, Loss: 0.8105, Avg Loss: 0.9079\n",
      "Epoch 94, Batch 100, Loss: 0.9487, Avg Loss: 0.9572\n",
      "Epoch 94, Batch 150, Loss: 0.9077, Avg Loss: 0.9781\n",
      "Epoch 94, Batch 200, Loss: 1.0819, Avg Loss: 0.9851\n",
      "Epoch 94, Batch 250, Loss: 1.0906, Avg Loss: 0.9916\n",
      "Epoch 94, Batch 300, Loss: 1.1287, Avg Loss: 1.0041\n",
      "Epoch 94, Batch 350, Loss: 1.2913, Avg Loss: 1.0235\n",
      "Epoch 94, Batch 400, Loss: 1.1183, Avg Loss: 1.0440\n",
      "Epoch 94, Batch 450, Loss: 1.0900, Avg Loss: 1.0584\n",
      "Epoch 94, Average Loss: 1.0672\n",
      "Epoch 95, Batch 0, Loss: 1.0706, Avg Loss: 1.0706\n",
      "Epoch 95, Batch 50, Loss: 0.7868, Avg Loss: 0.8985\n",
      "Epoch 95, Batch 100, Loss: 0.7681, Avg Loss: 0.9079\n",
      "Epoch 95, Batch 150, Loss: 0.7744, Avg Loss: 0.9295\n",
      "Epoch 95, Batch 200, Loss: 0.8227, Avg Loss: 0.9501\n",
      "Epoch 95, Batch 250, Loss: 1.1830, Avg Loss: 0.9614\n",
      "Epoch 95, Batch 300, Loss: 0.8730, Avg Loss: 0.9749\n",
      "Epoch 95, Batch 350, Loss: 1.0027, Avg Loss: 0.9884\n",
      "Epoch 95, Batch 400, Loss: 1.2640, Avg Loss: 1.0018\n",
      "Epoch 95, Batch 450, Loss: 1.1356, Avg Loss: 1.0111\n",
      "Epoch 95, Average Loss: 1.0212\n",
      "Epoch 96, Batch 0, Loss: 0.7898, Avg Loss: 0.7898\n",
      "Epoch 96, Batch 50, Loss: 0.7317, Avg Loss: 0.8618\n",
      "Epoch 96, Batch 100, Loss: 0.9865, Avg Loss: 0.8508\n",
      "Epoch 96, Batch 150, Loss: 0.9800, Avg Loss: 0.8784\n",
      "Epoch 96, Batch 200, Loss: 0.9291, Avg Loss: 0.9024\n",
      "Epoch 96, Batch 250, Loss: 0.8044, Avg Loss: 0.9258\n",
      "Epoch 96, Batch 300, Loss: 0.7712, Avg Loss: 0.9351\n",
      "Epoch 96, Batch 350, Loss: 0.8562, Avg Loss: 0.9491\n",
      "Epoch 96, Batch 400, Loss: 1.2572, Avg Loss: 0.9601\n",
      "Epoch 96, Batch 450, Loss: 0.9894, Avg Loss: 0.9727\n",
      "Epoch 96, Average Loss: 0.9848\n",
      "Epoch 97, Batch 0, Loss: 0.7624, Avg Loss: 0.7624\n",
      "Epoch 97, Batch 50, Loss: 0.8058, Avg Loss: 0.8055\n",
      "Epoch 97, Batch 100, Loss: 0.8440, Avg Loss: 0.8418\n",
      "Epoch 97, Batch 150, Loss: 0.8825, Avg Loss: 0.8552\n",
      "Epoch 97, Batch 200, Loss: 0.8130, Avg Loss: 0.8660\n",
      "Epoch 97, Batch 250, Loss: 0.9056, Avg Loss: 0.8819\n",
      "Epoch 97, Batch 300, Loss: 0.9256, Avg Loss: 0.8945\n",
      "Epoch 97, Batch 350, Loss: 0.8866, Avg Loss: 0.9081\n",
      "Epoch 97, Batch 400, Loss: 1.0138, Avg Loss: 0.9185\n",
      "Epoch 97, Batch 450, Loss: 1.4103, Avg Loss: 0.9287\n",
      "Epoch 97, Average Loss: 0.9351\n",
      "Epoch 98, Batch 0, Loss: 0.7258, Avg Loss: 0.7258\n",
      "Epoch 98, Batch 50, Loss: 0.9390, Avg Loss: 0.7498\n",
      "Epoch 98, Batch 100, Loss: 0.8839, Avg Loss: 0.7973\n",
      "Epoch 98, Batch 150, Loss: 0.7577, Avg Loss: 0.8226\n",
      "Epoch 98, Batch 200, Loss: 1.1517, Avg Loss: 0.8371\n",
      "Epoch 98, Batch 250, Loss: 1.2186, Avg Loss: 0.8517\n",
      "Epoch 98, Batch 300, Loss: 0.9911, Avg Loss: 0.8648\n",
      "Epoch 98, Batch 350, Loss: 1.0363, Avg Loss: 0.8806\n",
      "Epoch 98, Batch 400, Loss: 0.8473, Avg Loss: 0.8903\n",
      "Epoch 98, Batch 450, Loss: 0.7839, Avg Loss: 0.9009\n",
      "Epoch 98, Average Loss: 0.9075\n",
      "Epoch 99, Batch 0, Loss: 0.6654, Avg Loss: 0.6654\n",
      "Epoch 99, Batch 50, Loss: 0.6943, Avg Loss: 0.7865\n",
      "Epoch 99, Batch 100, Loss: 1.0109, Avg Loss: 0.7847\n",
      "Epoch 99, Batch 150, Loss: 0.6905, Avg Loss: 0.7945\n",
      "Epoch 99, Batch 200, Loss: 0.7035, Avg Loss: 0.8164\n",
      "Epoch 99, Batch 250, Loss: 0.8199, Avg Loss: 0.8294\n",
      "Epoch 99, Batch 300, Loss: 0.8615, Avg Loss: 0.8325\n",
      "Epoch 99, Batch 350, Loss: 0.8659, Avg Loss: 0.8464\n",
      "Epoch 99, Batch 400, Loss: 0.8694, Avg Loss: 0.8569\n",
      "Epoch 99, Batch 450, Loss: 1.1161, Avg Loss: 0.8605\n",
      "Epoch 99, Average Loss: 0.8739\n",
      "Epoch 100, Batch 0, Loss: 0.7544, Avg Loss: 0.7544\n",
      "Epoch 100, Batch 50, Loss: 0.6342, Avg Loss: 0.7006\n",
      "Epoch 100, Batch 100, Loss: 0.8791, Avg Loss: 0.7280\n",
      "Epoch 100, Batch 150, Loss: 0.6978, Avg Loss: 0.7300\n",
      "Epoch 100, Batch 200, Loss: 0.7634, Avg Loss: 0.7603\n",
      "Epoch 100, Batch 250, Loss: 0.8254, Avg Loss: 0.7747\n",
      "Epoch 100, Batch 300, Loss: 0.8655, Avg Loss: 0.7934\n",
      "Epoch 100, Batch 350, Loss: 0.8272, Avg Loss: 0.8085\n",
      "Epoch 100, Batch 400, Loss: 0.8842, Avg Loss: 0.8190\n",
      "Epoch 100, Batch 450, Loss: 0.8882, Avg Loss: 0.8248\n",
      "Epoch 100, Average Loss: 0.8403\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader,start_epoch=50, num_epochs=100, device=device, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f40c452-5a9f-4ed1-b3e7-c3d1afb259e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (pos_encoding): PositionalEncoding()\n",
       "  (layers): ModuleList(\n",
       "    (0-79): 80 x TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=384, out_features=4000, bias=True)\n",
       "        (fc2): Linear(in_features=4000, out_features=384, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=384, out_features=32000, bias=True)\n",
       "  (input_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a3e516-1269-4092-9b3a-4f8c80d410c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_30376\\1596221018.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_30376\\1596221018.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 12.0280, Avg Loss: 12.0280\n",
      "GPU Memory: 2603.6MB / 3276.0MB\n",
      "Epoch 1, Batch 10, Loss: 12.0386, Avg Loss: 12.1386\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 20, Loss: 11.8560, Avg Loss: 12.1141\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 30, Loss: 12.1311, Avg Loss: 12.1016\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 40, Loss: 12.0026, Avg Loss: 12.0856\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 50, Loss: 11.9014, Avg Loss: 12.0653\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 60, Loss: 11.7528, Avg Loss: 12.0317\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 70, Loss: 11.6829, Avg Loss: 11.9879\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 80, Loss: 11.7992, Avg Loss: 11.9480\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 90, Loss: 11.3347, Avg Loss: 11.9002\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 100, Loss: 11.4087, Avg Loss: 11.8494\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 110, Loss: 11.1728, Avg Loss: 11.7848\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 120, Loss: 10.9549, Avg Loss: 11.7175\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 130, Loss: 10.7283, Avg Loss: 11.6489\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 140, Loss: 10.6770, Avg Loss: 11.5787\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 150, Loss: 10.3783, Avg Loss: 11.5045\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 160, Loss: 10.1013, Avg Loss: 11.4318\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 170, Loss: 10.0589, Avg Loss: 11.3568\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 180, Loss: 10.2103, Avg Loss: 11.2833\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 190, Loss: 9.7784, Avg Loss: 11.2133\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 200, Loss: 9.6961, Avg Loss: 11.1417\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 210, Loss: 9.5640, Avg Loss: 11.0718\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 220, Loss: 9.5621, Avg Loss: 11.0060\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 230, Loss: 9.7804, Avg Loss: 10.9427\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 240, Loss: 9.6736, Avg Loss: 10.8825\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 250, Loss: 9.2701, Avg Loss: 10.8245\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 260, Loss: 9.8574, Avg Loss: 10.7727\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 270, Loss: 9.4439, Avg Loss: 10.7240\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 280, Loss: 9.5257, Avg Loss: 10.6748\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 290, Loss: 9.5533, Avg Loss: 10.6309\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 300, Loss: 9.6061, Avg Loss: 10.5868\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 310, Loss: 9.4245, Avg Loss: 10.5454\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 320, Loss: 9.4780, Avg Loss: 10.5069\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 330, Loss: 9.4528, Avg Loss: 10.4662\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 340, Loss: 9.4844, Avg Loss: 10.4294\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 350, Loss: 9.3488, Avg Loss: 10.3960\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 360, Loss: 9.2026, Avg Loss: 10.3649\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 370, Loss: 9.0430, Avg Loss: 10.3325\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 380, Loss: 9.4855, Avg Loss: 10.3017\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 390, Loss: 9.5471, Avg Loss: 10.2731\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 400, Loss: 9.0435, Avg Loss: 10.2474\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 410, Loss: 9.2723, Avg Loss: 10.2194\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 420, Loss: 9.3423, Avg Loss: 10.1934\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 430, Loss: 8.7282, Avg Loss: 10.1649\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 440, Loss: 9.1284, Avg Loss: 10.1431\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 450, Loss: 9.4127, Avg Loss: 10.1215\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 460, Loss: 8.8441, Avg Loss: 10.0983\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 470, Loss: 8.8658, Avg Loss: 10.0749\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 480, Loss: 9.0486, Avg Loss: 10.0526\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 490, Loss: 8.9979, Avg Loss: 10.0295\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Average Loss: 10.0110\n",
      "Epoch 2, Batch 0, Loss: 9.1332, Avg Loss: 9.1332\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 10, Loss: 8.9588, Avg Loss: 9.1157\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 20, Loss: 9.0244, Avg Loss: 9.0296\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 30, Loss: 9.1494, Avg Loss: 9.0050\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 40, Loss: 8.9609, Avg Loss: 9.0023\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 50, Loss: 8.4784, Avg Loss: 8.9697\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 60, Loss: 8.8282, Avg Loss: 8.9432\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 70, Loss: 8.7322, Avg Loss: 8.9007\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 80, Loss: 8.7018, Avg Loss: 8.8912\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 90, Loss: 9.0569, Avg Loss: 8.8965\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 100, Loss: 8.7150, Avg Loss: 8.8826\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 110, Loss: 8.9251, Avg Loss: 8.8871\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 120, Loss: 8.4762, Avg Loss: 8.8711\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 130, Loss: 8.4868, Avg Loss: 8.8663\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 140, Loss: 9.0607, Avg Loss: 8.8545\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 150, Loss: 8.6040, Avg Loss: 8.8241\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 160, Loss: 8.7790, Avg Loss: 8.8222\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 170, Loss: 9.1437, Avg Loss: 8.8093\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 180, Loss: 8.8072, Avg Loss: 8.8035\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 190, Loss: 8.7041, Avg Loss: 8.7987\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 200, Loss: 8.6586, Avg Loss: 8.7907\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 210, Loss: 8.6164, Avg Loss: 8.7780\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 220, Loss: 8.3844, Avg Loss: 8.7709\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 230, Loss: 8.7267, Avg Loss: 8.7577\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 240, Loss: 8.5593, Avg Loss: 8.7474\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 250, Loss: 8.5769, Avg Loss: 8.7340\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 260, Loss: 8.7207, Avg Loss: 8.7282\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 270, Loss: 7.9567, Avg Loss: 8.7155\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 280, Loss: 8.0300, Avg Loss: 8.7041\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 290, Loss: 8.8422, Avg Loss: 8.6933\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 300, Loss: 7.8724, Avg Loss: 8.6832\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 310, Loss: 7.9845, Avg Loss: 8.6691\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 320, Loss: 8.6144, Avg Loss: 8.6619\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 330, Loss: 8.6849, Avg Loss: 8.6506\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 340, Loss: 8.1611, Avg Loss: 8.6385\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 350, Loss: 8.2115, Avg Loss: 8.6264\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 360, Loss: 8.2708, Avg Loss: 8.6125\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 370, Loss: 8.0691, Avg Loss: 8.6009\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 380, Loss: 8.1816, Avg Loss: 8.5906\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 390, Loss: 8.0424, Avg Loss: 8.5840\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 400, Loss: 8.1567, Avg Loss: 8.5739\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 410, Loss: 8.4038, Avg Loss: 8.5603\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 420, Loss: 7.8802, Avg Loss: 8.5489\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 430, Loss: 8.1288, Avg Loss: 8.5362\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 440, Loss: 8.3249, Avg Loss: 8.5234\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 450, Loss: 7.7899, Avg Loss: 8.5128\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 460, Loss: 8.2030, Avg Loss: 8.5029\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 470, Loss: 7.6255, Avg Loss: 8.4924\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 480, Loss: 7.8338, Avg Loss: 8.4823\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 490, Loss: 7.8286, Avg Loss: 8.4716\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Average Loss: 8.4629\n",
      "Epoch 3, Batch 0, Loss: 8.0841, Avg Loss: 8.0841\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 10, Loss: 7.6196, Avg Loss: 7.9038\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 20, Loss: 8.1065, Avg Loss: 7.8303\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 30, Loss: 7.7583, Avg Loss: 7.8200\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 40, Loss: 7.8081, Avg Loss: 7.8155\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 50, Loss: 7.2662, Avg Loss: 7.8457\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 60, Loss: 8.0867, Avg Loss: 7.8290\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 70, Loss: 7.9150, Avg Loss: 7.8327\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 80, Loss: 7.4257, Avg Loss: 7.8417\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 90, Loss: 7.6038, Avg Loss: 7.8153\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 100, Loss: 7.9511, Avg Loss: 7.8097\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 110, Loss: 7.5622, Avg Loss: 7.7874\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 120, Loss: 8.2328, Avg Loss: 7.7902\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 130, Loss: 7.8026, Avg Loss: 7.7780\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 140, Loss: 7.5717, Avg Loss: 7.7731\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 150, Loss: 8.0099, Avg Loss: 7.7547\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 160, Loss: 7.3849, Avg Loss: 7.7442\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 170, Loss: 7.5429, Avg Loss: 7.7406\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 180, Loss: 7.2346, Avg Loss: 7.7292\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 190, Loss: 7.9218, Avg Loss: 7.7295\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 200, Loss: 7.2877, Avg Loss: 7.7175\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 210, Loss: 7.3025, Avg Loss: 7.7034\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 220, Loss: 7.4239, Avg Loss: 7.6939\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 230, Loss: 7.7499, Avg Loss: 7.6879\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 240, Loss: 7.7408, Avg Loss: 7.6864\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 250, Loss: 7.7325, Avg Loss: 7.6765\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 260, Loss: 7.9144, Avg Loss: 7.6745\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 270, Loss: 6.5910, Avg Loss: 7.6634\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 280, Loss: 7.2278, Avg Loss: 7.6491\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 290, Loss: 7.9863, Avg Loss: 7.6487\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 300, Loss: 7.6644, Avg Loss: 7.6406\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 310, Loss: 7.2255, Avg Loss: 7.6280\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 320, Loss: 7.1546, Avg Loss: 7.6121\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 330, Loss: 7.3411, Avg Loss: 7.6054\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 340, Loss: 6.8208, Avg Loss: 7.5963\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 350, Loss: 7.0589, Avg Loss: 7.5908\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 360, Loss: 6.7512, Avg Loss: 7.5794\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 370, Loss: 7.4654, Avg Loss: 7.5672\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 380, Loss: 6.9779, Avg Loss: 7.5577\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 390, Loss: 7.5761, Avg Loss: 7.5522\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 400, Loss: 7.4723, Avg Loss: 7.5456\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 410, Loss: 6.5601, Avg Loss: 7.5367\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 420, Loss: 7.1568, Avg Loss: 7.5324\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 430, Loss: 7.4591, Avg Loss: 7.5244\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 440, Loss: 7.1673, Avg Loss: 7.5205\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 450, Loss: 6.6514, Avg Loss: 7.5097\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 460, Loss: 7.2991, Avg Loss: 7.5007\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 470, Loss: 7.3215, Avg Loss: 7.4916\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 480, Loss: 7.6190, Avg Loss: 7.4826\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 490, Loss: 7.1551, Avg Loss: 7.4769\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Average Loss: 7.4711\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader\n",
    "train_dataset = TextDataset(df, max_len=max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, num_epochs=3, device=device, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4faad0d-86a0-4c15-9f8f-3056cf3efb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the checkpoint file\n",
    "checkpoint_path = \"checkpoint.pt\"\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Load only the model weights\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "print(\"Model loaded successfully for inference!\")\n",
    "\n",
    "# Example: Run inference\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.randn(1, 1024).to(device)  # Replace with actual input\n",
    "    output = model(input_tensor)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73b3a31d-1094-4fd8-b563-8d6fdf816cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is']\n"
     ]
    }
   ],
   "source": [
    "texts = [\"WRITE A STORY \", \"STORY\"]\n",
    "embeddings = get_embeddings(texts).unsqueeze(0).to(device)\n",
    "output = model(embeddings)\n",
    "decoded_texts = model.decode(output)\n",
    "print(decoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c79b2c7-ef71-47dc-ae8f-05d970506559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Write a short story about\n",
      "Generated: ?\n",
      " forAss?<<thAss><>istantistantthAss><OkinkthOkink I>><th howink I'ay toAss I of? need for toOk' I>Ok>< to?>> fromink me need do what downink Hmm usinginks down to that LetAss start. I start off' know step I. But\n",
      " The\n",
      " I.Ok start\n",
      "?\n",
      "\n",
      " can  to that The\n",
      " it start be haves by\n",
      "\n",
      "Input: Write a poem about\n",
      "Generated: \n",
      "Ass?\n",
      " howAssAssAss?thAssAss>>thinkOkOkink>thAss><><Okth to soayistant howinkay\n",
      "Okink I so so><Ok how\n",
      " figure> Hmms makeOk> howink Lets to I to have Hmm using start byth down' I Let Let do start is me Let Hmm what? start?. means have I, down it\n",
      " to have? off. which which is it a be\n",
      " But to down.\n",
      ". can it, is? down some\n",
      "\n",
      ". because1 can. is. it But it So to, like in? can means down\n",
      " the is' for. So it That to might.\n",
      "\n",
      " to'\n",
      " That\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import List, Union\n",
    "\n",
    "class TransformerInference:\n",
    "    def __init__(self, model, tokenizer, embedding_model, device=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding_model = embedding_model\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_embeddings(self, text: Union[str, List[str]]) -> torch.Tensor:\n",
    "        \"\"\"Convert input text to embeddings.\"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        embeddings = torch.tensor(\n",
    "            self.embedding_model.encode(text), \n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        return embeddings\n",
    "\n",
    "    def pad_embeddings(self, embeddings: torch.Tensor, max_len: int = 100) -> torch.Tensor:\n",
    "        \"\"\"Pad or truncate embeddings to specified length.\"\"\"\n",
    "        batch_size, seq_len, emb_dim = embeddings.shape\n",
    "        if seq_len < max_len:\n",
    "            padding = torch.zeros(batch_size, max_len - seq_len, emb_dim, device=embeddings.device)\n",
    "            return torch.cat([embeddings, padding], dim=1)\n",
    "        return embeddings[:, :max_len, :]\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    # def generate(\n",
    "    #     self,\n",
    "    #     text: Union[str, List[str]],\n",
    "    #     max_length: int = 100,\n",
    "    #     temperature: float = 1.0,\n",
    "    #     top_k: int = 50,\n",
    "    #     top_p: float = 0.9,\n",
    "    # ) -> List[str]:\n",
    "    #     \"\"\"\n",
    "    #     Generate text from input prompt.\n",
    "        \n",
    "    #     Args:\n",
    "    #         text: Input text or list of texts\n",
    "    #         max_length: Maximum length of generated sequence\n",
    "    #         temperature: Sampling temperature (1.0 = normal, <1.0 = more focused, >1.0 = more random)\n",
    "    #         top_k: Number of highest probability tokens to consider for sampling\n",
    "    #         top_p: Cumulative probability threshold for nucleus sampling\n",
    "        \n",
    "    #     Returns:\n",
    "    #         List of generated texts\n",
    "    #     \"\"\"\n",
    "    #     # Prepare input\n",
    "    #     embeddings = self.get_embeddings(text)\n",
    "    #     embeddings = embeddings.unsqueeze(0) if len(embeddings.shape) == 2 else embeddings\n",
    "    #     embeddings = embeddings.to(self.device)\n",
    "    #     embeddings = self.pad_embeddings(embeddings)\n",
    "        \n",
    "    #     # Generate\n",
    "    #     generated_ids = []\n",
    "        \n",
    "    #     # Forward pass through model\n",
    "    #     logits = self.model(embeddings)\n",
    "        \n",
    "    #     # Apply temperature\n",
    "    #     logits = logits / temperature\n",
    "        \n",
    "    #     # Apply top-k filtering\n",
    "    #     if top_k > 0:\n",
    "    #         indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "    #         logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "    #     # Apply nucleus (top-p) filtering\n",
    "    #     if top_p < 1.0:\n",
    "    #         sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    #         cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    #         sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    #         sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    #         sorted_indices_to_remove[..., 0] = 0\n",
    "    #         indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "    #         logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "    #     # Sample from the filtered distribution\n",
    "    #     probs = torch.softmax(logits, dim=-1)\n",
    "    #     next_tokens = torch.multinomial(probs.view(-1, probs.size(-1)), num_samples=1)\n",
    "    #     generated_ids.append(next_tokens)\n",
    "        \n",
    "    #     # Decode and return results\n",
    "    #     generated_ids = torch.cat(generated_ids, dim=-1)\n",
    "    #     generated_texts = [\n",
    "    #         self.tokenizer.decode(ids, skip_special_tokens=True)\n",
    "    #         for ids in generated_ids\n",
    "    #     ]\n",
    "        \n",
    "    #     return generated_texts\n",
    "\n",
    "\n",
    "    # multiple words.\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        text: Union[str, List[str]],\n",
    "        max_length: int = 100,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 0.9,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate text from input prompt.\n",
    "        \"\"\"\n",
    "        # Prepare input\n",
    "        embeddings = self.get_embeddings(text)\n",
    "        embeddings = embeddings.unsqueeze(0) if len(embeddings.shape) == 2 else embeddings\n",
    "        embeddings = embeddings.to(self.device)\n",
    "        embeddings = self.pad_embeddings(embeddings)\n",
    "    \n",
    "        generated_ids = []\n",
    "    \n",
    "        for _ in range(max_length):  # Iterate to generate more tokens\n",
    "            logits = self.model(embeddings)[:, -1, :]  # Only take last token logits\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "    \n",
    "            # Apply top-k filtering\n",
    "            if top_k > 0:\n",
    "                values, indices = torch.topk(logits, top_k)\n",
    "                min_values = values[..., -1, None]\n",
    "                logits[logits < min_values] = float('-inf')\n",
    "    \n",
    "            # Apply nucleus (top-p) filtering\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "    \n",
    "            # Sample next token\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "            # Append generated token\n",
    "            generated_ids.append(next_token)\n",
    "    \n",
    "            # Convert token ID to embeddings and feed back into the model\n",
    "            token_embedding = self.embedding_model.encode([self.tokenizer.decode(next_token.item())], convert_to_tensor=True)\n",
    "            token_embedding = token_embedding.unsqueeze(0).to(self.device)\n",
    "            embeddings = torch.cat([embeddings, token_embedding], dim=1)\n",
    "    \n",
    "        # Decode generated tokens\n",
    "        generated_ids = torch.cat(generated_ids, dim=-1)\n",
    "        generated_texts = [\n",
    "            self.tokenizer.decode(ids.tolist(), skip_special_tokens=True)\n",
    "            for ids in generated_ids\n",
    "        ]\n",
    "    \n",
    "        return generated_texts\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.generate(*args, **kwargs)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming model, tokenizer, and embedding_model are already defined\n",
    "    inference = TransformerInference(model, tokenizer, embedding_model)\n",
    "    \n",
    "    # Single text generation\n",
    "    prompt = \"Write a short story about\"\n",
    "    generated_text = inference(prompt)\n",
    "    print(f\"Input: {prompt}\")\n",
    "    print(f\"Generated: {generated_text[0]}\")\n",
    "    \n",
    "    # Batch generation\n",
    "    prompts = [\n",
    "        \"Write a poem about\",\n",
    "        \"Explain how to\",\n",
    "        \"Tell me about\"\n",
    "    ]\n",
    "    generated_texts = inference(\n",
    "        prompts,\n",
    "        max_length=150,\n",
    "        temperature=0.8,\n",
    "        top_k=40,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    for prompt, generated in zip(prompts, generated_texts):\n",
    "        print(f\"\\nInput: {prompt}\")\n",
    "        print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17af5548-cca3-422e-84ba-9f491c1f8dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Write a short story about\n",
      "Generated: Brow to that of a. to. to.. expression it is eterIll VI VerIll  isété is a Artikel. of. that.:秀 . entfer. the of ,.0 ben Brow-ά the. elements Based the elements b simultaneously children. step tov Verété, step '.util that. the.. So in bazie Ver0 a voices. is  is that. thatété in in.. brick ers. is, happ' in is of\n",
      "\n",
      "Input: Write a poem about\n",
      "Generated: voices . Zar is秀....,.. VideosIll is is, iseter is, Brow aIll.\n",
      " the на toIll на is объек. is is秀.. Ver Brow\n",
      " to matplotlib. elements...\n",
      ".\n",
      ".. is- step \n",
      ".- the to\n",
      " is.... simultaneously.,0 to. на. is is....0, in ..\n",
      " al. So.... it..ally and... step is console the, in is is.. is.. the..\n",
      " crown is,.. the,.\n",
      ",  toally   the\n",
      " \n",
      " thatutil So.....\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    self,\n",
    "    text: Union[str, List[str]],\n",
    "    max_length: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 0.9,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate multiple words instead of just one.\n",
    "    \"\"\"\n",
    "    # Convert input text to token IDs\n",
    "    input_ids = self.tokenizer.encode(text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Convert token IDs to embeddings\n",
    "        embeddings = self.get_embeddings(self.tokenizer.decode(input_ids[0]))\n",
    "\n",
    "        # Ensure correct shape\n",
    "        embeddings = embeddings.unsqueeze(0).to(self.device)\n",
    "        embeddings = self.pad_embeddings(embeddings)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = self.model(embeddings)[:, -1, :]  # Get last token logits\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Apply top-k filtering\n",
    "        if top_k > 0:\n",
    "            values, indices = torch.topk(logits, top_k)\n",
    "            min_values = values[..., -1, None]\n",
    "            logits[logits < min_values] = float('-inf')\n",
    "\n",
    "        # Apply nucleus (top-p) filtering\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "        # Sample the next token\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append token to input_ids\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # Stop if EOS token is generated\n",
    "        if next_token.item() == self.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Decode final text\n",
    "    generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return [generated_text]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming model, tokenizer, and embedding_model are already defined\n",
    "    inference = TransformerInference(model, tokenizer, embedding_model)\n",
    "    \n",
    "    # Single text generation\n",
    "    prompt = \"Write a short story about\"\n",
    "    generated_text = inference(prompt)\n",
    "    print(f\"Input: {prompt}\")\n",
    "    print(f\"Generated: {generated_text[0]}\")\n",
    "    \n",
    "    # Batch generation\n",
    "    prompts = [\n",
    "        \"Write a poem about\",\n",
    "        \"Explain how to\",\n",
    "        \"Tell me about\"\n",
    "    ]\n",
    "    generated_texts = inference(\n",
    "        prompts,\n",
    "        max_length=150,\n",
    "        temperature=0.8,\n",
    "        top_k=40,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    for prompt, generated in zip(prompts, generated_texts):\n",
    "        print(f\"\\nInput: {prompt}\")\n",
    "        print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c11b61-a9a0-4a6a-b8ec-d2e8408e87f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96cce5-540a-4688-b7e2-4ead1f356dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50f111-025c-4c64-ac1e-cf1c168e7b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "785e19c2-7e47-49b4-933d-ff53801c3f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model and other components\n",
    "inference = TransformerInference(model, tokenizer, embedding_model)\n",
    "\n",
    "# Generate from a single prompt\n",
    "text = inference(\"Write a story about\", max_length=100, temperature=0.8)\n",
    "\n",
    "# Or generate from multiple prompts\n",
    "texts = inference([\n",
    "    \"Write a story about\",\n",
    "    \"Explain how to\"\n",
    "], max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e3fafde-b98c-45d5-81c6-ebb9e0965d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['då방深 mentions tasks another \" Each tasks mentions ofpressionној an\\nargv深深 developing深 mentions perhaps of mut abouttest many one mentions aboutEND easy\\n phone\\' music many phone music\\' theној music Each about music \" musicyle could Smith pending. anotherној Each could mentions mentions Each Smith He about especiallyној2ној about Eachној of music manyx\\n.emplo Smith Smith especially\\nblem \" a aboutpow\\n so about I many step0 another\\' mut mentions I perhaps mentions']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c70076-55da-439d-a1c6-5283179ce00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1c8eb9e-fd22-4306-8b09-bf6db04692c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters per transformer layer: 27,548,416\n",
      "Total parameters: 2,229,073,408\n",
      "Total parameters in millions: 2229.07M\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(d_model=768, num_heads=12, hidden_dim=3072, num_layers=12, vocab_size=32000):\n",
    "    \"\"\"Calculate parameter count for each component of the transformer model\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        \"Input Projection Layer\": {\n",
    "            \"weight\": d_model * d_model,\n",
    "            \"bias\": d_model\n",
    "        },\n",
    "        \n",
    "        \"Positional Encoding\": {\n",
    "            \"parameters\": 0  # No trainable parameters\n",
    "        },\n",
    "        \n",
    "        \"Per Layer\": {\n",
    "            \"Self-Attention\": {\n",
    "                \"qkv_proj\": {\n",
    "                    \"weight\": d_model * (d_model * 3),\n",
    "                    \"bias\": d_model * 3\n",
    "                },\n",
    "                \"out_proj\": {\n",
    "                    \"weight\": d_model * d_model,\n",
    "                    \"bias\": d_model\n",
    "                }\n",
    "            },\n",
    "            \"Feed Forward\": {\n",
    "                \"fc1\": {\n",
    "                    \"weight\": d_model * hidden_dim,\n",
    "                    \"bias\": hidden_dim\n",
    "                },\n",
    "                \"fc2\": {\n",
    "                    \"weight\": hidden_dim * d_model,\n",
    "                    \"bias\": d_model\n",
    "                }\n",
    "            },\n",
    "            \"Layer Norm (2x)\": {\n",
    "                \"weight\": d_model * 2,\n",
    "                \"bias\": d_model * 2\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        \"Final Layer Norm\": {\n",
    "            \"weight\": d_model,\n",
    "            \"bias\": d_model\n",
    "        },\n",
    "        \n",
    "        \"Output Layer\": {\n",
    "            \"weight\": d_model * vocab_size,\n",
    "            \"bias\": vocab_size\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate total parameters per layer\n",
    "    params_per_layer = (\n",
    "        # Self-attention\n",
    "        (d_model * (d_model * 3) + d_model * 3) +  # qkv_proj\n",
    "        (d_model * d_model + d_model) +            # out_proj\n",
    "        # Feed forward\n",
    "        (d_model * hidden_dim + hidden_dim) +      # fc1\n",
    "        (hidden_dim * d_model + d_model) +         # fc2\n",
    "        # Layer norms\n",
    "        (d_model * 2 + d_model * 2)               # 2 layer norms with weights and biases\n",
    "    )\n",
    "    \n",
    "    # Calculate total parameters\n",
    "    total_params = (\n",
    "        # Input projection\n",
    "        (d_model * d_model + d_model) +\n",
    "        # All transformer layers\n",
    "        (params_per_layer * num_layers) +\n",
    "        # Final layer norm\n",
    "        (d_model * 2) +\n",
    "        # Output layer\n",
    "        (d_model * vocab_size + vocab_size)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"Parameters per layer\": params_per_layer,\n",
    "        \"Total parameters\": total_params,\n",
    "        \"Parameters in millions\": total_params / 1_000_000\n",
    "    }\n",
    "\n",
    "# Calculate parameters with current configuration\n",
    "results = count_parameters(d_model=768, num_heads=12, hidden_dim=16384, num_layers=80, vocab_size=32000)\n",
    "\n",
    "print(f\"Parameters per transformer layer: {results['Parameters per layer']:,}\")\n",
    "print(f\"Total parameters: {results['Total parameters']:,}\")\n",
    "print(f\"Total parameters in millions: {results['Parameters in millions']:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a0a594-75ee-40ab-ad4b-a678a109ccc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model parameters: 110.25M\n",
      "Scaled model parameters: 2229.07M\n"
     ]
    }
   ],
   "source": [
    "# Original parameters\n",
    "original_params = {\n",
    "    \"d_model\": 768,      # Keep this from embedding model\n",
    "    \"num_heads\": 12,\n",
    "    \"hidden_dim\": 3072,\n",
    "    \"num_layers\": 12,\n",
    "    \"vocab_size\": 32000\n",
    "}\n",
    "\n",
    "# Scaled up parameters (closer to LLaMA scale)\n",
    "scaled_params = {\n",
    "    \"d_model\": 768,              # Kept from embedding model\n",
    "    \"num_heads\": 32,             # Increased from 12\n",
    "    \"hidden_dim\": 16384,         # Significantly increased from 3072\n",
    "    \"num_layers\": 80,            # Significantly increased from 12\n",
    "    \"vocab_size\": 32000,         # LLaMA uses 32k vocab\n",
    "    \"max_len\": 2048,             # Increased context length\n",
    "    \"dropout_rate\": 0.1\n",
    "}\n",
    "\n",
    "def calculate_parameter_count(params):\n",
    "    d_model = params[\"d_model\"]\n",
    "    hidden_dim = params[\"hidden_dim\"]\n",
    "    num_layers = params[\"num_layers\"]\n",
    "    vocab_size = params[\"vocab_size\"]\n",
    "    \n",
    "    # Parameters per layer\n",
    "    attention_params = (d_model * (d_model * 3) + d_model * 3) + (d_model * d_model + d_model)\n",
    "    ffn_params = (d_model * hidden_dim + hidden_dim) + (hidden_dim * d_model + d_model)\n",
    "    layer_norm_params = 4 * d_model  # 2 layer norms per layer\n",
    "    \n",
    "    params_per_layer = attention_params + ffn_params + layer_norm_params\n",
    "    \n",
    "    # Total parameters\n",
    "    total_params = (\n",
    "        (d_model * d_model + d_model) +  # Input projection\n",
    "        (params_per_layer * num_layers) + # All transformer layers\n",
    "        (d_model * 2) +                   # Final layer norm\n",
    "        (d_model * vocab_size + vocab_size) # Output layer\n",
    "    )\n",
    "    \n",
    "    return total_params / 1_000_000  # Return in millions\n",
    "\n",
    "print(f\"Original model parameters: {calculate_parameter_count(original_params):.2f}M\")\n",
    "print(f\"Scaled model parameters: {calculate_parameter_count(scaled_params):.2f}M\")\n",
    "\n",
    "# Modified training setup for the scaled model\n",
    "def get_scaled_training_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,                    # Reduced due to model size\n",
    "        \"gradient_accumulation_steps\": 32,   # Increased for effective larger batch\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"warmup_steps\": 2000,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"num_epochs\": 3,\n",
    "        \"optimizer\": \"AdaFactor\",           # Changed from AdamW for memory efficiency\n",
    "        \"lr_scheduler\": \"cosine_with_warmup\"\n",
    "    }\n",
    "\n",
    "# Example training configuration with optimizations for large scale\n",
    "training_config = \"\"\"\n",
    "# Training setup for scaled model\n",
    "optimizer = transformers.Adafactor(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    eps=(1e-30, 1e-3),\n",
    "    clip_threshold=1.0,\n",
    "    decay_rate=-0.8,\n",
    "    beta1=None,\n",
    "    scale_parameter=True,\n",
    "    relative_step=True,\n",
    "    warmup_init=True\n",
    ")\n",
    "\n",
    "# Gradient accumulation setup\n",
    "gradient_accumulation_steps = 32\n",
    "model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
    "\n",
    "# Use mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Training loop with optimizations\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss = model(batch) / gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90b05c-a9ea-4752-93f4-b506585320b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e0145-7207-4455-ba42-300fd6ea2684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a171f5-1589-42f1-9ad7-9f961397cbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f8bfb-c696-4902-8635-35568dd464e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276914b-f464-42b1-af8e-e61bb886c0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb224e-1f9b-4fe3-af54-69ce925cd285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e931e-967d-4fbb-a1e2-e5ae9e255751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dcf2a-9729-4157-acf0-dfccb1527a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbdbc2d-9137-4ad5-ac29-feaee19b5927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514239b-74f7-4c4b-abc1-d741b25e235c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78294f6e-048c-4170-bc45-1991e3a88b74",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "X1 and X2 must have the same device type. X1: cuda X2: cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m get_embeddings(texts)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    104\u001b[0m output \u001b[38;5;241m=\u001b[39m model(embeddings)\n\u001b[1;32m--> 105\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_nearest_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoded output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, decoded_text)\n",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m, in \u001b[0;36mdecode_nearest_embedding\u001b[1;34m(embeddings)\u001b[0m\n\u001b[0;32m     16\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mvocab_size)))\n\u001b[0;32m     17\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m get_embeddings(all_tokens)\n\u001b[1;32m---> 18\u001b[0m similarities \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m closest_indices \u001b[38;5;241m=\u001b[39m similarities\u001b[38;5;241m.\u001b[39margmin(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(idx) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m closest_indices]\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\functional.py:1478\u001b[0m, in \u001b[0;36mcdist\u001b[1;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[0;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   1475\u001b[0m         cdist, (x1, x2), x1, x2, p\u001b[38;5;241m=\u001b[39mp, compute_mode\u001b[38;5;241m=\u001b[39mcompute_mode\n\u001b[0;32m   1476\u001b[0m     )\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist_if_necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mcdist(x1, x2, p, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: X1 and X2 must have the same device type. X1: cuda X2: cpu"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load LLaMA tokenizer and MiniLM embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    return torch.tensor(embedding_model.encode(texts), dtype=torch.float32)\n",
    "\n",
    "def decode_nearest_embedding(embeddings):\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(list(range(tokenizer.vocab_size)))\n",
    "    all_embeddings = get_embeddings(all_tokens)\n",
    "    similarities = torch.cdist(embeddings, all_embeddings)\n",
    "    closest_indices = similarities.argmin(dim=-1)\n",
    "    return [tokenizer.decode(idx) for idx in closest_indices]\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Multi-Head Self-Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv_proj(x).reshape(B, T, 3, self.num_heads, self.d_k).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = (attn_weights @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "# Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# Full Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim, num_layers, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_model, num_heads, hidden_dim) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Example usage\n",
    "d_model = 384  # MiniLM embedding size\n",
    "num_heads = 8\n",
    "hidden_dim = 1536\n",
    "num_layers = 6\n",
    "max_len = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Transformer(d_model, num_heads, hidden_dim, num_layers, max_len).to(device)\n",
    "texts = [\"Hello, world!\", \"How are you?\"]\n",
    "embeddings = get_embeddings(texts).unsqueeze(0).to(device)\n",
    "output = model(embeddings)\n",
    "decoded_text = decode_nearest_embedding(output.squeeze(0))\n",
    "print(\"Decoded output:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac716b7f-2225-4aba-9a43-3582018c5107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
