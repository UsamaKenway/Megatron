{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "863e5462-1f92-4989-a98a-0edec609e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting speech emotion recognition model training...\n",
      "Using device: cuda\n",
      "Train dataset: Dataset({\n",
      "    features: ['name', 'path', 'emotion', 'text'],\n",
      "    num_rows: 768\n",
      "})\n",
      "Evaluation dataset: Dataset({\n",
      "    features: ['name', 'path', 'emotion', 'text'],\n",
      "    num_rows: 192\n",
      "})\n",
      "Classification problem with 5 classes: ['anger', 'disgust', 'fear', 'happiness', 'sadness']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847aafaec86545549f62df87df84bff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/262 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ef9f64bb32a432aa60b4df97d38640d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10cfd7117ff44cb899fd3dad6a3328c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/300 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b529c9a7bfd4483af12f0b6ff98d5a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sampling rate: 16000\n",
      "Preprocessing train dataset...\n",
      "Preprocessing evaluation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_19832\\603617397.py:411: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CTCTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CTCTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 07:48, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.414100</td>\n",
       "      <td>1.269033</td>\n",
       "      <td>0.526042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.737600</td>\n",
       "      <td>0.936295</td>\n",
       "      <td>0.651042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.570700</td>\n",
       "      <td>0.618982</td>\n",
       "      <td>0.796875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.413200</td>\n",
       "      <td>0.381633</td>\n",
       "      <td>0.885417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.383807</td>\n",
       "      <td>0.911458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.873000</td>\n",
       "      <td>0.292262</td>\n",
       "      <td>0.927083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.436600</td>\n",
       "      <td>0.282031</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Training metrics: {'train_runtime': 471.7046, 'train_samples_per_second': 13.025, 'train_steps_per_second': 0.322, 'total_flos': 7.734316525162577e+17, 'train_loss': 2.2675500719955095, 'epoch': 7.623376623376624}\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'eval_loss': 0.30156782269477844, 'eval_accuracy': 0.9270833134651184, 'eval_runtime': 10.8425, 'eval_samples_per_second': 17.708, 'eval_steps_per_second': 1.845, 'epoch': 7.623376623376624}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.92      0.92      0.92        38\n",
      "     disgust       0.95      0.90      0.92        39\n",
      "        fear       1.00      0.82      0.90        38\n",
      "   happiness       0.91      1.00      0.95        39\n",
      "     sadness       0.88      1.00      0.94        38\n",
      "\n",
      "    accuracy                           0.93       192\n",
      "   macro avg       0.93      0.93      0.93       192\n",
      "weighted avg       0.93      0.93      0.93       192\n",
      "\n",
      "Saving model to ./emotion_recognition_model\n",
      "Model training and saving completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Complete script for speech emotion recognition training\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union, Any, Tuple\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from packaging import version\n",
    "import torchaudio\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoConfig,\n",
    "    EvalPrediction,\n",
    "    is_apex_available\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "# Check for native AMP\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "# Define the classification head\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    \"\"\"\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch\n",
    "\n",
    "# # Custom trainer class\n",
    "# class CTCTrainer(Trainer):\n",
    "#     def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Perform a training step on a batch of inputs.\n",
    "#         \"\"\"\n",
    "#         model.train()\n",
    "#         inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "#         if self.use_amp:\n",
    "#             with autocast():\n",
    "#                 loss = self.compute_loss(model, inputs)\n",
    "#         else:\n",
    "#             loss = self.compute_loss(model, inputs)\n",
    "\n",
    "#         if self.args.gradient_accumulation_steps > 1:\n",
    "#             loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "#         if self.use_amp:\n",
    "#             self.scaler.scale(loss).backward()\n",
    "#         elif self.use_apex:\n",
    "#             with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "#                 scaled_loss.backward()\n",
    "#         elif self.deepspeed:\n",
    "#             self.deepspeed.backward(loss)\n",
    "#         else:\n",
    "#             loss.backward()\n",
    "\n",
    "#         return loss.detach()\n",
    "\n",
    "# class CTCTrainer(Trainer):\n",
    "#     def training_step(\n",
    "#         self, \n",
    "#         model: nn.Module, \n",
    "#         inputs: Dict[str, Union[torch.Tensor, Any]], \n",
    "#         num_items_in_batch: Optional[int] = None  # Add the new argument\n",
    "#     ) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Perform a training step on a batch of inputs.\n",
    "#         \"\"\"\n",
    "#         model.train()\n",
    "#         inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "#         if self.use_amp:\n",
    "#             with autocast():\n",
    "#                 loss = self.compute_loss(model, inputs)\n",
    "#         else:\n",
    "#             loss = self.compute_loss(model, inputs)\n",
    "\n",
    "#         if self.args.gradient_accumulation_steps > 1:\n",
    "#             loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "#         if self.use_amp:\n",
    "#             self.scaler.scale(loss).backward()\n",
    "#         elif self.use_apex:\n",
    "#             with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "#                 scaled_loss.backward()\n",
    "#         elif self.deepspeed:\n",
    "#             self.deepspeed.backward(loss)\n",
    "#         else:\n",
    "#             loss.backward()\n",
    "\n",
    "#         return loss.detach()\n",
    "class CTCTrainer(Trainer):\n",
    "    pass\n",
    "\n",
    "\n",
    "def train_emotion_recognition_model():\n",
    "    print(\"Starting speech emotion recognition model training...\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    data_files = {\n",
    "        \"train\": \"dataset/train.csv\",\n",
    "        \"validation\": \"dataset/test.csv\",\n",
    "    }\n",
    "    \n",
    "    dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "    \n",
    "    print(f\"Train dataset: {train_dataset}\")\n",
    "    print(f\"Evaluation dataset: {eval_dataset}\")\n",
    "    \n",
    "    input_column = \"path\"\n",
    "    output_column = \"emotion\"\n",
    "    \n",
    "    label_list = train_dataset.unique(output_column)\n",
    "    label_list.sort()  # Sort for determinism\n",
    "    num_labels = len(label_list)\n",
    "    print(f\"Classification problem with {num_labels} classes: {label_list}\")\n",
    "    \n",
    "    # Setup model configuration\n",
    "    model_name_or_path = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "    pooling_mode = \"mean\"\n",
    "    \n",
    "    # Create config\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        label2id={label: i for i, label in enumerate(label_list)},\n",
    "        id2label={i: label for i, label in enumerate(label_list)},\n",
    "        finetuning_task=\"wav2vec2_clf\",\n",
    "    )\n",
    "    setattr(config, 'pooling_mode', pooling_mode)\n",
    "    \n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_name_or_path, force_download=True)\n",
    "    target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "    print(f\"Target sampling rate: {target_sampling_rate}\")\n",
    "    \n",
    "    # # Define preprocessing functions\n",
    "    # def speech_file_to_array_fn(path):\n",
    "    #     import torch\n",
    "    #     speech_array, sampling_rate = torchaudio.load(path)\n",
    "    #     resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "    #     speech = resampler(speech_array).squeeze().numpy()\n",
    "    #     return speech\n",
    "    def speech_file_to_array_fn(path):\n",
    "        import torchaudio\n",
    "        import torch \n",
    "    \n",
    "        try:\n",
    "            speech_array, sampling_rate = torchaudio.load(path)\n",
    "    \n",
    "            # --- Convert to mono by averaging channels if necessary ---\n",
    "            if speech_array.shape[0] > 1:  # Check if number of channels > 1\n",
    "                print(f\"Warning: Converting stereo audio to mono: {path}\") # Optional: log this\n",
    "                speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
    "            # --------------------------------------------------------\n",
    "    \n",
    "            # Ensure sampling rate matches target\n",
    "            if sampling_rate != target_sampling_rate:\n",
    "                 # Initialize resampler only if needed\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=target_sampling_rate)\n",
    "                speech_array = resampler(speech_array) # Resample\n",
    "    \n",
    "            # Squeeze and convert to numpy\n",
    "            # Ensure it's float32 for consistency\n",
    "            speech = speech_array.squeeze().to(torch.float32).numpy()\n",
    "    \n",
    "            # Optional: Check for empty arrays after processing\n",
    "            if speech.size == 0:\n",
    "                print(f\"Warning: Empty audio array after processing: {path}\")\n",
    "                # Return a small non-empty array to avoid downstream errors,\n",
    "                # though this might impact training slightly.\n",
    "                # A better approach might be to filter these files beforehand.\n",
    "                return np.zeros(1, dtype=np.float32)\n",
    "    \n",
    "            return speech\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {path}: {e}\")\n",
    "            # Return a placeholder or raise the exception depending on desired behavior\n",
    "            # Returning a dummy array might be safer for .map() not to fail entirely\n",
    "            return np.zeros(1, dtype=np.float32) # Or handle more gracefully\n",
    "    \n",
    "    def label_to_id(label, label_list):\n",
    "        if len(label_list) > 0:\n",
    "            return label_list.index(label) if label in label_list else -1\n",
    "        return label\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n",
    "        target_list = [label_to_id(label, label_list) for label in examples[output_column]]\n",
    "        \n",
    "        result = processor(speech_list, sampling_rate=target_sampling_rate)\n",
    "        result[\"labels\"] = list(target_list)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    print(\"Preprocessing train dataset...\")\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess_function,\n",
    "        batch_size=100,\n",
    "        batched=True,\n",
    "        num_proc=4\n",
    "    )\n",
    "    \n",
    "    print(\"Preprocessing evaluation dataset...\")\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        preprocess_function,\n",
    "        batch_size=100,\n",
    "        batched=True,\n",
    "        num_proc=4\n",
    "    )\n",
    "    \n",
    "    # Create data collator\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "    \n",
    "    # Define compute metrics function\n",
    "    def compute_metrics(p: EvalPrediction):\n",
    "        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "    \n",
    "    model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    # Freeze feature extractor\n",
    "    model.freeze_feature_extractor()\n",
    "    \n",
    "    # Define training arguments\n",
    "    output_dir = \"./emotion_recognition_model\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=10,\n",
    "        per_device_eval_batch_size=10,\n",
    "        gradient_accumulation_steps=4,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        num_train_epochs=8.0,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        # save_steps=100,\n",
    "        eval_steps=20,\n",
    "        logging_steps=10,\n",
    "        learning_rate=1e-4,\n",
    "        save_total_limit=2,\n",
    "        # dataloader_num_workers=2,\n",
    "        report_to=\"none\",  # Disable wandb\n",
    "    )\n",
    "    \n",
    "    trainer = CTCTrainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting model training...\")\n",
    "    train_result = trainer.train()\n",
    "    print(f\"Training completed. Training metrics: {train_result.metrics}\")\n",
    "    \n",
    "    print(\"Evaluating model...\")\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(f\"Evaluation metrics: {eval_result}\")\n",
    "\n",
    "\n",
    "    preds = trainer.predict(eval_dataset)\n",
    "    \n",
    "    # Extract predictions and true labels\n",
    "    test_preds = np.argmax(preds.predictions, axis=1)\n",
    "    test_true = preds.label_ids\n",
    "    \n",
    "    label_list = [model.config.id2label[i] for i in range(len(model.config.id2label))]\n",
    "    \n",
    "    print(classification_report(test_true, test_preds, target_names=label_list))\n",
    "    \n",
    "    print(f\"Saving model to {output_dir}\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save training args and other configurations\n",
    "    with open(f\"{output_dir}/training_args.json\", \"w\") as f:\n",
    "        json.dump(training_args.to_dict(), f)\n",
    "    \n",
    "    # Save label mappings\n",
    "    with open(f\"{output_dir}/label_mappings.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"label_list\": label_list,\n",
    "            \"num_labels\": num_labels,\n",
    "            \"label2id\": {label: i for i, label in enumerate(label_list)},\n",
    "            \"id2label\": {i: label for i, label in enumerate(label_list)}\n",
    "        }, f)\n",
    "    \n",
    "    print(\"Model training and saving completed successfully!\")\n",
    "    return output_dir\n",
    "\n",
    "# Run the training function\n",
    "if __name__ == \"__main__\":\n",
    "    train_emotion_recognition_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b11d2-90c6-4fa5-92e5-e0e5063133c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload  trained model to Huggingface Hub\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "def upload_model_to_hub(model_path):\n",
    "    \"\"\"\n",
    "    Upload a trained model to the Huggingface Hub\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model directory\n",
    "    \"\"\"\n",
    "\n",
    "    # Set repository name\n",
    "    model_name = \"wav2vec2-emotion-recognition-for-ravdness\"\n",
    "    repo_name = \"usamakenway/wav2vec2-large-xlsr-53-english-ravdess\"# \n",
    "    \n",
    "    # Login to Huggingface\n",
    "    print(\"Logging in to Huggingface Hub...\")\n",
    "    token = getpass(\"Enter your Huggingface token: \")\n",
    "    HfFolder.save_token(token)\n",
    "    api = HfApi()\n",
    "    \n",
    "    # Create repository if it doesn't exist\n",
    "    try:\n",
    "        api.create_repo(repo_id=repo_name, exist_ok=True)\n",
    "        print(f\"Repository {repo_name} is ready\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating repository: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Upload model files\n",
    "    print(f\"Uploading model files from {model_path} to {repo_name}...\")\n",
    "    api.upload_folder(\n",
    "        folder_path=model_path,\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload emotion recognition model\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Model has been uploaded to https://huggingface.co/{repo_name}\")\n",
    "    \n",
    "    # Extract label information for the model card\n",
    "    import json\n",
    "    try:\n",
    "        with open(os.path.join(model_path, \"label_mappings.json\"), \"r\") as f:\n",
    "            label_info = json.load(f)\n",
    "            label_list = label_info.get(\"label_list\", ['happiness', 'disgust', 'anger', 'fear', 'sadness'])\n",
    "    except:\n",
    "        label_list = ['happiness', 'disgust', 'anger', 'fear', 'sadness']\n",
    "    \n",
    "    # Create model card\n",
    "    print(\"Creating model card...\")\n",
    "    model_card = f\"\"\"---\n",
    "language: en\n",
    "license: apache-2.0\n",
    "tags:\n",
    "  - audio\n",
    "  - speech\n",
    "  - emotion-recognition\n",
    "  - wav2vec2\n",
    "datasets:\n",
    "  - RAVDESS\n",
    "model-index:\n",
    "  - name: {model_name}\n",
    "    results:\n",
    "      - task:\n",
    "          name: Speech Emotion Recognition\n",
    "          type: audio-classification\n",
    "        metrics:\n",
    "          - name: Training Accuracy\n",
    "            value: 0.9427  # Training accuracy from last epoch\n",
    "          - name: Validation Accuracy\n",
    "            value: 0.9427 \n",
    "          - name: Training Loss\n",
    "            value: 2.38  \n",
    "          - name: Validation Loss\n",
    "            value: 0.26  \n",
    "---\n",
    "\n",
    "# Speech Emotion Recognition Model\n",
    "\n",
    "This model is fine-tuned for speech emotion recognition. It can detect emotions such as happiness, sadness, anger, fear, disgust, etc. in speech.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- Model type: Fine-tuned Wav2Vec2\n",
    "- Base model: lighteternal/wav2vec2-large-xlsr-53-english\n",
    "- Training data: RAVDESS dataset\n",
    "- Supported emotions: {', '.join(label_list)}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import Wav2Vec2Processor, AutoModelForAudioClassification\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "# Load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"{repo_name}\")\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"{repo_name}\")\n",
    "\n",
    "# Function to predict emotion from audio file\n",
    "def predict_emotion(audio_path):\n",
    "    # Load audio\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sampling_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sampling_rate, 16000)\n",
    "        speech = resampler(speech_array).squeeze().numpy()\n",
    "    else:\n",
    "        speech = speech_array.squeeze().numpy()\n",
    "    \n",
    "    # Process audio\n",
    "    inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "        predicted_emotion = model.config.id2label[predicted_class_id]\n",
    "    \n",
    "    return predicted_emotion\n",
    "\n",
    "# Example usage\n",
    "emotion = predict_emotion(\"path/to/audio.wav\")\n",
    "print(f\"Detected emotion: sadness\")\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "The model was trained for 8 epochs using the following parameters:\n",
    "- Learning rate: 1e-4\n",
    "- Batch size: 20\n",
    "- Gradient accumulation steps: 4\n",
    "\n",
    "## Limitations\n",
    "\n",
    "This model works best with clear speech recordings in quiet environments. Performance may vary with different accents, languages, or noisy backgrounds.\n",
    "\"\"\"\n",
    "\n",
    "    # model card\n",
    "    with open(\"README.md\", \"w\") as f:\n",
    "        f.write(model_card)\n",
    "        \n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"README.md\",\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Add model card\"\n",
    "    )\n",
    "    \n",
    "    print(\"Model card has been uploaded\")\n",
    "    print(f\"Your model is now available at: https://huggingface.co/{repo_name}\")\n",
    "\n",
    "model_path = \"./emotion_recognition_model\" \n",
    "upload_model_to_hub(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600580d-b287-49c5-81d7-a6144e120a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
