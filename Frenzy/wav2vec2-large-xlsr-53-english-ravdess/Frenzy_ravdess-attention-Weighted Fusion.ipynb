{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7484691-b98c-4c46-a857-8d21f3d1d70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultimodalEmotionClassifier(\n",
      "  (audio_encoder): Wav2Vec2Model(\n",
      "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (feature_projection): Wav2Vec2FeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
      "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "        (conv): ParametrizedConv1d(\n",
      "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (padding): Wav2Vec2SamePadLayer()\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
      "          (attention): Wav2Vec2SdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.05, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.05, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (text_encoder): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (audio_projection): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (text_projection): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (attention): Linear(in_features=1024, out_features=2, bias=True)\n",
      "  (fusion_layers): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=5, bias=True)\n",
      "  )\n",
      "  (audio_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (text_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Model Initialized with 5 emotion classes\n",
      "Emotion to Index mapping: {'happiness': 0, 'disgust': 1, 'anger': 2, 'fear': 3, 'sadness': 4}\n",
      "Epoch 1/50\n",
      "Train Loss: 1.6212, Train Accuracy: 0.2394\n",
      "Val Loss: 1.6097, Val Accuracy: 0.2403\n",
      "Saved new best model with validation accuracy: 0.2403\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        38\n",
      "           1       0.26      0.30      0.28        27\n",
      "           2       0.22      0.42      0.29        26\n",
      "           3       0.00      0.00      0.00        35\n",
      "           4       0.24      0.64      0.35        28\n",
      "\n",
      "    accuracy                           0.24       154\n",
      "   macro avg       0.15      0.27      0.18       154\n",
      "weighted avg       0.13      0.24      0.16       154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "Train Loss: 1.6136, Train Accuracy: 0.2329\n",
      "Val Loss: 1.6055, Val Accuracy: 0.2468\n",
      "Saved new best model with validation accuracy: 0.2468\n",
      "Epoch 3/50\n",
      "Train Loss: 1.6113, Train Accuracy: 0.2410\n",
      "Val Loss: 1.5987, Val Accuracy: 0.2532\n",
      "Saved new best model with validation accuracy: 0.2532\n",
      "Epoch 4/50\n",
      "Train Loss: 1.5902, Train Accuracy: 0.2557\n",
      "Val Loss: 1.5977, Val Accuracy: 0.2468\n",
      "Epoch 5/50\n",
      "Train Loss: 1.6038, Train Accuracy: 0.2231\n",
      "Val Loss: 1.5949, Val Accuracy: 0.2662\n",
      "Saved new best model with validation accuracy: 0.2662\n",
      "Epoch 6/50\n",
      "Train Loss: 1.6038, Train Accuracy: 0.2378\n",
      "Val Loss: 1.5934, Val Accuracy: 0.2597\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.05      0.09        38\n",
      "           1       0.24      0.41      0.31        27\n",
      "           2       0.21      0.46      0.29        26\n",
      "           3       0.38      0.14      0.21        35\n",
      "           4       0.30      0.36      0.33        28\n",
      "\n",
      "    accuracy                           0.26       154\n",
      "   macro avg       0.31      0.28      0.24       154\n",
      "weighted avg       0.32      0.26      0.23       154\n",
      "\n",
      "Epoch 7/50\n",
      "Train Loss: 1.5926, Train Accuracy: 0.2394\n",
      "Val Loss: 1.5905, Val Accuracy: 0.2987\n",
      "Saved new best model with validation accuracy: 0.2987\n",
      "Epoch 8/50\n",
      "Train Loss: 1.5876, Train Accuracy: 0.3013\n",
      "Val Loss: 1.5863, Val Accuracy: 0.2792\n",
      "Epoch 9/50\n",
      "Train Loss: 1.5937, Train Accuracy: 0.2590\n",
      "Val Loss: 1.5858, Val Accuracy: 0.2922\n",
      "Epoch 10/50\n",
      "Train Loss: 1.5876, Train Accuracy: 0.2508\n",
      "Val Loss: 1.5861, Val Accuracy: 0.2857\n",
      "Epoch 11/50\n",
      "Train Loss: 1.5876, Train Accuracy: 0.2720\n",
      "Val Loss: 1.5844, Val Accuracy: 0.2727\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        38\n",
      "           1       0.33      0.30      0.31        27\n",
      "           2       0.21      0.38      0.27        26\n",
      "           3       0.35      0.23      0.28        35\n",
      "           4       0.28      0.57      0.37        28\n",
      "\n",
      "    accuracy                           0.27       154\n",
      "   macro avg       0.23      0.30      0.25       154\n",
      "weighted avg       0.22      0.27      0.23       154\n",
      "\n",
      "Epoch 12/50\n",
      "Train Loss: 1.5872, Train Accuracy: 0.2752\n",
      "Val Loss: 1.5802, Val Accuracy: 0.2857\n",
      "Epoch 13/50\n",
      "Train Loss: 1.5890, Train Accuracy: 0.2638\n",
      "Val Loss: 1.5762, Val Accuracy: 0.2792\n",
      "Epoch 14/50\n",
      "Train Loss: 1.5866, Train Accuracy: 0.2622\n",
      "Val Loss: 1.5751, Val Accuracy: 0.2857\n",
      "Epoch 15/50\n",
      "Train Loss: 1.5808, Train Accuracy: 0.2687\n",
      "Val Loss: 1.5722, Val Accuracy: 0.2987\n",
      "Epoch 16/50\n",
      "Train Loss: 1.5734, Train Accuracy: 0.2818\n",
      "Val Loss: 1.5726, Val Accuracy: 0.2857\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        38\n",
      "           1       0.31      0.52      0.39        27\n",
      "           2       0.15      0.19      0.17        26\n",
      "           3       0.36      0.29      0.32        35\n",
      "           4       0.31      0.54      0.39        28\n",
      "\n",
      "    accuracy                           0.29       154\n",
      "   macro avg       0.23      0.31      0.25       154\n",
      "weighted avg       0.22      0.29      0.24       154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50\n",
      "Train Loss: 1.5755, Train Accuracy: 0.2785\n",
      "Val Loss: 1.5695, Val Accuracy: 0.2922\n",
      "Epoch 18/50\n",
      "Train Loss: 1.5880, Train Accuracy: 0.2818\n",
      "Val Loss: 1.5705, Val Accuracy: 0.2727\n",
      "Epoch 19/50\n",
      "Train Loss: 1.5879, Train Accuracy: 0.2443\n",
      "Val Loss: 1.5701, Val Accuracy: 0.2727\n",
      "Epoch 20/50\n",
      "Train Loss: 1.5779, Train Accuracy: 0.2687\n",
      "Val Loss: 1.5671, Val Accuracy: 0.2857\n",
      "Epoch 21/50\n",
      "Train Loss: 1.5779, Train Accuracy: 0.2964\n",
      "Val Loss: 1.5651, Val Accuracy: 0.2857\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        38\n",
      "           1       0.27      0.52      0.36        27\n",
      "           2       0.19      0.35      0.25        26\n",
      "           3       0.42      0.37      0.39        35\n",
      "           4       0.32      0.29      0.30        28\n",
      "\n",
      "    accuracy                           0.29       154\n",
      "   macro avg       0.24      0.30      0.26       154\n",
      "weighted avg       0.23      0.29      0.25       154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "Train Loss: 1.5716, Train Accuracy: 0.2866\n",
      "Val Loss: 1.5601, Val Accuracy: 0.2987\n",
      "Epoch 23/50\n",
      "Train Loss: 1.5775, Train Accuracy: 0.2704\n",
      "Val Loss: 1.5569, Val Accuracy: 0.3117\n",
      "Saved new best model with validation accuracy: 0.3117\n",
      "Epoch 24/50\n",
      "Train Loss: 1.5712, Train Accuracy: 0.2834\n",
      "Val Loss: 1.5580, Val Accuracy: 0.3052\n",
      "Epoch 25/50\n",
      "Train Loss: 1.5820, Train Accuracy: 0.2818\n",
      "Val Loss: 1.5588, Val Accuracy: 0.2792\n",
      "Epoch 26/50\n",
      "Train Loss: 1.5743, Train Accuracy: 0.2850\n",
      "Val Loss: 1.5567, Val Accuracy: 0.2922\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        38\n",
      "           1       0.29      0.44      0.35        27\n",
      "           2       0.18      0.35      0.24        26\n",
      "           3       0.44      0.43      0.43        35\n",
      "           4       0.31      0.32      0.32        28\n",
      "\n",
      "    accuracy                           0.29       154\n",
      "   macro avg       0.25      0.31      0.27       154\n",
      "weighted avg       0.24      0.29      0.26       154\n",
      "\n",
      "Epoch 27/50\n",
      "Train Loss: 1.5740, Train Accuracy: 0.2785\n",
      "Val Loss: 1.5539, Val Accuracy: 0.2857\n",
      "Epoch 28/50\n",
      "Train Loss: 1.5611, Train Accuracy: 0.2883\n",
      "Val Loss: 1.5530, Val Accuracy: 0.2857\n",
      "Epoch 29/50\n",
      "Train Loss: 1.5810, Train Accuracy: 0.2541\n",
      "Val Loss: 1.5527, Val Accuracy: 0.3182\n",
      "Saved new best model with validation accuracy: 0.3182\n",
      "Epoch 30/50\n",
      "Train Loss: 1.5678, Train Accuracy: 0.2801\n",
      "Val Loss: 1.5488, Val Accuracy: 0.2987\n",
      "Epoch 31/50\n",
      "Train Loss: 1.5696, Train Accuracy: 0.2590\n",
      "Val Loss: 1.5478, Val Accuracy: 0.3052\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.03      0.05        38\n",
      "           1       0.33      0.67      0.44        27\n",
      "           2       0.16      0.23      0.19        26\n",
      "           3       0.40      0.46      0.43        35\n",
      "           4       0.38      0.21      0.27        28\n",
      "\n",
      "    accuracy                           0.31       154\n",
      "   macro avg       0.29      0.32      0.27       154\n",
      "weighted avg       0.29      0.31      0.27       154\n",
      "\n",
      "Epoch 32/50\n",
      "Train Loss: 1.5665, Train Accuracy: 0.3078\n",
      "Val Loss: 1.5485, Val Accuracy: 0.2857\n",
      "Epoch 33/50\n",
      "Train Loss: 1.5776, Train Accuracy: 0.2964\n",
      "Val Loss: 1.5462, Val Accuracy: 0.3247\n",
      "Saved new best model with validation accuracy: 0.3247\n",
      "Epoch 34/50\n",
      "Train Loss: 1.5677, Train Accuracy: 0.2818\n",
      "Val Loss: 1.5486, Val Accuracy: 0.3182\n",
      "Epoch 35/50\n",
      "Train Loss: 1.5666, Train Accuracy: 0.2948\n",
      "Val Loss: 1.5424, Val Accuracy: 0.3182\n",
      "Epoch 36/50\n",
      "Train Loss: 1.5732, Train Accuracy: 0.2720\n",
      "Val Loss: 1.5427, Val Accuracy: 0.3312\n",
      "Saved new best model with validation accuracy: 0.3312\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.05      0.09        38\n",
      "           1       0.33      0.81      0.47        27\n",
      "           2       0.17      0.19      0.18        26\n",
      "           3       0.44      0.46      0.45        35\n",
      "           4       0.35      0.21      0.27        28\n",
      "\n",
      "    accuracy                           0.33       154\n",
      "   macro avg       0.34      0.35      0.29       154\n",
      "weighted avg       0.35      0.33      0.29       154\n",
      "\n",
      "Epoch 37/50\n",
      "Train Loss: 1.5697, Train Accuracy: 0.2850\n",
      "Val Loss: 1.5424, Val Accuracy: 0.3312\n",
      "Epoch 38/50\n",
      "Train Loss: 1.5725, Train Accuracy: 0.2704\n",
      "Val Loss: 1.5453, Val Accuracy: 0.3247\n",
      "Epoch 39/50\n",
      "Train Loss: 1.5718, Train Accuracy: 0.2655\n",
      "Val Loss: 1.5473, Val Accuracy: 0.3312\n",
      "Epoch 40/50\n",
      "Train Loss: 1.5602, Train Accuracy: 0.2866\n",
      "Val Loss: 1.5450, Val Accuracy: 0.3377\n",
      "Saved new best model with validation accuracy: 0.3377\n",
      "Epoch 41/50\n",
      "Train Loss: 1.5723, Train Accuracy: 0.2687\n",
      "Val Loss: 1.5447, Val Accuracy: 0.3377\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.16      0.23        38\n",
      "           1       0.32      0.78      0.45        27\n",
      "           2       0.22      0.15      0.18        26\n",
      "           3       0.39      0.40      0.39        35\n",
      "           4       0.35      0.25      0.29        28\n",
      "\n",
      "    accuracy                           0.34       154\n",
      "   macro avg       0.34      0.35      0.31       154\n",
      "weighted avg       0.35      0.34      0.31       154\n",
      "\n",
      "Epoch 42/50\n",
      "Train Loss: 1.5549, Train Accuracy: 0.2915\n",
      "Val Loss: 1.5452, Val Accuracy: 0.3377\n",
      "Epoch 43/50\n",
      "Train Loss: 1.5614, Train Accuracy: 0.2866\n",
      "Val Loss: 1.5454, Val Accuracy: 0.3506\n",
      "Saved new best model with validation accuracy: 0.3506\n",
      "Epoch 44/50\n",
      "Train Loss: 1.5674, Train Accuracy: 0.2899\n",
      "Val Loss: 1.5458, Val Accuracy: 0.3247\n",
      "Epoch 45/50\n",
      "Train Loss: 1.5696, Train Accuracy: 0.2834\n",
      "Val Loss: 1.5462, Val Accuracy: 0.2987\n",
      "Epoch 46/50\n",
      "Train Loss: 1.5753, Train Accuracy: 0.2834\n",
      "Val Loss: 1.5432, Val Accuracy: 0.3182\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.30      0.08      0.12        38\n",
      "           1       0.31      0.78      0.45        27\n",
      "           2       0.18      0.19      0.19        26\n",
      "           3       0.41      0.37      0.39        35\n",
      "           4       0.41      0.25      0.31        28\n",
      "\n",
      "    accuracy                           0.32       154\n",
      "   macro avg       0.32      0.33      0.29       154\n",
      "weighted avg       0.33      0.32      0.29       154\n",
      "\n",
      "Epoch 47/50\n",
      "Train Loss: 1.5676, Train Accuracy: 0.2948\n",
      "Val Loss: 1.5410, Val Accuracy: 0.3377\n",
      "Epoch 48/50\n",
      "Train Loss: 1.5627, Train Accuracy: 0.2785\n",
      "Val Loss: 1.5404, Val Accuracy: 0.3312\n",
      "Epoch 49/50\n",
      "Train Loss: 1.5673, Train Accuracy: 0.3029\n",
      "Val Loss: 1.5404, Val Accuracy: 0.3312\n",
      "Epoch 50/50\n",
      "Train Loss: 1.5590, Train Accuracy: 0.2704\n",
      "Val Loss: 1.5421, Val Accuracy: 0.3312\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   happiness       0.18      0.05      0.08        38\n",
      "     disgust       0.27      0.54      0.36        39\n",
      "       anger       0.20      0.18      0.19        38\n",
      "        fear       0.25      0.26      0.26        38\n",
      "     sadness       0.18      0.13      0.15        39\n",
      "\n",
      "    accuracy                           0.23       192\n",
      "   macro avg       0.22      0.23      0.21       192\n",
      "weighted avg       0.22      0.23      0.21       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import librosa\n",
    "import os\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, \n",
    "    Wav2Vec2Model, \n",
    "    BertModel, \n",
    "    BertTokenizer\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Custom Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultimodalEmotionDataset(Dataset):\n",
    "    def __init__(self, csv_path, processor, tokenizer, max_length=128, target_audio_length=16000):\n",
    "        # Read the CSV\n",
    "        self.data = pd.read_csv(csv_path, sep='\\t')\n",
    "        \n",
    "        # Add random text column if not exists\n",
    "        if 'text' not in self.data.columns:\n",
    "            print([f\"Random text for {name}\" for name in self.data['name']])\n",
    "            self.data['text'] = [f\"Random text for {name}\" for name in self.data['name']]\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.target_audio_length = target_audio_length\n",
    "        \n",
    "        # Mapping emotions to indices\n",
    "        self.emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.data['emotion'].unique())}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Load audio file\n",
    "        speech_array, sampling_rate = torchaudio.load(row['path'])  # (Channels, Samples)\n",
    "        \n",
    "        # Convert to mono (if stereo, take the first channel)\n",
    "        if speech_array.shape[0] > 1:\n",
    "            speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
    "        \n",
    "        speech_array = speech_array.squeeze().numpy()  # Convert to numpy array\n",
    "        \n",
    "        # Resample to target sampling rate\n",
    "        speech_array = librosa.resample(y=speech_array, orig_sr=sampling_rate, target_sr=self.processor.feature_extractor.sampling_rate)\n",
    "        \n",
    "        # Ensure fixed length\n",
    "        if len(speech_array) > self.target_audio_length:\n",
    "            speech_array = speech_array[:self.target_audio_length]\n",
    "        elif len(speech_array) < self.target_audio_length:\n",
    "            padding = self.target_audio_length - len(speech_array)\n",
    "            speech_array = np.pad(speech_array, (0, padding), mode='constant', constant_values=0)\n",
    "    \n",
    "        # Process audio with Wav2Vec2\n",
    "        audio_inputs = self.processor(\n",
    "            speech_array, \n",
    "            sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Process text\n",
    "        text_inputs = self.tokenizer(\n",
    "            row['text'], \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get emotion label\n",
    "        label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "        return {\n",
    "            'audio_input': audio_inputs.input_values.squeeze(),  # Ensure correct shape\n",
    "            'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "            'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "            'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     row = self.data.iloc[idx]\n",
    "        \n",
    "    #     # Process Audio\n",
    "    #     speech_array, sampling_rate = torchaudio.load(row['path'])\n",
    "    #     speech_array = speech_array.squeeze().numpy()\n",
    "    #     # speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, self.processor.feature_extractor.sampling_rate)\n",
    "    #     speech_array = librosa.resample(y=np.asarray(speech_array), orig_sr=sampling_rate, target_sr=self.processor.feature_extractor.sampling_rate)\n",
    "    #     # Pad or truncate audio\n",
    "    #     if len(speech_array) > self.target_audio_length:\n",
    "    #         speech_array = speech_array[:self.target_audio_length]\n",
    "    #     elif len(speech_array) < self.target_audio_length:\n",
    "    #         padding = self.target_audio_length - len(speech_array)\n",
    "    #         speech_array = np.pad(speech_array, (0, padding), mode='constant', constant_values=0)\n",
    "        \n",
    "    #     audio_inputs = self.processor(\n",
    "    #         speech_array, \n",
    "    #         sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "    #         return_tensors=\"pt\"\n",
    "    #     )\n",
    "        \n",
    "    #     # Process Text\n",
    "    #     text_inputs = self.tokenizer(\n",
    "    #         row['text'], \n",
    "    #         max_length=self.max_length, \n",
    "    #         padding='max_length', \n",
    "    #         truncation=True, \n",
    "    #         return_tensors=\"pt\"\n",
    "    #     )\n",
    "        \n",
    "    #     # Get emotion label\n",
    "    #     label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "    #     return {\n",
    "    #         'audio_input': audio_inputs.input_values.squeeze(),\n",
    "    #         'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "    #         'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "    #         'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "    #         'label': label\n",
    "    #     }\n",
    "\n",
    "\n",
    "class MultimodalEmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, audio_model_path, text_model_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Audio Encoder (Wav2Vec2)\n",
    "        self.audio_encoder = Wav2Vec2Model.from_pretrained(audio_model_path)\n",
    "        \n",
    "        # Text Encoder (BERT)\n",
    "        self.text_encoder = BertModel.from_pretrained(text_model_path)\n",
    "        \n",
    "        # Freeze pretrained encoders (optional)\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Feature dimensions\n",
    "        self.audio_feature_dim = self.audio_encoder.config.hidden_size\n",
    "        self.text_feature_dim = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        # Projection layers to common dimension\n",
    "        common_dim = 512\n",
    "        self.audio_projection = nn.Linear(self.audio_feature_dim, common_dim)\n",
    "        self.text_projection = nn.Linear(self.text_feature_dim, common_dim)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Linear(common_dim * 2, 2)\n",
    "        \n",
    "        # Fusion Layer\n",
    "        self.fusion_layers = nn.Sequential(\n",
    "            nn.Linear(common_dim * 2, common_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(common_dim, num_labels)\n",
    "        )\n",
    "        \n",
    "        self.audio_norm = nn.LayerNorm(self.audio_feature_dim)\n",
    "        self.text_norm = nn.LayerNorm(self.text_feature_dim)\n",
    "\n",
    "    def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "        # Extract audio features\n",
    "        audio_outputs = self.audio_encoder(audio_input, attention_mask=audio_mask)\n",
    "        audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)\n",
    "        audio_features = self.audio_norm(audio_features)\n",
    "        \n",
    "        # Extract text features\n",
    "        text_outputs = self.text_encoder(text_input_ids, attention_mask=text_attention_mask)\n",
    "        text_features = text_outputs.pooler_output\n",
    "        text_features = self.text_norm(text_features)\n",
    "        \n",
    "        # Project to common dimension\n",
    "        audio_projected = self.audio_projection(audio_features)\n",
    "        text_projected = self.text_projection(text_features)\n",
    "        \n",
    "        # Compute attention weights\n",
    "        combined = torch.cat([audio_projected, text_projected], dim=1)\n",
    "        # # attention_weights = F.softmax(self.attention(combined), dim=1)\n",
    "        # bias = torch.tensor([2.0, 1.0], device=combined.device)  # Give more weight to audio\n",
    "        # attention_logits = self.attention(combined) + bias\n",
    "        # attention_weights = F.softmax(attention_logits, dim=1)\n",
    "\n",
    "        \n",
    "        # # # Apply attention weights\n",
    "        # modal_features = torch.cat([\n",
    "        #     audio_projected * attention_weights[:, 0].unsqueeze(1).expand_as(audio_projected),\n",
    "        #     text_projected * attention_weights[:, 1].unsqueeze(1).expand_as(text_projected)\n",
    "        # ], dim=1)\n",
    "        # Updated:\n",
    "        bias = torch.tensor([2.0, 1.0], device=combined.device)  # More weight to audio\n",
    "        attention_logits = self.attention(torch.cat([audio_projected, text_projected], dim=1)) + bias\n",
    "        attention_weights = F.softmax(attention_logits, dim=1)\n",
    "        \n",
    "        modal_features = torch.cat([\n",
    "            audio_projected * attention_weights[:, 0].unsqueeze(1).expand_as(audio_projected),\n",
    "            text_projected * attention_weights[:, 1].unsqueeze(1).expand_as(text_projected)\n",
    "        ], dim=1)\n",
    "        # modal_features = torch.cat([\n",
    "        #     0.7 * audio_projected,\n",
    "        #     0.3 * text_projected\n",
    "        # ], dim=1)\n",
    "        # Classification\n",
    "        logits = self.fusion_layers(modal_features)\n",
    "        \n",
    "        return logits, attention_weights\n",
    "\n",
    "    # def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "    #     # Extract audio features\n",
    "    #     audio_outputs = self.audio_encoder(\n",
    "    #         audio_input, \n",
    "    #         attention_mask=audio_mask\n",
    "    #     )\n",
    "    #     audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)\n",
    "        \n",
    "    #     # Extract text features\n",
    "    #     text_outputs = self.text_encoder(\n",
    "    #         text_input_ids, \n",
    "    #         attention_mask=text_attention_mask\n",
    "    #     )\n",
    "    #     text_features = text_outputs.pooler_output\n",
    "        \n",
    "    #     # Concatenate features\n",
    "    #     combined_features = torch.cat([audio_features, text_features], dim=1)\n",
    "        \n",
    "    #     # Classification\n",
    "    #     logits = self.fusion_layers(combined_features)\n",
    "        \n",
    "    #     return logits\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10, learning_rate=1e-5, text_penalty=0.1):\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('saved_models', exist_ok=True)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_true = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move data to device\n",
    "            audio_input = batch['audio_input'].to(device)\n",
    "            audio_mask = batch['audio_mask'].to(device)\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs_tuple = model(\n",
    "                audio_input, \n",
    "                audio_mask, \n",
    "                text_input_ids, \n",
    "                text_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Handle the tuple return value\n",
    "            if isinstance(outputs_tuple, tuple):\n",
    "                outputs, attention_weights = outputs_tuple\n",
    "                \n",
    "                # Compute classification loss\n",
    "                class_loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Add penalty for relying too much on text features\n",
    "                text_reliance_penalty = text_penalty * attention_weights[:, 1].mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = class_loss + text_reliance_penalty\n",
    "            else:\n",
    "                # For backward compatibility with original model\n",
    "                outputs = outputs_tuple\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track training metrics\n",
    "            train_losses.append(loss.item())\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Move data to device\n",
    "                audio_input = batch['audio_input'].to(device)\n",
    "                audio_mask = batch['audio_mask'].to(device)\n",
    "                text_input_ids = batch['text_input_ids'].to(device)\n",
    "                text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs_tuple = model(\n",
    "                    audio_input, \n",
    "                    audio_mask, \n",
    "                    text_input_ids, \n",
    "                    text_attention_mask\n",
    "                )\n",
    "                \n",
    "                # Handle the tuple return value\n",
    "                if isinstance(outputs_tuple, tuple):\n",
    "                    outputs, _ = outputs_tuple\n",
    "                    loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    outputs = outputs_tuple\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                # Track validation metrics\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Compute metrics\n",
    "        train_accuracy = accuracy_score(train_true, train_preds)\n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {np.mean(val_losses):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_accuracy': best_val_accuracy,\n",
    "                'epoch': epoch\n",
    "            }, f'saved_models/best_multimodal_model.pth')\n",
    "            \n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_accuracy:.4f}\")\n",
    "        \n",
    "        # Optional: Print classification report for validation set\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"\\nValidation Classification Report:\")\n",
    "            print(classification_report(val_true, val_preds))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Paths and Configurations\n",
    "    audio_model_path = \"./emotion_recognition_model\"\n",
    "    text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "    # Initialize Processors\n",
    "    audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "    text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "    \n",
    "    # Create Datasets\n",
    "    train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "    test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "    \n",
    "    # Split train into train and validation\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, \n",
    "        [int(len(train_dataset)*0.8), len(train_dataset)-int(len(train_dataset)*0.8)]\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Get number of labels\n",
    "    num_labels = len(train_dataset.dataset.emotion_to_idx)\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    print(model)\n",
    "    print(f\"Model Initialized with {num_labels} emotion classes\")\n",
    "    print(\"Emotion to Index mapping:\", train_dataset.dataset.emotion_to_idx)\n",
    "    \n",
    "    # Train Model\n",
    "    trained_model = train_model(model, train_loader, val_loader, device, epochs=50)\n",
    "    \n",
    "    # Optional: Load and evaluate best saved model\n",
    "    best_model_path = 'saved_models/best_multimodal_model.pth'\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    \n",
    "    # Reinitialize model and load state dict\n",
    "    best_model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Test model\n",
    "    # Test model\n",
    "    best_model.eval()\n",
    "    test_preds = []\n",
    "    test_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Move data to device\n",
    "            audio_input = batch['audio_input'].to(device)\n",
    "            audio_mask = batch['audio_mask'].to(device)\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs_tuple = best_model(\n",
    "                audio_input, \n",
    "                audio_mask, \n",
    "                text_input_ids, \n",
    "                text_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Handle the tuple return value\n",
    "            if isinstance(outputs_tuple, tuple):\n",
    "                outputs, _ = outputs_tuple\n",
    "            else:\n",
    "                outputs = outputs_tuple\n",
    "            \n",
    "            # Track test metrics\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_true.extend(labels.cpu().numpy())\n",
    "    # Print test classification report\n",
    "    print(\"\\nTest Classification Report:\")\n",
    "    print(classification_report(test_true, test_preds, \n",
    "        target_names=list(train_dataset.dataset.emotion_to_idx.keys())))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0dd7a-0cd2-4857-b158-9b054a889ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
