{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62234197-1c41-4553-94f5-1094bdc328af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Processor\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e33fa4c-6f4b-4fe5-874e-c39021f4b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c0c2d29-86d5-4a62-bb20-7b713acf6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Frenzy(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3591cf44-33dd-4fbd-8698-c77b9f068acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Frenzy(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Wav2Vec2ClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Frenzy(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf784e3-86a7-4a3f-aee3-41ad488dae4f",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2758a2a-cfda-41b6-9612-53ab472f7975",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 99 (65948161.py, line 101)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 101\u001b[1;36m\u001b[0m\n\u001b[1;33m    audio_outputs = self.audio_encoder(audio_input, attention_mask=audio_mask)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after function definition on line 99\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import librosa\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, \n",
    "    Wav2Vec2Model, \n",
    "    BertModel, \n",
    "    BertTokenizer\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom Dataset\n",
    "class MultimodalEmotionDataset(Dataset):\n",
    "    def __init__(self, csv_path, processor, tokenizer, max_length=128):\n",
    "        # Read the CSV\n",
    "        self.data = pd.read_csv(csv_path, sep='\\t')\n",
    "        \n",
    "        # Add random text column if not exists\n",
    "        if 'text' not in self.data.columns:\n",
    "            self.data['text'] = [f\"Random text for {name}\" for name in self.data['name']]\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Mapping emotions to indices\n",
    "        self.emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.data['emotion'].unique())}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Process Audio\n",
    "        speech_array, sampling_rate = torchaudio.load(row['path'])\n",
    "        speech_array = speech_array.squeeze().numpy()\n",
    "        speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, self.processor.feature_extractor.sampling_rate)\n",
    "        \n",
    "        audio_inputs = self.processor(\n",
    "            speech_array, \n",
    "            sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Process Text\n",
    "        text_inputs = self.tokenizer(\n",
    "            row['text'], \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get emotion label\n",
    "        label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "        return {\n",
    "            'audio_input': audio_inputs.input_values.squeeze(),\n",
    "            'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "            'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "            'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "# Multimodal Fusion Model\n",
    "class MultimodalEmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, audio_model_path, text_model_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Audio Encoder (Wav2Vec2)\n",
    "        self.audio_encoder = Wav2Vec2Model.from_pretrained(audio_model_path)\n",
    "        \n",
    "        # Text Encoder (BERT)\n",
    "        self.text_encoder = BertModel.from_pretrained(text_model_path)\n",
    "        \n",
    "        # Freeze pretrained encoders (optional)\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Fusion Layer\n",
    "        audio_feature_dim = self.audio_encoder.config.hidden_size\n",
    "        text_feature_dim = self.text_encoder.config.hidden_size\n",
    "        fusion_dim = audio_feature_dim + text_feature_dim\n",
    "        \n",
    "        self.fusion_layers = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, fusion_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(fusion_dim // 2, num_labels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "    # Extract audio features\n",
    "    audio_outputs = self.audio_encoder(audio_input, attention_mask=audio_mask)\n",
    "    audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)  # Mean pooling\n",
    "    audio_features = self.audio_norm(audio_features)  # Apply LayerNorm\n",
    "\n",
    "    # Extract text features\n",
    "    text_outputs = self.text_encoder(text_input_ids, attention_mask=text_attention_mask)\n",
    "    text_features = text_outputs.pooler_output  # [CLS] token representation\n",
    "    text_features = self.text_norm(text_features)  # Apply LayerNorm\n",
    "\n",
    "    # Concatenate features\n",
    "    combined_features = torch.cat([audio_features, text_features], dim=1)\n",
    "\n",
    "    # Classification\n",
    "    logits = self.fusion_layers(combined_features)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "    # def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "    #     # Extract audio features\n",
    "    #     audio_outputs = self.audio_encoder(\n",
    "    #         audio_input, \n",
    "    #         attention_mask=audio_mask\n",
    "    #     )\n",
    "    #     audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)\n",
    "        \n",
    "    #     # Extract text features\n",
    "    #     text_outputs = self.text_encoder(\n",
    "    #         text_input_ids, \n",
    "    #         attention_mask=text_attention_mask\n",
    "    #     )\n",
    "    #     text_features = text_outputs.pooler_output\n",
    "        \n",
    "    #     # Concatenate features\n",
    "    #     combined_features = torch.cat([audio_features, text_features], dim=1)\n",
    "        \n",
    "    #     # Classification\n",
    "    #     logits = self.fusion_layers(combined_features)\n",
    "        \n",
    "    #     return logits\n",
    "\n",
    "# Training Setup\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Paths and Configurations\n",
    "    audio_model_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "    text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "    # Initialize Processors\n",
    "    audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "    text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "    \n",
    "    # Create Datasets\n",
    "    train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "    test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "    \n",
    "    # Get number of labels\n",
    "    num_labels = len(train_dataset.emotion_to_idx)\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model Initialized with {num_labels} emotion classes\")\n",
    "    print(\"Emotion to Index mapping:\", train_dataset.emotion_to_idx)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #main() \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Paths and Configurations\n",
    "    audio_model_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "    text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "    # Initialize Processors\n",
    "    audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "    text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "    \n",
    "    # Create Datasets\n",
    "    train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "    test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "    \n",
    "    # Get number of labels\n",
    "    num_labels = len(train_dataset.emotion_to_idx)\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model Initialized with {num_labels} emotion classes\")\n",
    "    print(\"Emotion to Index mapping:\", train_dataset.emotion_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbebea92-a587-48b5-9e40-fc3520cb77a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalEmotionClassifier(\n",
       "  (audio_encoder): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fusion_layers): Sequential(\n",
       "    (0): Linear(in_features=1792, out_features=896, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=896, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18e105-dc98-4393-b5b7-45d42ad7c962",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f9e3db9-ab71-45dd-892d-7f006bf3525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized with 5 emotion classes\n",
      "Emotion to Index mapping: {'disgust': 0, 'happiness': 1, 'fear': 2, 'sadness': 3, 'anger': 4}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [16000] at entry 0 and [16000, 85935] at entry 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 355\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classification_report(test_true, test_preds, \n\u001b[0;32m    352\u001b[0m         target_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39memotion_to_idx\u001b[38;5;241m.\u001b[39mkeys())))\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 355\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 308\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmotion to Index mapping:\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_dataset\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39memotion_to_idx)\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Train Model\u001b[39;00m\n\u001b[1;32m--> 308\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;66;03m# Optional: Load and evaluate best saved model\u001b[39;00m\n\u001b[0;32m    311\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msaved_models/best_multimodal_model.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 175\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, device, epochs, learning_rate)\u001b[0m\n\u001b[0;32m    172\u001b[0m train_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    173\u001b[0m train_true \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 175\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move data to device\u001b[39;49;00m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_input\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio_input\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maudio_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m--> 171\u001b[0m         \u001b[43m{\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:172\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMutableMapping):\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# The mapping type may have extra properties, so we can't just\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# use `type(data)(...)` to create the new mapping.\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# Create a clone and update it if the mapping type is mutable.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     clone \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(elem)\n\u001b[0;32m    170\u001b[0m     clone\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    171\u001b[0m         {\n\u001b[1;32m--> 172\u001b[0m             key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    175\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem\n\u001b[0;32m    176\u001b[0m         }\n\u001b[0;32m    177\u001b[0m     )\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m clone\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [16000] at entry 0 and [16000, 85935] at entry 4"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import librosa\n",
    "import os\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, \n",
    "    Wav2Vec2Model, \n",
    "    BertModel, \n",
    "    BertTokenizer\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Custom Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultimodalEmotionDataset(Dataset):\n",
    "    def __init__(self, csv_path, processor, tokenizer, max_length=128, target_audio_length=16000):\n",
    "        # Read the CSV\n",
    "        self.data = pd.read_csv(csv_path, sep='\\t')\n",
    "        \n",
    "        # Add random text column if not exists\n",
    "        if 'text' not in self.data.columns:\n",
    "            print([f\"Random text for {name}\" for name in self.data['name']])\n",
    "            self.data['text'] = [f\"Random text for {name}\" for name in self.data['name']]\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.target_audio_length = target_audio_length\n",
    "        \n",
    "        # Mapping emotions to indices\n",
    "        self.emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.data['emotion'].unique())}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Load audio file\n",
    "        speech_array, sampling_rate = torchaudio.load(row['path'])  # (Channels, Samples)\n",
    "        \n",
    "        # Convert to mono (if stereo, take the first channel)\n",
    "        if speech_array.shape[0] > 1:\n",
    "            speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
    "        \n",
    "        speech_array = speech_array.squeeze().numpy()  # Convert to numpy array\n",
    "        \n",
    "        # Resample to target sampling rate\n",
    "        speech_array = librosa.resample(y=speech_array, orig_sr=sampling_rate, target_sr=self.processor.feature_extractor.sampling_rate)\n",
    "        \n",
    "        # Ensure fixed length\n",
    "        if len(speech_array) > self.target_audio_length:\n",
    "            speech_array = speech_array[:self.target_audio_length]\n",
    "        elif len(speech_array) < self.target_audio_length:\n",
    "            padding = self.target_audio_length - len(speech_array)\n",
    "            speech_array = np.pad(speech_array, (0, padding), mode='constant', constant_values=0)\n",
    "    \n",
    "        # Process audio with Wav2Vec2\n",
    "        audio_inputs = self.processor(\n",
    "            speech_array, \n",
    "            sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Process text\n",
    "        text_inputs = self.tokenizer(\n",
    "            row['text'], \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get emotion label\n",
    "        label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "        return {\n",
    "            'audio_input': audio_inputs.input_values.squeeze(),  # Ensure correct shape\n",
    "            'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "            'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "            'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     row = self.data.iloc[idx]\n",
    "        \n",
    "    #     # Process Audio\n",
    "    #     speech_array, sampling_rate = torchaudio.load(row['path'])\n",
    "    #     speech_array = speech_array.squeeze().numpy()\n",
    "    #     # speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, self.processor.feature_extractor.sampling_rate)\n",
    "    #     speech_array = librosa.resample(y=np.asarray(speech_array), orig_sr=sampling_rate, target_sr=self.processor.feature_extractor.sampling_rate)\n",
    "    #     # Pad or truncate audio\n",
    "    #     if len(speech_array) > self.target_audio_length:\n",
    "    #         speech_array = speech_array[:self.target_audio_length]\n",
    "    #     elif len(speech_array) < self.target_audio_length:\n",
    "    #         padding = self.target_audio_length - len(speech_array)\n",
    "    #         speech_array = np.pad(speech_array, (0, padding), mode='constant', constant_values=0)\n",
    "        \n",
    "    #     audio_inputs = self.processor(\n",
    "    #         speech_array, \n",
    "    #         sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "    #         return_tensors=\"pt\"\n",
    "    #     )\n",
    "        \n",
    "    #     # Process Text\n",
    "    #     text_inputs = self.tokenizer(\n",
    "    #         row['text'], \n",
    "    #         max_length=self.max_length, \n",
    "    #         padding='max_length', \n",
    "    #         truncation=True, \n",
    "    #         return_tensors=\"pt\"\n",
    "    #     )\n",
    "        \n",
    "    #     # Get emotion label\n",
    "    #     label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "    #     return {\n",
    "    #         'audio_input': audio_inputs.input_values.squeeze(),\n",
    "    #         'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "    #         'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "    #         'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "    #         'label': label\n",
    "    #     }\n",
    "\n",
    "\n",
    "# Multimodal Fusion Model\n",
    "class MultimodalEmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, audio_model_path, text_model_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Audio Encoder (Wav2Vec2)\n",
    "        self.audio_encoder = Wav2Vec2Model.from_pretrained(audio_model_path)\n",
    "        \n",
    "        # Text Encoder (BERT)\n",
    "        self.text_encoder = BertModel.from_pretrained(text_model_path)\n",
    "        \n",
    "        # Freeze pretrained encoders (optional)\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Fusion Layer\n",
    "        audio_feature_dim = self.audio_encoder.config.hidden_size\n",
    "        text_feature_dim = self.text_encoder.config.hidden_size\n",
    "        fusion_dim = audio_feature_dim + text_feature_dim\n",
    "        \n",
    "        self.fusion_layers = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, fusion_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(fusion_dim // 2, num_labels)\n",
    "        )\n",
    "        self.audio_norm = nn.LayerNorm(audio_feature_dim)\n",
    "        self.text_norm = nn.LayerNorm(text_feature_dim)\n",
    "\n",
    "    def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "        # Extract audio features\n",
    "        audio_outputs = self.audio_encoder(audio_input, attention_mask=audio_mask)\n",
    "        audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)  # Mean pooling\n",
    "        audio_features = self.audio_norm(audio_features)  # Apply LayerNorm\n",
    "    \n",
    "        # Extract text features\n",
    "        text_outputs = self.text_encoder(text_input_ids, attention_mask=text_attention_mask)\n",
    "        text_features = text_outputs.pooler_output  # [CLS] token representation\n",
    "        text_features = self.text_norm(text_features)  # Apply LayerNorm\n",
    "    \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([audio_features, text_features], dim=1)\n",
    "    \n",
    "        # Classification\n",
    "        logits = self.fusion_layers(combined_features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    # def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "    #     # Extract audio features\n",
    "    #     audio_outputs = self.audio_encoder(\n",
    "    #         audio_input, \n",
    "    #         attention_mask=audio_mask\n",
    "    #     )\n",
    "    #     audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)\n",
    "        \n",
    "    #     # Extract text features\n",
    "    #     text_outputs = self.text_encoder(\n",
    "    #         text_input_ids, \n",
    "    #         attention_mask=text_attention_mask\n",
    "    #     )\n",
    "    #     text_features = text_outputs.pooler_output\n",
    "        \n",
    "    #     # Concatenate features\n",
    "    #     combined_features = torch.cat([audio_features, text_features], dim=1)\n",
    "        \n",
    "    #     # Classification\n",
    "    #     logits = self.fusion_layers(combined_features)\n",
    "        \n",
    "    #     return logits\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10, learning_rate=1e-4):\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('saved_models', exist_ok=True)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_true = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move data to device\n",
    "            audio_input = batch['audio_input'].to(device)\n",
    "            audio_mask = batch['audio_mask'].to(device)\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                audio_input, \n",
    "                audio_mask, \n",
    "                text_input_ids, \n",
    "                text_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track training metrics\n",
    "            train_losses.append(loss.item())\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Move data to device\n",
    "                audio_input = batch['audio_input'].to(device)\n",
    "                audio_mask = batch['audio_mask'].to(device)\n",
    "                text_input_ids = batch['text_input_ids'].to(device)\n",
    "                text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    audio_input, \n",
    "                    audio_mask, \n",
    "                    text_input_ids, \n",
    "                    text_attention_mask\n",
    "                )\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                # Track validation metrics\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Compute metrics\n",
    "        train_accuracy = accuracy_score(train_true, train_preds)\n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {np.mean(val_losses):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_accuracy': best_val_accuracy,\n",
    "                'epoch': epoch\n",
    "            }, f'saved_models/best_multimodal_model.pth')\n",
    "            \n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_accuracy:.4f}\")\n",
    "        \n",
    "        # Optional: Print classification report for validation set\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"\\nValidation Classification Report:\")\n",
    "            print(classification_report(val_true, val_preds))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Paths and Configurations\n",
    "    audio_model_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "    text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "    # Initialize Processors\n",
    "    audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "    text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "    \n",
    "    # Create Datasets\n",
    "    train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "    test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "    \n",
    "    # Split train into train and validation\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, \n",
    "        [int(len(train_dataset)*0.8), len(train_dataset)-int(len(train_dataset)*0.8)]\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Get number of labels\n",
    "    num_labels = len(train_dataset.dataset.emotion_to_idx)\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model Initialized with {num_labels} emotion classes\")\n",
    "    print(\"Emotion to Index mapping:\", train_dataset.dataset.emotion_to_idx)\n",
    "    \n",
    "    # Train Model\n",
    "    trained_model = train_model(model, train_loader, val_loader, device, epochs=10)\n",
    "    \n",
    "    # Optional: Load and evaluate best saved model\n",
    "    best_model_path = 'saved_models/best_multimodal_model.pth'\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    \n",
    "    # Reinitialize model and load state dict\n",
    "    best_model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Test model\n",
    "    best_model.eval()\n",
    "    test_preds = []\n",
    "    test_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Move data to device\n",
    "            audio_input = batch['audio_input'].to(device)\n",
    "            audio_mask = batch['audio_mask'].to(device)\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = best_model(\n",
    "                audio_input, \n",
    "                audio_mask, \n",
    "                text_input_ids, \n",
    "                text_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Track test metrics\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_true.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Print test classification report\n",
    "    print(\"\\nTest Classification Report:\")\n",
    "    print(classification_report(test_true, test_preds, \n",
    "        target_names=list(train_dataset.dataset.emotion_to_idx.keys())))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2c40cb-5f2a-4e2f-a36a-0de08d586e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a974e96-5915-4713-9fb5-1ed14f733915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4d5969-9d11-412f-81a2-26bf35e13f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a07df2e-edee-4ba2-8304-79daabbb7dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5542c8e4-68ae-4bdf-9357-0b8a163f08f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Paths and Configurations\u001b[39;00m\n\u001b[0;32m      4\u001b[0m audio_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths and Configurations\n",
    "audio_model_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "text_model_path = \"bert-base-uncased\"\n",
    "\n",
    "# Initialize Processors\n",
    "audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "\n",
    "# Split train into train and validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, \n",
    "    [int(len(train_dataset)*0.8), len(train_dataset)-int(len(train_dataset)*0.8)]\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Get number of labels\n",
    "num_labels = len(train_dataset.dataset.emotion_to_idx)\n",
    "\n",
    "# Initialize Model\n",
    "model = MultimodalEmotionClassifier(\n",
    "    num_labels=num_labels, \n",
    "    audio_model_path=audio_model_path, \n",
    "    text_model_path=text_model_path\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model Initialized with {num_labels} emotion classes\")\n",
    "print(\"Emotion to Index mapping:\", train_dataset.dataset.emotion_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4a7f16-dac9-41ec-b17e-f3cc47b034ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torchaudio\n",
    "# import librosa\n",
    "# from transformers import (\n",
    "#     Wav2Vec2Processor, \n",
    "#     Wav2Vec2Model, \n",
    "#     BertModel, \n",
    "#     BertTokenizer\n",
    "# )\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Custom Dataset (previous implementation remains the same)\n",
    "# class MultimodalEmotionDataset(Dataset):\n",
    "#     def __init__(self, csv_path, processor, tokenizer, max_length=128):\n",
    "#         # Read the CSV\n",
    "#         self.data = pd.read_csv(csv_path, sep='\\t')\n",
    "        \n",
    "#         # Add random text column if not exists\n",
    "#         if 'text' not in self.data.columns:\n",
    "#             self.data['text'] = [f\"Random text for {name}\" for name in self.data['name']]\n",
    "        \n",
    "#         self.processor = processor\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "        \n",
    "#         # Mapping emotions to indices\n",
    "#         self.emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.data['emotion'].unique())}\n",
    "#         self.idx_to_emotion = {idx: emotion for emotion, idx in self.emotion_to_idx.items()}\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.data.iloc[idx]\n",
    "        \n",
    "#         # Process Audio\n",
    "#         speech_array, sampling_rate = torchaudio.load(row['path'])\n",
    "#         speech_array = speech_array.squeeze().numpy()\n",
    "#         speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, self.processor.feature_extractor.sampling_rate)\n",
    "        \n",
    "#         audio_inputs = self.processor(\n",
    "#             speech_array, \n",
    "#             sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "        \n",
    "#         # Process Text\n",
    "#         text_inputs = self.tokenizer(\n",
    "#             row['text'], \n",
    "#             max_length=self.max_length, \n",
    "#             padding='max_length', \n",
    "#             truncation=True, \n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "        \n",
    "#         # Get emotion label\n",
    "#         label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "#         return {\n",
    "#             'audio_input': audio_inputs.input_values.squeeze(),\n",
    "#             'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "#             'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "#             'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "#             'label': label\n",
    "#         }\n",
    "\n",
    "# # Multimodal Fusion Model (previous implementation remains the same)\n",
    "# class MultimodalEmotionClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels, audio_model_path, text_model_path):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # Audio Encoder (Wav2Vec2)\n",
    "#         self.audio_encoder = Wav2Vec2Model.from_pretrained(audio_model_path)\n",
    "        \n",
    "#         # Text Encoder (BERT)\n",
    "#         self.text_encoder = BertModel.from_pretrained(text_model_path)\n",
    "        \n",
    "#         # Freeze pretrained encoders (optional)\n",
    "#         for param in self.audio_encoder.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         for param in self.text_encoder.parameters():\n",
    "#             param.requires_grad = False\n",
    "        \n",
    "#         # Fusion Layer\n",
    "#         audio_feature_dim = self.audio_encoder.config.hidden_size\n",
    "#         text_feature_dim = self.text_encoder.config.hidden_size\n",
    "#         fusion_dim = audio_feature_dim + text_feature_dim\n",
    "        \n",
    "#         self.fusion_layers = nn.Sequential(\n",
    "#             nn.Linear(fusion_dim, fusion_dim // 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(fusion_dim // 2, num_labels)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "#         # Extract audio features\n",
    "#         audio_outputs = self.audio_encoder(\n",
    "#             audio_input, \n",
    "#             attention_mask=audio_mask\n",
    "#         )\n",
    "#         audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)\n",
    "        \n",
    "#         # Extract text features\n",
    "#         text_outputs = self.text_encoder(\n",
    "#             text_input_ids, \n",
    "#             attention_mask=text_attention_mask\n",
    "#         )\n",
    "#         text_features = text_outputs.pooler_output\n",
    "        \n",
    "#         # Concatenate features\n",
    "#         combined_features = torch.cat([audio_features, text_features], dim=1)\n",
    "        \n",
    "#         # Classification\n",
    "#         logits = self.fusion_layers(combined_features)\n",
    "        \n",
    "#         return logits\n",
    "\n",
    "# # Training Function\n",
    "# def train_model(model, train_loader, val_loader, device, epochs=10, learning_rate=1e-3):\n",
    "#     # Loss and Optimizer\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(\n",
    "#         [\n",
    "#             {'params': model.fusion_layers.parameters(), 'lr': learning_rate},\n",
    "#             # Uncomment and adjust learning rates if unfreezing more layers\n",
    "#             # {'params': model.audio_encoder.parameters(), 'lr': learning_rate * 0.1},\n",
    "#             # {'params': model.text_encoder.parameters(), 'lr': learning_rate * 0.1}\n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     best_val_accuracy = 0\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         # Training Phase\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         train_correct = 0\n",
    "#         train_total = 0\n",
    "        \n",
    "#         for batch in train_loader:\n",
    "#             # Move data to device\n",
    "#             audio_input = batch['audio_input'].to(device)\n",
    "#             audio_mask = batch['audio_mask'].to(device)\n",
    "#             text_input_ids = batch['text_input_ids'].to(device)\n",
    "#             text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "#             labels = batch['label'].to(device)\n",
    "            \n",
    "#             # Zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             # Forward pass\n",
    "#             outputs = model(audio_input, audio_mask, text_input_ids, text_attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             # Backward pass and optimize\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # Compute training metrics\n",
    "#             train_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             train_total += labels.size(0)\n",
    "#             train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "#         # Validation Phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         val_predictions = []\n",
    "#         val_true_labels = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 # Move data to device\n",
    "#                 audio_input = batch['audio_input'].to(device)\n",
    "#                 audio_mask = batch['audio_mask'].to(device)\n",
    "#                 text_input_ids = batch['text_input_ids'].to(device)\n",
    "#                 text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "#                 labels = batch['label'].to(device)\n",
    "                \n",
    "#                 # Forward pass\n",
    "#                 outputs = model(audio_input, audio_mask, text_input_ids, text_attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 # Compute validation metrics\n",
    "#                 val_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 val_total += labels.size(0)\n",
    "#                 val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "#                 val_predictions.extend(predicted.cpu().numpy())\n",
    "#                 val_true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "#         # Print epoch summary\n",
    "#         train_accuracy = 100 * train_correct / train_total\n",
    "#         val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "#         print(f'Epoch [{epoch+1}/{epochs}]')\n",
    "#         print(f'Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "#         print(f'Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "#         # Classification Report\n",
    "#         print(\"\\nClassification Report:\")\n",
    "#         print(classification_report(\n",
    "#             val_true_labels, \n",
    "#             val_predictions, \n",
    "#             target_names=list(train_dataset.emotion_to_idx.keys())\n",
    "#         ))\n",
    "        \n",
    "#         # Save best model\n",
    "#         if val_accuracy > best_val_accuracy:\n",
    "#             best_val_accuracy = val_accuracy\n",
    "#             torch.save(model.state_dict(), 'best_multimodal_emotion_model.pth')\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Inference Function\n",
    "# def predict_emotion(model, audio_path, text, processor, tokenizer, device):\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Process Audio\n",
    "#     speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "#     speech_array = speech_array.squeeze().numpy()\n",
    "#     speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, processor.feature_extractor.sampling_rate)\n",
    "    \n",
    "#     audio_inputs = processor(\n",
    "#         speech_array, \n",
    "#         sampling_rate=processor.feature_extractor.sampling_rate, \n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "    \n",
    "#     # Process Text\n",
    "#     text_inputs = tokenizer(\n",
    "#         text, \n",
    "#         max_length=128, \n",
    "#         padding='max_length', \n",
    "#         truncation=True, \n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "    \n",
    "#     # Move to device\n",
    "#     audio_input = audio_inputs.input_values.to(device).squeeze()\n",
    "#     audio_mask = audio_inputs.attention_mask.to(device).squeeze()\n",
    "#     text_input_ids = text_inputs['input_ids'].to(device).squeeze()\n",
    "#     text_attention_mask = text_inputs['attention_mask'].to(device).squeeze()\n",
    "    \n",
    "#     # Predict\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(\n",
    "#             audio_input.unsqueeze(0), \n",
    "#             audio_mask.unsqueeze(0), \n",
    "#             text_input_ids.unsqueeze(0), \n",
    "#             text_attention_mask.unsqueeze(0)\n",
    "#         )\n",
    "#         probabilities = torch.softmax(outputs, dim=1)\n",
    "#         predicted_class = torch.argmax(probabilities, dim=1)\n",
    "    \n",
    "#     return {\n",
    "#         'emotion': train_dataset.idx_to_emotion[predicted_class.item()],\n",
    "#         'probabilities': {emotion: prob.item() for emotion, prob in zip(train_dataset.idx_to_emotion.values(), probabilities[0])}\n",
    "#     }\n",
    "\n",
    "# # Main Execution\n",
    "# def main():\n",
    "#     # Set device\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     # Paths and Configurations\n",
    "#     audio_model_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "#     text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "#     # Initialize Processors\n",
    "#     audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "#     text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "    \n",
    "#     # Create Datasets\n",
    "#     train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "#     test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "    \n",
    "#     # DataLoaders\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "#     val_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "#     # Get number of labels\n",
    "#     num_labels = len(train_dataset.emotion_to_idx)\n",
    "    \n",
    "#     # Initialize Model\n",
    "#     model = MultimodalEmotionClassifier(\n",
    "#         num_labels=num_labels, \n",
    "#         audio_model_path=audio_model_path, \n",
    "#         text_model_path=text_model_path\n",
    "#     ).to(device)\n",
    "    \n",
    "#     # Train Model\n",
    "#     trained_model = train_model(model, train_loader, val_loader, device)\n",
    "    \n",
    "#     # Example Inference\n",
    "#     # Assuming you have a sample audio file and want to predict its emotion\n",
    "#     sample_audio_path = test_dataset.data.iloc[0]['path']\n",
    "#     sample_text = \"Some random descriptive text\"\n",
    "    \n",
    "#     prediction = predict_emotion(\n",
    "#         trained_model, \n",
    "#         sample_audio_path, \n",
    "#         sample_text, \n",
    "#         audio_processor, \n",
    "#         text_tokenizer, \n",
    "#         device\n",
    "#     )\n",
    "    \n",
    "#     print(\"\\nSample Prediction:\")\n",
    "#     print(\"Predicted Emotion:\", prediction['emotion'])\n",
    "#     print(\"Emotion Probabilities:\")\n",
    "#     for emotion, prob in prediction['probabilities'].items():\n",
    "#         print(f\"{emotion}: {prob:.4f}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c906106-3719-41b3-bee3-0750928b04b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 150\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 117\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m text_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Create inference object\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m inferencer \u001b[38;5;241m=\u001b[39m \u001b[43mMultimodalEmotionInference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_model_path\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Example usage - Single prediction\u001b[39;00m\n\u001b[0;32m    124\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrenzy/data/anger/a01 (2).wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[10], line 18\u001b[0m, in \u001b[0;36mMultimodalEmotionInference.__init__\u001b[1;34m(self, model_path, audio_model_path, text_model_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load processors\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, \n",
    "    BertTokenizer\n",
    ")\n",
    "\n",
    "class MultimodalEmotionInference:\n",
    "    def __init__(self, model_path, audio_model_path, text_model_path):\n",
    "        # Set device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = torch.load(model_path).to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load processors\n",
    "        self.audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "        self.text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "        \n",
    "        # Emotion mapping (reverse of what was used during training)\n",
    "        self.idx_to_emotion = {\n",
    "            0: 'happiness', \n",
    "            1: 'anger', \n",
    "            2: 'disgust', \n",
    "            3: 'sadness', \n",
    "            4: 'fear'\n",
    "        }\n",
    "    \n",
    "    def preprocess_audio(self, audio_path):\n",
    "        \"\"\"Preprocess audio file\"\"\"\n",
    "        speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "        speech_array = speech_array.squeeze().numpy()\n",
    "        speech_array = librosa.resample(\n",
    "            np.asarray(speech_array), \n",
    "            sampling_rate, \n",
    "            self.audio_processor.feature_extractor.sampling_rate\n",
    "        )\n",
    "        \n",
    "        audio_inputs = self.audio_processor(\n",
    "            speech_array, \n",
    "            sampling_rate=self.audio_processor.feature_extractor.sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return audio_inputs\n",
    "    \n",
    "    def preprocess_text(self, text, max_length=128):\n",
    "        \"\"\"Preprocess text input\"\"\"\n",
    "        text_inputs = self.text_tokenizer(\n",
    "            text, \n",
    "            max_length=max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return text_inputs\n",
    "    \n",
    "    def predict(self, audio_path, text):\n",
    "        \"\"\"Predict emotion for given audio and text\"\"\"\n",
    "        # Preprocess inputs\n",
    "        audio_inputs = self.preprocess_audio(audio_path)\n",
    "        text_inputs = self.preprocess_text(text)\n",
    "        \n",
    "        # Move to device\n",
    "        audio_input = audio_inputs.input_values.to(self.device)\n",
    "        audio_mask = audio_inputs.attention_mask.to(self.device)\n",
    "        text_input_ids = text_inputs['input_ids'].to(self.device)\n",
    "        text_attention_mask = text_inputs['attention_mask'].to(self.device)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(\n",
    "                audio_input, \n",
    "                audio_mask, \n",
    "                text_input_ids, \n",
    "                text_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        # Prepare results\n",
    "        results = [\n",
    "            {\n",
    "                \"emotion\": self.idx_to_emotion[i], \n",
    "                \"probability\": float(prob)\n",
    "            } \n",
    "            for i, prob in enumerate(probs)\n",
    "        ]\n",
    "        \n",
    "        # Sort by probability in descending order\n",
    "        results.sort(key=lambda x: x['probability'], reverse=True)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_predict(self, audio_paths, texts):\n",
    "        \"\"\"Batch prediction for multiple samples\"\"\"\n",
    "        results = []\n",
    "        for audio_path, text in zip(audio_paths, texts):\n",
    "            result = self.predict(audio_path, text)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    model_path = \"saved_models/best_multimodal_model.pth\"  # Assuming you saved the model\n",
    "    audio_model_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "    text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "    # Create inference object\n",
    "    inferencer = MultimodalEmotionInference(\n",
    "        model_path, \n",
    "        audio_model_path, \n",
    "        text_model_path\n",
    "    )\n",
    "    \n",
    "    # Example usage - Single prediction\n",
    "    audio_path = \"Frenzy/data/anger/a01 (2).wav\"\n",
    "    text = \"hi im angry\"\n",
    "    \n",
    "    single_result = inferencer.predict(audio_path, text)\n",
    "    print(\"Single Prediction:\")\n",
    "    for pred in single_result:\n",
    "        print(f\"{pred['emotion']}: {pred['probability']:.4f}\")\n",
    "    \n",
    "    # Example usage - Batch prediction\n",
    "    audio_paths = [\n",
    "        \"Frenzy/data/anger/a01 (2).wav\", \n",
    "        \"Frenzy/data/anger/a01 (3).wav\"\n",
    "    ]\n",
    "    texts = [\n",
    "        \"im angry\", \n",
    "        \"im angry\"\n",
    "    ]\n",
    "    \n",
    "    batch_results = inferencer.batch_predict(audio_paths, texts)\n",
    "    print(\"\\nBatch Prediction:\")\n",
    "    for i, results in enumerate(batch_results):\n",
    "        print(f\"\\nAudio {i+1}:\")\n",
    "        for pred in results:\n",
    "            print(f\"{pred['emotion']}: {pred['probability']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96e477-1be4-4792-907c-bc6698a4846d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
