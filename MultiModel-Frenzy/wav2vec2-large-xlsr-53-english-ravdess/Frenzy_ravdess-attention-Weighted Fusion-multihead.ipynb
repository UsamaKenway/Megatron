{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7484691-b98c-4c46-a857-8d21f3d1d70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultimodalEmotionClassifier(\n",
      "  (audio_encoder): Wav2Vec2Model(\n",
      "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (feature_projection): Wav2Vec2FeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
      "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "        (conv): ParametrizedConv1d(\n",
      "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (padding): Wav2Vec2SamePadLayer()\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
      "          (attention): Wav2Vec2SdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.05, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.05, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (text_encoder): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (audio_projection): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (text_projection): Linear(in_features=768, out_features=512, bias=True)\n",
      "  (multihead_attention): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "  )\n",
      "  (fusion_layers): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=512, out_features=5, bias=True)\n",
      "  )\n",
      "  (audio_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (text_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Model Initialized with 5 emotion classes\n",
      "Emotion to Index mapping: {'happiness': 0, 'disgust': 1, 'anger': 2, 'fear': 3, 'sadness': 4}\n",
      "Epoch 1/50\n",
      "Train Loss: 1.6168, Train Accuracy: 0.1906\n",
      "Val Loss: 1.6093, Val Accuracy: 0.1558\n",
      "Saved new best model with validation accuracy: 0.1558\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.03      0.05        31\n",
      "           1       1.00      0.00      0.00        34\n",
      "           2       0.19      0.64      0.29        33\n",
      "           3       0.06      0.07      0.07        29\n",
      "           4       1.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.16       154\n",
      "   macro avg       0.47      0.15      0.08       154\n",
      "weighted avg       0.47      0.16      0.08       154\n",
      "\n",
      "Epoch 2/50\n",
      "Train Loss: 1.6055, Train Accuracy: 0.2248\n",
      "Val Loss: 1.6065, Val Accuracy: 0.1818\n",
      "Saved new best model with validation accuracy: 0.1818\n",
      "Epoch 3/50\n",
      "Train Loss: 1.6002, Train Accuracy: 0.2264\n",
      "Val Loss: 1.6033, Val Accuracy: 0.2078\n",
      "Saved new best model with validation accuracy: 0.2078\n",
      "Epoch 4/50\n",
      "Train Loss: 1.5999, Train Accuracy: 0.2459\n",
      "Val Loss: 1.6021, Val Accuracy: 0.2273\n",
      "Saved new best model with validation accuracy: 0.2273\n",
      "Epoch 5/50\n",
      "Train Loss: 1.5885, Train Accuracy: 0.2687\n",
      "Val Loss: 1.5995, Val Accuracy: 0.2078\n",
      "Epoch 6/50\n",
      "Train Loss: 1.5846, Train Accuracy: 0.2638\n",
      "Val Loss: 1.5998, Val Accuracy: 0.2078\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.06      0.10        31\n",
      "           1       0.36      0.24      0.29        34\n",
      "           2       0.30      0.09      0.14        33\n",
      "           3       0.12      0.21      0.15        29\n",
      "           4       0.21      0.48      0.30        27\n",
      "\n",
      "    accuracy                           0.21       154\n",
      "   macro avg       0.24      0.22      0.19       154\n",
      "weighted avg       0.24      0.21      0.19       154\n",
      "\n",
      "Epoch 7/50\n",
      "Train Loss: 1.5849, Train Accuracy: 0.2866\n",
      "Val Loss: 1.6011, Val Accuracy: 0.1883\n",
      "Epoch 8/50\n",
      "Train Loss: 1.5870, Train Accuracy: 0.2492\n",
      "Val Loss: 1.6046, Val Accuracy: 0.2078\n",
      "Epoch 9/50\n",
      "Train Loss: 1.5761, Train Accuracy: 0.2752\n",
      "Val Loss: 1.6062, Val Accuracy: 0.2273\n",
      "Epoch 10/50\n",
      "Train Loss: 1.5873, Train Accuracy: 0.2899\n",
      "Val Loss: 1.6054, Val Accuracy: 0.2078\n",
      "Epoch 11/50\n",
      "Train Loss: 1.5762, Train Accuracy: 0.2638\n",
      "Val Loss: 1.6047, Val Accuracy: 0.2208\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.13      0.16        31\n",
      "           1       0.36      0.29      0.32        34\n",
      "           2       0.00      0.00      0.00        33\n",
      "           3       0.17      0.31      0.22        29\n",
      "           4       0.21      0.41      0.28        27\n",
      "\n",
      "    accuracy                           0.22       154\n",
      "   macro avg       0.19      0.23      0.20       154\n",
      "weighted avg       0.19      0.22      0.19       154\n",
      "\n",
      "Epoch 12/50\n",
      "Train Loss: 1.5796, Train Accuracy: 0.2573\n",
      "Val Loss: 1.6061, Val Accuracy: 0.2273\n",
      "Epoch 13/50\n",
      "Train Loss: 1.5690, Train Accuracy: 0.2704\n",
      "Val Loss: 1.6063, Val Accuracy: 0.2208\n",
      "Epoch 14/50\n",
      "Train Loss: 1.5704, Train Accuracy: 0.2883\n",
      "Val Loss: 1.6077, Val Accuracy: 0.2143\n",
      "Epoch 15/50\n",
      "Train Loss: 1.5759, Train Accuracy: 0.2638\n",
      "Val Loss: 1.6072, Val Accuracy: 0.2208\n",
      "Epoch 16/50\n",
      "Train Loss: 1.5649, Train Accuracy: 0.2997\n",
      "Val Loss: 1.6061, Val Accuracy: 0.2078\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.09      0.03      0.05        31\n",
      "           1       0.30      0.38      0.33        34\n",
      "           2       0.00      0.00      0.00        33\n",
      "           3       0.16      0.28      0.20        29\n",
      "           4       0.21      0.37      0.27        27\n",
      "\n",
      "    accuracy                           0.21       154\n",
      "   macro avg       0.15      0.21      0.17       154\n",
      "weighted avg       0.15      0.21      0.17       154\n",
      "\n",
      "Epoch 17/50\n",
      "Train Loss: 1.5685, Train Accuracy: 0.2801\n",
      "Val Loss: 1.6071, Val Accuracy: 0.2143\n",
      "Epoch 18/50\n",
      "Train Loss: 1.5698, Train Accuracy: 0.2541\n",
      "Val Loss: 1.6042, Val Accuracy: 0.2208\n",
      "Epoch 19/50\n",
      "Train Loss: 1.5766, Train Accuracy: 0.2899\n",
      "Val Loss: 1.6045, Val Accuracy: 0.2078\n",
      "Epoch 20/50\n",
      "Train Loss: 1.5639, Train Accuracy: 0.2899\n",
      "Val Loss: 1.6067, Val Accuracy: 0.2273\n",
      "Epoch 21/50\n",
      "Train Loss: 1.5580, Train Accuracy: 0.3029\n",
      "Val Loss: 1.6094, Val Accuracy: 0.2013\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.12      0.10      0.11        31\n",
      "           1       0.28      0.35      0.31        34\n",
      "           2       0.22      0.06      0.10        33\n",
      "           3       0.16      0.21      0.18        29\n",
      "           4       0.20      0.30      0.24        27\n",
      "\n",
      "    accuracy                           0.20       154\n",
      "   macro avg       0.20      0.20      0.19       154\n",
      "weighted avg       0.20      0.20      0.19       154\n",
      "\n",
      "Epoch 22/50\n",
      "Train Loss: 1.5643, Train Accuracy: 0.2769\n",
      "Val Loss: 1.6110, Val Accuracy: 0.1948\n",
      "Epoch 23/50\n",
      "Train Loss: 1.5667, Train Accuracy: 0.2850\n",
      "Val Loss: 1.6106, Val Accuracy: 0.2338\n",
      "Saved new best model with validation accuracy: 0.2338\n",
      "Epoch 24/50\n",
      "Train Loss: 1.5582, Train Accuracy: 0.3013\n",
      "Val Loss: 1.6110, Val Accuracy: 0.2078\n",
      "Epoch 25/50\n",
      "Train Loss: 1.5588, Train Accuracy: 0.2785\n",
      "Val Loss: 1.6106, Val Accuracy: 0.2338\n",
      "Epoch 26/50\n",
      "Train Loss: 1.5625, Train Accuracy: 0.2899\n",
      "Val Loss: 1.6122, Val Accuracy: 0.2403\n",
      "Saved new best model with validation accuracy: 0.2403\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.15      0.16      0.16        31\n",
      "           1       0.35      0.53      0.42        34\n",
      "           2       0.20      0.06      0.09        33\n",
      "           3       0.19      0.21      0.20        29\n",
      "           4       0.21      0.22      0.22        27\n",
      "\n",
      "    accuracy                           0.24       154\n",
      "   macro avg       0.22      0.24      0.22       154\n",
      "weighted avg       0.22      0.24      0.22       154\n",
      "\n",
      "Epoch 27/50\n",
      "Train Loss: 1.5478, Train Accuracy: 0.3029\n",
      "Val Loss: 1.6123, Val Accuracy: 0.2338\n",
      "Epoch 28/50\n",
      "Train Loss: 1.5503, Train Accuracy: 0.2866\n",
      "Val Loss: 1.6092, Val Accuracy: 0.2273\n",
      "Epoch 29/50\n",
      "Train Loss: 1.5565, Train Accuracy: 0.2850\n",
      "Val Loss: 1.6106, Val Accuracy: 0.2338\n",
      "Epoch 30/50\n",
      "Train Loss: 1.5577, Train Accuracy: 0.3078\n",
      "Val Loss: 1.6103, Val Accuracy: 0.2468\n",
      "Saved new best model with validation accuracy: 0.2468\n",
      "Epoch 31/50\n",
      "Train Loss: 1.5609, Train Accuracy: 0.2964\n",
      "Val Loss: 1.6108, Val Accuracy: 0.2597\n",
      "Saved new best model with validation accuracy: 0.2597\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.19      0.21        31\n",
      "           1       0.35      0.56      0.43        34\n",
      "           2       0.17      0.06      0.09        33\n",
      "           3       0.17      0.17      0.17        29\n",
      "           4       0.25      0.30      0.27        27\n",
      "\n",
      "    accuracy                           0.26       154\n",
      "   macro avg       0.23      0.26      0.23       154\n",
      "weighted avg       0.24      0.26      0.24       154\n",
      "\n",
      "Epoch 32/50\n",
      "Train Loss: 1.5644, Train Accuracy: 0.3127\n",
      "Val Loss: 1.6105, Val Accuracy: 0.2338\n",
      "Epoch 33/50\n",
      "Train Loss: 1.5537, Train Accuracy: 0.2801\n",
      "Val Loss: 1.6133, Val Accuracy: 0.2338\n",
      "Epoch 34/50\n",
      "Train Loss: 1.5552, Train Accuracy: 0.2948\n",
      "Val Loss: 1.6132, Val Accuracy: 0.2208\n",
      "Epoch 35/50\n",
      "Train Loss: 1.5618, Train Accuracy: 0.2883\n",
      "Val Loss: 1.6131, Val Accuracy: 0.2208\n",
      "Epoch 36/50\n",
      "Train Loss: 1.5593, Train Accuracy: 0.2948\n",
      "Val Loss: 1.6138, Val Accuracy: 0.2208\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.16      0.10      0.12        31\n",
      "           1       0.30      0.53      0.38        34\n",
      "           2       0.27      0.09      0.14        33\n",
      "           3       0.15      0.17      0.16        29\n",
      "           4       0.16      0.19      0.17        27\n",
      "\n",
      "    accuracy                           0.22       154\n",
      "   macro avg       0.21      0.21      0.19       154\n",
      "weighted avg       0.21      0.22      0.20       154\n",
      "\n",
      "Epoch 37/50\n",
      "Train Loss: 1.5378, Train Accuracy: 0.2980\n",
      "Val Loss: 1.6136, Val Accuracy: 0.2338\n",
      "Epoch 38/50\n",
      "Train Loss: 1.5615, Train Accuracy: 0.2915\n",
      "Val Loss: 1.6154, Val Accuracy: 0.2532\n",
      "Epoch 39/50\n",
      "Train Loss: 1.5578, Train Accuracy: 0.3127\n",
      "Val Loss: 1.6127, Val Accuracy: 0.2338\n",
      "Epoch 40/50\n",
      "Train Loss: 1.5496, Train Accuracy: 0.2883\n",
      "Val Loss: 1.6124, Val Accuracy: 0.2403\n",
      "Epoch 41/50\n",
      "Train Loss: 1.5438, Train Accuracy: 0.2883\n",
      "Val Loss: 1.6141, Val Accuracy: 0.2403\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.13      0.15        31\n",
      "           1       0.34      0.65      0.44        34\n",
      "           2       0.27      0.09      0.14        33\n",
      "           3       0.13      0.14      0.13        29\n",
      "           4       0.17      0.15      0.16        27\n",
      "\n",
      "    accuracy                           0.24       154\n",
      "   macro avg       0.22      0.23      0.20       154\n",
      "weighted avg       0.22      0.24      0.21       154\n",
      "\n",
      "Epoch 42/50\n",
      "Train Loss: 1.5572, Train Accuracy: 0.2883\n",
      "Val Loss: 1.6128, Val Accuracy: 0.2338\n",
      "Epoch 43/50\n",
      "Train Loss: 1.5439, Train Accuracy: 0.3062\n",
      "Val Loss: 1.6125, Val Accuracy: 0.2338\n",
      "Epoch 44/50\n",
      "Train Loss: 1.5459, Train Accuracy: 0.3062\n",
      "Val Loss: 1.6181, Val Accuracy: 0.2532\n",
      "Epoch 45/50\n",
      "Train Loss: 1.5517, Train Accuracy: 0.2932\n",
      "Val Loss: 1.6154, Val Accuracy: 0.2338\n",
      "Epoch 46/50\n",
      "Train Loss: 1.5407, Train Accuracy: 0.2964\n",
      "Val Loss: 1.6155, Val Accuracy: 0.2468\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.10      0.12        31\n",
      "           1       0.34      0.62      0.44        34\n",
      "           2       0.27      0.09      0.14        33\n",
      "           3       0.13      0.14      0.13        29\n",
      "           4       0.24      0.26      0.25        27\n",
      "\n",
      "    accuracy                           0.25       154\n",
      "   macro avg       0.22      0.24      0.21       154\n",
      "weighted avg       0.23      0.25      0.22       154\n",
      "\n",
      "Epoch 47/50\n",
      "Train Loss: 1.5450, Train Accuracy: 0.2948\n",
      "Val Loss: 1.6138, Val Accuracy: 0.2273\n",
      "Epoch 48/50\n",
      "Train Loss: 1.5437, Train Accuracy: 0.3176\n",
      "Val Loss: 1.6150, Val Accuracy: 0.2532\n",
      "Epoch 49/50\n",
      "Train Loss: 1.5463, Train Accuracy: 0.3094\n",
      "Val Loss: 1.6172, Val Accuracy: 0.2403\n",
      "Epoch 50/50\n",
      "Train Loss: 1.5382, Train Accuracy: 0.3208\n",
      "Val Loss: 1.6180, Val Accuracy: 0.2338\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   happiness       0.23      0.18      0.20        38\n",
      "     disgust       0.26      0.44      0.32        39\n",
      "       anger       0.07      0.03      0.04        38\n",
      "        fear       0.16      0.21      0.18        38\n",
      "     sadness       0.26      0.21      0.23        39\n",
      "\n",
      "    accuracy                           0.21       192\n",
      "   macro avg       0.19      0.21      0.20       192\n",
      "weighted avg       0.19      0.21      0.20       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import librosa\n",
    "import os\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, \n",
    "    Wav2Vec2Model, \n",
    "    BertModel, \n",
    "    BertTokenizer\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Custom Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultimodalEmotionDataset(Dataset):\n",
    "    def __init__(self, csv_path, processor, tokenizer, max_length=128, target_audio_length=16000):\n",
    "        # Read the CSV\n",
    "        self.data = pd.read_csv(csv_path, sep='\\t')\n",
    "        \n",
    "        # Add random text column if not exists\n",
    "        if 'text' not in self.data.columns:\n",
    "            print([f\"Random text for {name}\" for name in self.data['name']])\n",
    "            self.data['text'] = [f\"Random text for {name}\" for name in self.data['name']]\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.target_audio_length = target_audio_length\n",
    "        \n",
    "        # Mapping emotions to indices\n",
    "        self.emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.data['emotion'].unique())}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Load audio file\n",
    "        speech_array, sampling_rate = torchaudio.load(row['path'])  # (Channels, Samples)\n",
    "        \n",
    "        # Convert to mono (if stereo, take the first channel)\n",
    "        if speech_array.shape[0] > 1:\n",
    "            speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
    "        \n",
    "        speech_array = speech_array.squeeze().numpy()  # Convert to numpy array\n",
    "        \n",
    "        # Resample to target sampling rate\n",
    "        speech_array = librosa.resample(y=speech_array, orig_sr=sampling_rate, target_sr=self.processor.feature_extractor.sampling_rate)\n",
    "        \n",
    "        # Ensure fixed length\n",
    "        if len(speech_array) > self.target_audio_length:\n",
    "            speech_array = speech_array[:self.target_audio_length]\n",
    "        elif len(speech_array) < self.target_audio_length:\n",
    "            padding = self.target_audio_length - len(speech_array)\n",
    "            speech_array = np.pad(speech_array, (0, padding), mode='constant', constant_values=0)\n",
    "    \n",
    "        # Process audio with Wav2Vec2\n",
    "        audio_inputs = self.processor(\n",
    "            speech_array, \n",
    "            sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Process text\n",
    "        text_inputs = self.tokenizer(\n",
    "            row['text'], \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get emotion label\n",
    "        label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "        return {\n",
    "            'audio_input': audio_inputs.input_values.squeeze(),  # Ensure correct shape\n",
    "            'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "            'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "            'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     row = self.data.iloc[idx]\n",
    "        \n",
    "    #     # Process Audio\n",
    "    #     speech_array, sampling_rate = torchaudio.load(row['path'])\n",
    "    #     speech_array = speech_array.squeeze().numpy()\n",
    "    #     # speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, self.processor.feature_extractor.sampling_rate)\n",
    "    #     speech_array = librosa.resample(y=np.asarray(speech_array), orig_sr=sampling_rate, target_sr=self.processor.feature_extractor.sampling_rate)\n",
    "    #     # Pad or truncate audio\n",
    "    #     if len(speech_array) > self.target_audio_length:\n",
    "    #         speech_array = speech_array[:self.target_audio_length]\n",
    "    #     elif len(speech_array) < self.target_audio_length:\n",
    "    #         padding = self.target_audio_length - len(speech_array)\n",
    "    #         speech_array = np.pad(speech_array, (0, padding), mode='constant', constant_values=0)\n",
    "        \n",
    "    #     audio_inputs = self.processor(\n",
    "    #         speech_array, \n",
    "    #         sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "    #         return_tensors=\"pt\"\n",
    "    #     )\n",
    "        \n",
    "    #     # Process Text\n",
    "    #     text_inputs = self.tokenizer(\n",
    "    #         row['text'], \n",
    "    #         max_length=self.max_length, \n",
    "    #         padding='max_length', \n",
    "    #         truncation=True, \n",
    "    #         return_tensors=\"pt\"\n",
    "    #     )\n",
    "        \n",
    "    #     # Get emotion label\n",
    "    #     label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "    #     return {\n",
    "    #         'audio_input': audio_inputs.input_values.squeeze(),\n",
    "    #         'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "    #         'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "    #         'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "    #         'label': label\n",
    "    #     }\n",
    "\n",
    "\n",
    "class MultimodalEmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, audio_model_path, text_model_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Audio Encoder (Wav2Vec2)\n",
    "        self.audio_encoder = Wav2Vec2Model.from_pretrained(audio_model_path)\n",
    "        \n",
    "        # Text Encoder (BERT)\n",
    "        self.text_encoder = BertModel.from_pretrained(text_model_path)\n",
    "        \n",
    "        # Freeze pretrained encoders (optional)\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Feature dimensions\n",
    "        self.audio_feature_dim = self.audio_encoder.config.hidden_size\n",
    "        self.text_feature_dim = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        # Projection layers to common dimension\n",
    "        common_dim = 512\n",
    "        self.audio_projection = nn.Linear(self.audio_feature_dim, common_dim)\n",
    "        self.text_projection = nn.Linear(self.text_feature_dim, common_dim)\n",
    "\n",
    "        # Multi-head attention layer to fuse audio and text\n",
    "        self.multihead_attention = nn.MultiheadAttention(embed_dim=common_dim, num_heads=8, batch_first=True)\n",
    "        \n",
    "        # Fusion Layer\n",
    "        self.fusion_layers = nn.Sequential(\n",
    "            nn.Linear(common_dim * 2, common_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(common_dim, num_labels)\n",
    "        )\n",
    "        \n",
    "        self.audio_norm = nn.LayerNorm(self.audio_feature_dim)\n",
    "        self.text_norm = nn.LayerNorm(self.text_feature_dim)\n",
    "\n",
    "    \n",
    "    # Prioritize audio by applying more weight\n",
    "    def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "        audio_features = torch.mean(self.audio_encoder(audio_input, attention_mask=audio_mask).last_hidden_state, dim=1)\n",
    "        text_features = self.text_encoder(text_input_ids, attention_mask=text_attention_mask).pooler_output\n",
    "    \n",
    "        audio_features = self.audio_norm(audio_features)\n",
    "        text_features = self.text_norm(text_features)\n",
    "        \n",
    "        audio_projected = self.audio_projection(audio_features)\n",
    "        text_projected = self.text_projection(text_features)\n",
    "    \n",
    "        # Fusion layer with more weight for audio\n",
    "        # combined = torch.cat([audio_projected * 0.7, text_projected * 0.3], dim=1)\n",
    "        combined = torch.cat([audio_projected * 0.9, text_projected * 0.1], dim=1)\n",
    "    \n",
    "        return self.fusion_layers(combined)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "    #     # Extract audio features\n",
    "    #     audio_outputs = self.audio_encoder(\n",
    "    #         audio_input, \n",
    "    #         attention_mask=audio_mask\n",
    "    #     )\n",
    "    #     audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)\n",
    "        \n",
    "    #     # Extract text features\n",
    "    #     text_outputs = self.text_encoder(\n",
    "    #         text_input_ids, \n",
    "    #         attention_mask=text_attention_mask\n",
    "    #     )\n",
    "    #     text_features = text_outputs.pooler_output\n",
    "        \n",
    "    #     # Concatenate features\n",
    "    #     combined_features = torch.cat([audio_features, text_features], dim=1)\n",
    "        \n",
    "    #     # Classification\n",
    "    #     logits = self.fusion_layers(combined_features)\n",
    "        \n",
    "    #     return logits\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10, learning_rate=1e-5, text_penalty=0.1):\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('saved_models', exist_ok=True)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_true = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move data to device\n",
    "            audio_input = batch['audio_input'].to(device)\n",
    "            audio_mask = batch['audio_mask'].to(device)\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs_tuple = model(\n",
    "                audio_input, \n",
    "                audio_mask, \n",
    "                text_input_ids, \n",
    "                text_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Handle the tuple return value\n",
    "            if isinstance(outputs_tuple, tuple):\n",
    "                outputs, attention_weights = outputs_tuple\n",
    "                \n",
    "                # Compute classification loss\n",
    "                class_loss = criterion(outputs, labels)\n",
    "                \n",
    "                # Add penalty for relying too much on text features\n",
    "                text_reliance_penalty = text_penalty * attention_weights[:, 1].mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = class_loss + text_reliance_penalty\n",
    "            else:\n",
    "                # For backward compatibility with original model\n",
    "                outputs = outputs_tuple\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track training metrics\n",
    "            train_losses.append(loss.item())\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Move data to device\n",
    "                audio_input = batch['audio_input'].to(device)\n",
    "                audio_mask = batch['audio_mask'].to(device)\n",
    "                text_input_ids = batch['text_input_ids'].to(device)\n",
    "                text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs_tuple = model(\n",
    "                    audio_input, \n",
    "                    audio_mask, \n",
    "                    text_input_ids, \n",
    "                    text_attention_mask\n",
    "                )\n",
    "                \n",
    "                # Handle the tuple return value\n",
    "                if isinstance(outputs_tuple, tuple):\n",
    "                    outputs, _ = outputs_tuple\n",
    "                    loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    outputs = outputs_tuple\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                # Track validation metrics\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Compute metrics\n",
    "        train_accuracy = accuracy_score(train_true, train_preds)\n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {np.mean(val_losses):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_accuracy': best_val_accuracy,\n",
    "                'epoch': epoch\n",
    "            }, f'saved_models/best_multimodal_model.pth')\n",
    "            \n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_accuracy:.4f}\")\n",
    "        \n",
    "        # Optional: Print classification report for validation set\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"\\nValidation Classification Report:\")\n",
    "            print(classification_report(val_true, val_preds, zero_division=1))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Paths and Configurations\n",
    "    audio_model_path = \"./emotion_recognition_model\"\n",
    "    text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "    # Initialize Processors\n",
    "    audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "    text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "    \n",
    "    # Create Datasets\n",
    "    train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "    test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "    \n",
    "    # Split train into train and validation\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, \n",
    "        [int(len(train_dataset)*0.8), len(train_dataset)-int(len(train_dataset)*0.8)]\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Get number of labels\n",
    "    num_labels = len(train_dataset.dataset.emotion_to_idx)\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    print(model)\n",
    "    print(f\"Model Initialized with {num_labels} emotion classes\")\n",
    "    print(\"Emotion to Index mapping:\", train_dataset.dataset.emotion_to_idx)\n",
    "    \n",
    "    # Train Model\n",
    "    trained_model = train_model(model, train_loader, val_loader, device, epochs=50)\n",
    "    \n",
    "    # Optional: Load and evaluate best saved model\n",
    "    best_model_path = 'saved_models/best_multimodal_model.pth'\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    \n",
    "    # Reinitialize model and load state dict\n",
    "    best_model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Test model\n",
    "    # Test model\n",
    "    best_model.eval()\n",
    "    test_preds = []\n",
    "    test_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Move data to device\n",
    "            audio_input = batch['audio_input'].to(device)\n",
    "            audio_mask = batch['audio_mask'].to(device)\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs_tuple = best_model(\n",
    "                audio_input, \n",
    "                audio_mask, \n",
    "                text_input_ids, \n",
    "                text_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Handle the tuple return value\n",
    "            if isinstance(outputs_tuple, tuple):\n",
    "                outputs, _ = outputs_tuple\n",
    "            else:\n",
    "                outputs = outputs_tuple\n",
    "            \n",
    "            # Track test metrics\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_true.extend(labels.cpu().numpy())\n",
    "    # Print test classification report\n",
    "    print(\"\\nTest Classification Report:\")\n",
    "    print(classification_report(test_true, test_preds, \n",
    "        target_names=list(train_dataset.dataset.emotion_to_idx.keys()), zero_division=1))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0dd7a-0cd2-4857-b158-9b054a889ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
