{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7484691-b98c-4c46-a857-8d21f3d1d70a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultimodalEmotionClassifier(\n",
      "  (audio_encoder): Wav2Vec2Model(\n",
      "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
      "      (conv_layers): ModuleList(\n",
      "        (0): Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
      "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
      "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (feature_projection): Wav2Vec2FeatureProjection(\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "    )\n",
      "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
      "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
      "        (conv): ParametrizedConv1d(\n",
      "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
      "          (parametrizations): ModuleDict(\n",
      "            (weight): ParametrizationList(\n",
      "              (0): _WeightNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (padding): Wav2Vec2SamePadLayer()\n",
      "        (activation): GELUActivation()\n",
      "      )\n",
      "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.05, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
      "          (attention): Wav2Vec2SdpaAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.05, inplace=False)\n",
      "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Wav2Vec2FeedForward(\n",
      "            (intermediate_dropout): Dropout(p=0.05, inplace=False)\n",
      "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (output_dropout): Dropout(p=0.05, inplace=False)\n",
      "          )\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (text_encoder): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (fusion_layers): Sequential(\n",
      "    (0): Linear(in_features=1792, out_features=896, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=896, out_features=5, bias=True)\n",
      "  )\n",
      "  (audio_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (text_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "Model Initialized with 5 emotion classes\n",
      "Emotion to Index mapping: {'happiness': 0, 'disgust': 1, 'anger': 2, 'fear': 3, 'sadness': 4}\n",
      "Epoch 1/50\n",
      "Train Loss: 1.6905, Train Accuracy: 0.2085\n",
      "Val Loss: 1.6714, Val Accuracy: 0.3182\n",
      "Saved new best model with validation accuracy: 0.3182\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.31      0.29        32\n",
      "           1       0.32      0.73      0.44        33\n",
      "           2       0.40      0.07      0.11        30\n",
      "           3       0.38      0.38      0.38        32\n",
      "           4       0.20      0.04      0.06        27\n",
      "\n",
      "    accuracy                           0.32       154\n",
      "   macro avg       0.31      0.30      0.26       154\n",
      "weighted avg       0.32      0.32      0.27       154\n",
      "\n",
      "Epoch 2/50\n",
      "Train Loss: 1.6436, Train Accuracy: 0.2508\n",
      "Val Loss: 1.6845, Val Accuracy: 0.1818\n",
      "Epoch 3/50\n",
      "Train Loss: 1.6379, Train Accuracy: 0.2427\n",
      "Val Loss: 1.6312, Val Accuracy: 0.2792\n",
      "Epoch 4/50\n",
      "Train Loss: 1.6254, Train Accuracy: 0.2655\n",
      "Val Loss: 1.6368, Val Accuracy: 0.2338\n",
      "Epoch 5/50\n",
      "Train Loss: 1.6218, Train Accuracy: 0.2508\n",
      "Val Loss: 1.7044, Val Accuracy: 0.2013\n",
      "Epoch 6/50\n",
      "Train Loss: 1.5966, Train Accuracy: 0.2785\n",
      "Val Loss: 1.6304, Val Accuracy: 0.2922\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.28      0.28        32\n",
      "           1       0.46      0.36      0.41        33\n",
      "           2       0.32      0.30      0.31        30\n",
      "           3       0.29      0.12      0.17        32\n",
      "           4       0.20      0.41      0.27        27\n",
      "\n",
      "    accuracy                           0.29       154\n",
      "   macro avg       0.31      0.30      0.29       154\n",
      "weighted avg       0.32      0.29      0.29       154\n",
      "\n",
      "Epoch 7/50\n",
      "Train Loss: 1.6100, Train Accuracy: 0.2866\n",
      "Val Loss: 1.6309, Val Accuracy: 0.2597\n",
      "Epoch 8/50\n",
      "Train Loss: 1.5765, Train Accuracy: 0.2752\n",
      "Val Loss: 1.6165, Val Accuracy: 0.2792\n",
      "Epoch 9/50\n",
      "Train Loss: 1.6301, Train Accuracy: 0.2524\n",
      "Val Loss: 1.6234, Val Accuracy: 0.2403\n",
      "Epoch 10/50\n",
      "Train Loss: 1.5988, Train Accuracy: 0.2769\n",
      "Val Loss: 1.6091, Val Accuracy: 0.2727\n",
      "Epoch 11/50\n",
      "Train Loss: 1.5705, Train Accuracy: 0.2818\n",
      "Val Loss: 1.6193, Val Accuracy: 0.2078\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.06      0.08        32\n",
      "           1       0.36      0.30      0.33        33\n",
      "           2       0.18      0.07      0.10        30\n",
      "           3       0.24      0.16      0.19        32\n",
      "           4       0.17      0.48      0.25        27\n",
      "\n",
      "    accuracy                           0.21       154\n",
      "   macro avg       0.21      0.21      0.19       154\n",
      "weighted avg       0.21      0.21      0.19       154\n",
      "\n",
      "Epoch 12/50\n",
      "Train Loss: 1.5795, Train Accuracy: 0.2671\n",
      "Val Loss: 1.6224, Val Accuracy: 0.2403\n",
      "Epoch 13/50\n",
      "Train Loss: 1.5829, Train Accuracy: 0.2818\n",
      "Val Loss: 1.5896, Val Accuracy: 0.2338\n",
      "Epoch 14/50\n",
      "Train Loss: 1.5664, Train Accuracy: 0.2801\n",
      "Val Loss: 1.5982, Val Accuracy: 0.2597\n",
      "Epoch 15/50\n",
      "Train Loss: 1.5789, Train Accuracy: 0.2720\n",
      "Val Loss: 1.5957, Val Accuracy: 0.2597\n",
      "Epoch 16/50\n",
      "Train Loss: 1.5761, Train Accuracy: 0.2622\n",
      "Val Loss: 1.5726, Val Accuracy: 0.2597\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.03      0.05        32\n",
      "           1       0.29      0.76      0.42        33\n",
      "           2       0.34      0.33      0.34        30\n",
      "           3       0.15      0.12      0.14        32\n",
      "           4       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.26       154\n",
      "   macro avg       0.19      0.25      0.19       154\n",
      "weighted avg       0.20      0.26      0.20       154\n",
      "\n",
      "Epoch 17/50\n",
      "Train Loss: 1.5721, Train Accuracy: 0.2801\n",
      "Val Loss: 1.5814, Val Accuracy: 0.2662\n",
      "Epoch 18/50\n",
      "Train Loss: 1.5672, Train Accuracy: 0.2932\n",
      "Val Loss: 1.5866, Val Accuracy: 0.2987\n",
      "Epoch 19/50\n",
      "Train Loss: 1.5747, Train Accuracy: 0.2834\n",
      "Val Loss: 1.5790, Val Accuracy: 0.2597\n",
      "Epoch 20/50\n",
      "Train Loss: 1.5630, Train Accuracy: 0.2883\n",
      "Val Loss: 1.6038, Val Accuracy: 0.2338\n",
      "Epoch 21/50\n",
      "Train Loss: 1.5574, Train Accuracy: 0.2997\n",
      "Val Loss: 1.5989, Val Accuracy: 0.2662\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.50      0.32        32\n",
      "           1       0.33      0.48      0.40        33\n",
      "           2       0.23      0.20      0.21        30\n",
      "           3       0.25      0.09      0.14        32\n",
      "           4       0.00      0.00      0.00        27\n",
      "\n",
      "    accuracy                           0.27       154\n",
      "   macro avg       0.21      0.26      0.21       154\n",
      "weighted avg       0.22      0.27      0.22       154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50\n",
      "Train Loss: 1.5686, Train Accuracy: 0.2980\n",
      "Val Loss: 1.5883, Val Accuracy: 0.2468\n",
      "Epoch 23/50\n",
      "Train Loss: 1.5625, Train Accuracy: 0.3046\n",
      "Val Loss: 1.6077, Val Accuracy: 0.2403\n",
      "Epoch 24/50\n",
      "Train Loss: 1.5660, Train Accuracy: 0.2720\n",
      "Val Loss: 1.6174, Val Accuracy: 0.2208\n",
      "Epoch 25/50\n",
      "Train Loss: 1.5485, Train Accuracy: 0.2915\n",
      "Val Loss: 1.5796, Val Accuracy: 0.2987\n",
      "Epoch 26/50\n",
      "Train Loss: 1.5814, Train Accuracy: 0.2508\n",
      "Val Loss: 1.5825, Val Accuracy: 0.2987\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.09      0.15        32\n",
      "           1       0.32      0.82      0.46        33\n",
      "           2       0.41      0.23      0.30        30\n",
      "           3       0.26      0.16      0.20        32\n",
      "           4       0.15      0.15      0.15        27\n",
      "\n",
      "    accuracy                           0.30       154\n",
      "   macro avg       0.31      0.29      0.25       154\n",
      "weighted avg       0.32      0.30      0.26       154\n",
      "\n",
      "Epoch 27/50\n",
      "Train Loss: 1.5636, Train Accuracy: 0.2785\n",
      "Val Loss: 1.5950, Val Accuracy: 0.2662\n",
      "Epoch 28/50\n",
      "Train Loss: 1.5614, Train Accuracy: 0.2866\n",
      "Val Loss: 1.5878, Val Accuracy: 0.2792\n",
      "Epoch 29/50\n",
      "Train Loss: 1.5435, Train Accuracy: 0.3094\n",
      "Val Loss: 1.5936, Val Accuracy: 0.2662\n",
      "Epoch 30/50\n",
      "Train Loss: 1.5592, Train Accuracy: 0.2801\n",
      "Val Loss: 1.5892, Val Accuracy: 0.2857\n",
      "Epoch 31/50\n",
      "Train Loss: 1.5380, Train Accuracy: 0.3062\n",
      "Val Loss: 1.5881, Val Accuracy: 0.2987\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.23      0.22      0.22        32\n",
      "           1       0.35      0.64      0.45        33\n",
      "           2       0.42      0.17      0.24        30\n",
      "           3       0.31      0.31      0.31        32\n",
      "           4       0.16      0.11      0.13        27\n",
      "\n",
      "    accuracy                           0.30       154\n",
      "   macro avg       0.29      0.29      0.27       154\n",
      "weighted avg       0.30      0.30      0.28       154\n",
      "\n",
      "Epoch 32/50\n",
      "Train Loss: 1.5653, Train Accuracy: 0.2785\n",
      "Val Loss: 1.5770, Val Accuracy: 0.3247\n",
      "Saved new best model with validation accuracy: 0.3247\n",
      "Epoch 33/50\n",
      "Train Loss: 1.5719, Train Accuracy: 0.2687\n",
      "Val Loss: 1.5964, Val Accuracy: 0.2338\n",
      "Epoch 34/50\n",
      "Train Loss: 1.5526, Train Accuracy: 0.2850\n",
      "Val Loss: 1.6181, Val Accuracy: 0.2662\n",
      "Epoch 35/50\n",
      "Train Loss: 1.5465, Train Accuracy: 0.2948\n",
      "Val Loss: 1.6125, Val Accuracy: 0.2662\n",
      "Epoch 36/50\n",
      "Train Loss: 1.5356, Train Accuracy: 0.2997\n",
      "Val Loss: 1.5889, Val Accuracy: 0.2792\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.11      0.09      0.10        32\n",
      "           1       0.33      0.79      0.46        33\n",
      "           2       0.50      0.20      0.29        30\n",
      "           3       0.25      0.19      0.21        32\n",
      "           4       0.17      0.07      0.10        27\n",
      "\n",
      "    accuracy                           0.28       154\n",
      "   macro avg       0.27      0.27      0.23       154\n",
      "weighted avg       0.27      0.28      0.24       154\n",
      "\n",
      "Epoch 37/50\n",
      "Train Loss: 1.5339, Train Accuracy: 0.2769\n",
      "Val Loss: 1.5970, Val Accuracy: 0.2662\n",
      "Epoch 38/50\n",
      "Train Loss: 1.5453, Train Accuracy: 0.2769\n",
      "Val Loss: 1.5933, Val Accuracy: 0.3312\n",
      "Saved new best model with validation accuracy: 0.3312\n",
      "Epoch 39/50\n",
      "Train Loss: 1.5505, Train Accuracy: 0.2964\n",
      "Val Loss: 1.6000, Val Accuracy: 0.2857\n",
      "Epoch 40/50\n",
      "Train Loss: 1.5309, Train Accuracy: 0.3094\n",
      "Val Loss: 1.5804, Val Accuracy: 0.2857\n",
      "Epoch 41/50\n",
      "Train Loss: 1.5355, Train Accuracy: 0.2883\n",
      "Val Loss: 1.5840, Val Accuracy: 0.3117\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.28      0.22      0.25        32\n",
      "           1       0.34      0.79      0.48        33\n",
      "           2       0.35      0.20      0.26        30\n",
      "           3       0.33      0.25      0.29        32\n",
      "           4       0.08      0.04      0.05        27\n",
      "\n",
      "    accuracy                           0.31       154\n",
      "   macro avg       0.28      0.30      0.26       154\n",
      "weighted avg       0.28      0.31      0.27       154\n",
      "\n",
      "Epoch 42/50\n",
      "Train Loss: 1.5232, Train Accuracy: 0.2980\n",
      "Val Loss: 1.5718, Val Accuracy: 0.3117\n",
      "Epoch 43/50\n",
      "Train Loss: 1.5496, Train Accuracy: 0.3143\n",
      "Val Loss: 1.5830, Val Accuracy: 0.2597\n",
      "Epoch 44/50\n",
      "Train Loss: 1.5253, Train Accuracy: 0.2997\n",
      "Val Loss: 1.5840, Val Accuracy: 0.3117\n",
      "Epoch 45/50\n",
      "Train Loss: 1.5394, Train Accuracy: 0.2964\n",
      "Val Loss: 1.5858, Val Accuracy: 0.2727\n",
      "Epoch 46/50\n",
      "Train Loss: 1.5525, Train Accuracy: 0.2948\n",
      "Val Loss: 1.5846, Val Accuracy: 0.2662\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.21      0.25      0.23        32\n",
      "           1       0.33      0.58      0.42        33\n",
      "           2       0.21      0.10      0.14        30\n",
      "           3       0.33      0.28      0.31        32\n",
      "           4       0.12      0.07      0.09        27\n",
      "\n",
      "    accuracy                           0.27       154\n",
      "   macro avg       0.24      0.26      0.24       154\n",
      "weighted avg       0.25      0.27      0.24       154\n",
      "\n",
      "Epoch 47/50\n",
      "Train Loss: 1.5466, Train Accuracy: 0.2932\n",
      "Val Loss: 1.5725, Val Accuracy: 0.3182\n",
      "Epoch 48/50\n",
      "Train Loss: 1.5371, Train Accuracy: 0.2801\n",
      "Val Loss: 1.5794, Val Accuracy: 0.2987\n",
      "Epoch 49/50\n",
      "Train Loss: 1.5464, Train Accuracy: 0.3192\n",
      "Val Loss: 1.5873, Val Accuracy: 0.3182\n",
      "Epoch 50/50\n",
      "Train Loss: 1.5426, Train Accuracy: 0.2818\n",
      "Val Loss: 1.5865, Val Accuracy: 0.3117\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   happiness       0.00      0.00      0.00        38\n",
      "     disgust       0.28      0.54      0.37        39\n",
      "       anger       0.21      0.16      0.18        38\n",
      "        fear       0.19      0.34      0.25        38\n",
      "     sadness       0.24      0.13      0.17        39\n",
      "\n",
      "    accuracy                           0.23       192\n",
      "   macro avg       0.18      0.23      0.19       192\n",
      "weighted avg       0.19      0.23      0.19       192\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import librosa\n",
    "import os\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, \n",
    "    Wav2Vec2Model, \n",
    "    BertModel, \n",
    "    BertTokenizer\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Custom Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultimodalEmotionDataset(Dataset):\n",
    "    def __init__(self, csv_path, processor, tokenizer, max_length=128, target_audio_length=16000):\n",
    "        # Read the CSV\n",
    "        self.data = pd.read_csv(csv_path, sep='\\t')\n",
    "        \n",
    "        # Add random text column if not exists\n",
    "        if 'text' not in self.data.columns:\n",
    "            print([f\"Random text for {name}\" for name in self.data['name']])\n",
    "            self.data['text'] = [f\"Random text for {name}\" for name in self.data['name']]\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.target_audio_length = target_audio_length\n",
    "        \n",
    "        # Mapping emotions to indices\n",
    "        self.emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.data['emotion'].unique())}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Load audio file\n",
    "        speech_array, sampling_rate = torchaudio.load(row['path'])  # (Channels, Samples)\n",
    "        \n",
    "        # Convert to mono (if stereo, take the first channel)\n",
    "        if speech_array.shape[0] > 1:\n",
    "            speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
    "        \n",
    "        speech_array = speech_array.squeeze().numpy()  # Convert to numpy array\n",
    "        \n",
    "        # Resample to target sampling rate\n",
    "        speech_array = librosa.resample(y=speech_array, orig_sr=sampling_rate, target_sr=self.processor.feature_extractor.sampling_rate)\n",
    "        \n",
    "        # Ensure fixed length\n",
    "        if len(speech_array) > self.target_audio_length:\n",
    "            speech_array = speech_array[:self.target_audio_length]\n",
    "        elif len(speech_array) < self.target_audio_length:\n",
    "            padding = self.target_audio_length - len(speech_array)\n",
    "            speech_array = np.pad(speech_array, (0, padding), mode='constant', constant_values=0)\n",
    "    \n",
    "        # Process audio with Wav2Vec2\n",
    "        audio_inputs = self.processor(\n",
    "            speech_array, \n",
    "            sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Process text\n",
    "        text_inputs = self.tokenizer(\n",
    "            row['text'], \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get emotion label\n",
    "        label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "        return {\n",
    "            'audio_input': audio_inputs.input_values.squeeze(),  # Ensure correct shape\n",
    "            'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "            'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "            'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "\n",
    "    # def __getitem__(self, idx):\n",
    "    #     row = self.data.iloc[idx]\n",
    "        \n",
    "    #     # Process Audio\n",
    "    #     speech_array, sampling_rate = torchaudio.load(row['path'])\n",
    "    #     speech_array = speech_array.squeeze().numpy()\n",
    "    #     # speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, self.processor.feature_extractor.sampling_rate)\n",
    "    #     speech_array = librosa.resample(y=np.asarray(speech_array), orig_sr=sampling_rate, target_sr=self.processor.feature_extractor.sampling_rate)\n",
    "    #     # Pad or truncate audio\n",
    "    #     if len(speech_array) > self.target_audio_length:\n",
    "    #         speech_array = speech_array[:self.target_audio_length]\n",
    "    #     elif len(speech_array) < self.target_audio_length:\n",
    "    #         padding = self.target_audio_length - len(speech_array)\n",
    "    #         speech_array = np.pad(speech_array, (0, padding), mode='constant', constant_values=0)\n",
    "        \n",
    "    #     audio_inputs = self.processor(\n",
    "    #         speech_array, \n",
    "    #         sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "    #         return_tensors=\"pt\"\n",
    "    #     )\n",
    "        \n",
    "    #     # Process Text\n",
    "    #     text_inputs = self.tokenizer(\n",
    "    #         row['text'], \n",
    "    #         max_length=self.max_length, \n",
    "    #         padding='max_length', \n",
    "    #         truncation=True, \n",
    "    #         return_tensors=\"pt\"\n",
    "    #     )\n",
    "        \n",
    "    #     # Get emotion label\n",
    "    #     label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "    #     return {\n",
    "    #         'audio_input': audio_inputs.input_values.squeeze(),\n",
    "    #         'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "    #         'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "    #         'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "    #         'label': label\n",
    "    #     }\n",
    "\n",
    "\n",
    "class MultimodalEmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, audio_model_path, text_model_path, audio_weight=2.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Audio Encoder (Wav2Vec2)\n",
    "        self.audio_encoder = Wav2Vec2Model.from_pretrained(audio_model_path)\n",
    "        \n",
    "        # Text Encoder (BERT)\n",
    "        self.text_encoder = BertModel.from_pretrained(text_model_path)\n",
    "        \n",
    "        # Freeze pretrained encoders (optional)\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Feature dimensions\n",
    "        self.audio_feature_dim = self.audio_encoder.config.hidden_size\n",
    "        self.text_feature_dim = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        # Audio weighting factor\n",
    "        self.audio_weight = audio_weight\n",
    "        \n",
    "        # Fusion Layer\n",
    "        fusion_dim = self.audio_feature_dim + self.text_feature_dim\n",
    "        \n",
    "        self.fusion_layers = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, fusion_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(fusion_dim // 2, num_labels)\n",
    "        )\n",
    "        \n",
    "        self.audio_norm = nn.LayerNorm(self.audio_feature_dim)\n",
    "        self.text_norm = nn.LayerNorm(self.text_feature_dim)\n",
    "\n",
    "    def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "        # Extract audio features\n",
    "        audio_outputs = self.audio_encoder(audio_input, attention_mask=audio_mask)\n",
    "        audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)  # Mean pooling\n",
    "        audio_features = self.audio_norm(audio_features)  # Apply LayerNorm\n",
    "        \n",
    "        # Apply weighting to audio features\n",
    "        weighted_audio_features = audio_features * self.audio_weight\n",
    "    \n",
    "        # Extract text features\n",
    "        text_outputs = self.text_encoder(text_input_ids, attention_mask=text_attention_mask)\n",
    "        text_features = text_outputs.pooler_output  # [CLS] token representation\n",
    "        text_features = self.text_norm(text_features)  # Apply LayerNorm\n",
    "    \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([weighted_audio_features, text_features], dim=1)\n",
    "    \n",
    "        # Classification\n",
    "        logits = self.fusion_layers(combined_features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    # def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "    #     # Extract audio features\n",
    "    #     audio_outputs = self.audio_encoder(\n",
    "    #         audio_input, \n",
    "    #         attention_mask=audio_mask\n",
    "    #     )\n",
    "    #     audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)\n",
    "        \n",
    "    #     # Extract text features\n",
    "    #     text_outputs = self.text_encoder(\n",
    "    #         text_input_ids, \n",
    "    #         attention_mask=text_attention_mask\n",
    "    #     )\n",
    "    #     text_features = text_outputs.pooler_output\n",
    "        \n",
    "    #     # Concatenate features\n",
    "    #     combined_features = torch.cat([audio_features, text_features], dim=1)\n",
    "        \n",
    "    #     # Classification\n",
    "    #     logits = self.fusion_layers(combined_features)\n",
    "        \n",
    "    #     return logits\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10, learning_rate=1e-4):\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('saved_models', exist_ok=True)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_true = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move data to device\n",
    "            audio_input = batch['audio_input'].to(device)\n",
    "            audio_mask = batch['audio_mask'].to(device)\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                audio_input, \n",
    "                audio_mask, \n",
    "                text_input_ids, \n",
    "                text_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track training metrics\n",
    "            train_losses.append(loss.item())\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Move data to device\n",
    "                audio_input = batch['audio_input'].to(device)\n",
    "                audio_mask = batch['audio_mask'].to(device)\n",
    "                text_input_ids = batch['text_input_ids'].to(device)\n",
    "                text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    audio_input, \n",
    "                    audio_mask, \n",
    "                    text_input_ids, \n",
    "                    text_attention_mask\n",
    "                )\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                # Track validation metrics\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Compute metrics\n",
    "        train_accuracy = accuracy_score(train_true, train_preds)\n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {np.mean(val_losses):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_accuracy': best_val_accuracy,\n",
    "                'epoch': epoch\n",
    "            }, f'saved_models/best_multimodal_model.pth')\n",
    "            \n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_accuracy:.4f}\")\n",
    "        \n",
    "        # Optional: Print classification report for validation set\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"\\nValidation Classification Report:\")\n",
    "            print(classification_report(val_true, val_preds))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Paths and Configurations\n",
    "    audio_model_path = \"./emotion_recognition_model\"\n",
    "    text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "    # Initialize Processors\n",
    "    audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "    text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "    \n",
    "    # Create Datasets\n",
    "    train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "    test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "    \n",
    "    # Split train into train and validation\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, \n",
    "        [int(len(train_dataset)*0.8), len(train_dataset)-int(len(train_dataset)*0.8)]\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Get number of labels\n",
    "    num_labels = len(train_dataset.dataset.emotion_to_idx)\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    print(model)\n",
    "    print(f\"Model Initialized with {num_labels} emotion classes\")\n",
    "    print(\"Emotion to Index mapping:\", train_dataset.dataset.emotion_to_idx)\n",
    "    \n",
    "    # Train Model\n",
    "    trained_model = train_model(model, train_loader, val_loader, device, epochs=50)\n",
    "    \n",
    "    # Optional: Load and evaluate best saved model\n",
    "    best_model_path = 'saved_models/best_multimodal_model.pth'\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    \n",
    "    # Reinitialize model and load state dict\n",
    "    best_model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Test model\n",
    "    best_model.eval()\n",
    "    test_preds = []\n",
    "    test_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Move data to device\n",
    "            audio_input = batch['audio_input'].to(device)\n",
    "            audio_mask = batch['audio_mask'].to(device)\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = best_model(\n",
    "                audio_input, \n",
    "                audio_mask, \n",
    "                text_input_ids, \n",
    "                text_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Track test metrics\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_true.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Print test classification report\n",
    "    print(\"\\nTest Classification Report:\")\n",
    "    print(classification_report(test_true, test_preds, \n",
    "        target_names=list(train_dataset.dataset.emotion_to_idx.keys())))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b0dd7a-0cd2-4857-b158-9b054a889ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
