{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b46efb73-49f9-47c0-bbb0-8b45cbd1254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"C:\\\\MachineLearning\\\\UsamaKenway\\\\DeepSeek-R1-OpenHermes-2.5\\\\datasets\\\\CoT_dataset\\\\dataset_0010.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e24d0b0-1635-40bd-baa9-4e10949e0042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 22)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e79793f5-c98c-4e9c-80ab-e66dab7228ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['instruct_prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3122ea5-7961-4004-9e09-1eef1a0c2a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75f46ee-72e2-464d-8205-b97d1dc33c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\xformers\\__init__.py\", line 57, in _is_triton_available\n",
      "    import triton  # noqa\n",
      "    ^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\__init__.py\", line 20, in <module>\n",
      "    from .runtime import (\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\runtime\\__init__.py\", line 1, in <module>\n",
      "    from .autotuner import (Autotuner, Config, Heuristics, autotune, heuristics)\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\runtime\\autotuner.py\", line 9, in <module>\n",
      "    from .jit import KernelInterface\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\runtime\\jit.py\", line 12, in <module>\n",
      "    from ..runtime.driver import driver\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\runtime\\driver.py\", line 1, in <module>\n",
      "    from ..backends import backends\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\backends\\__init__.py\", line 50, in <module>\n",
      "    backends = _discover_backends()\n",
      "               ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\backends\\__init__.py\", line 43, in _discover_backends\n",
      "    compiler = _load_module(name, os.path.join(root, name, 'compiler.py'))\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\backends\\__init__.py\", line 12, in _load_module\n",
      "    spec.loader.exec_module(module)\n",
      "  File \"C:\\Python\\py311env\\Lib\\site-packages\\triton\\backends\\amd\\compiler.py\", line 2, in <module>\n",
      "    from triton._C.libtriton import ir, passes, llvm, amd\n",
      "ImportError: DLL load failed while importing libtriton: A dynamic link library (DLL) initialization routine failed.\n",
      "Some weights of the model checkpoint at NovaSearch/stella_en_400M_v5 were not used when initializing NewModel: ['new.pooler.dense.bias', 'new.pooler.dense.weight']\n",
      "- This IS expected if you are initializing NewModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing NewModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model's dimensions: 1024\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "# Load LLaMA tokenizer and MiniLM embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "embedding_model =  SentenceTransformer(\"NovaSearch/stella_en_400M_v5\", trust_remote_code=True, device='cuda').cuda() #= SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "print(f\"Embedding model's dimensions: {len(embedding_model.encode('ok'))}\")\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    return torch.tensor(embedding_model.encode(texts), dtype=torch.float32)\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Multi-Head Self-Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv_proj(x).reshape(B, T, 3, self.num_heads, self.d_k).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = (attn_weights @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "# Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "        \n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, max_len=100):\n",
    "        self.texts = df['instruct_prompt'].tolist()\n",
    "        # Reshape embeddings to match expected dimensions\n",
    "        self.embeddings = [\n",
    "            torch.nn.functional.pad(\n",
    "                get_embeddings(text).unsqueeze(0), \n",
    "                (0, 0, 0, max_len - 1)\n",
    "            ) if len(get_embeddings(text).unsqueeze(0)) < max_len \n",
    "            else get_embeddings(text).unsqueeze(0)[:max_len]\n",
    "            for text in self.texts\n",
    "        ]\n",
    "        self.tokenized = [\n",
    "            tokenizer(\n",
    "                text, \n",
    "                return_tensors='pt', \n",
    "                padding=\"max_length\", \n",
    "                truncation=True, \n",
    "                max_length=max_len\n",
    "            )[\"input_ids\"].squeeze(0) \n",
    "            for text in self.texts\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.tokenized[idx]\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim, num_layers, vocab_size, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_model, num_heads, hidden_dim) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        # Add input projection layer to handle single token embeddings\n",
    "        self.input_projection = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure input has correct shape [batch_size, seq_len, d_model]\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        \n",
    "        # Project input\n",
    "        x = self.input_projection(x)\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            # x = layer(x)\n",
    "            x = checkpoint.checkpoint(layer, x)  # Apply gradient checkpointing per layer\n",
    "        return self.output_layer(x)\n",
    "\n",
    "    def decode(self, logits):\n",
    "        token_ids = torch.argmax(logits, dim=-1)\n",
    "        return [tokenizer.decode(ids, skip_special_tokens=True) for ids in token_ids]\n",
    "\n",
    "# Training setup\n",
    "def train_model(model, train_loader, num_epochs, device, vocab_size):\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Reshape outputs and targets for loss calculation\n",
    "            outputs = outputs.view(-1, vocab_size)\n",
    "            targets = targets.view(-1)\n",
    "            \n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Add gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffecf439-d949-4c4a-8367-bdbfdeb08bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 37 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.20521261, -0.08252388, -2.8384926 , ..., -1.2719604 ,\n",
       "        0.5373522 , -0.71815664], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "embedding_model.encode(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b50b33df-81b2-44bc-a550-e5da67f617b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, start_epoch, num_epochs, device, vocab_size):\n",
    "    \"\"\"\n",
    "    Memory-optimized training function using Adafactor and gradient accumulation\n",
    "    \"\"\"\n",
    "    # Import Adafactor from transformers\n",
    "    from transformers.optimization import Adafactor\n",
    "    \n",
    "    # Initialize Adafactor optimizer with memory-efficient settings\n",
    "    optimizer = Adafactor(\n",
    "        model.parameters(),\n",
    "        eps=(1e-30, 1e-3),\n",
    "        clip_threshold=1.0,\n",
    "        decay_rate=-0.8,\n",
    "        beta1=None,\n",
    "        weight_decay=0.01,\n",
    "        scale_parameter=True,\n",
    "        relative_step=True,  # Use relative step sizing\n",
    "        warmup_init=True     # Enable warmup\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "    \n",
    "    # Gradient accumulation steps\n",
    "    gradient_accumulation_steps = 4\n",
    "    \n",
    "    # Enable gradient checkpointing for memory efficiency\n",
    "    # model.gradient_checkpointing_enable() # im not using it because of error\n",
    "    \n",
    "    # Initialize mixed precision training\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    \n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            # Move to device and handle data types\n",
    "            inputs = inputs.to(device).float()\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Use automatic mixed precision\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(inputs)\n",
    "                outputs = outputs.view(-1, vocab_size)\n",
    "                targets = targets.view(-1)\n",
    "                \n",
    "                # Scale loss by gradient accumulation steps\n",
    "                loss = criterion(outputs, targets) / gradient_accumulation_steps\n",
    "            \n",
    "            # Scale gradients and backward pass\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # Update weights if we've accumulated enough gradients\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                # Unscale gradients for clipping\n",
    "                scaler.unscale_(optimizer)\n",
    "                \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                \n",
    "                # Optimizer step with scaler\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "            \n",
    "            # Track loss (multiply by accumulation steps to get true loss)\n",
    "            total_loss += loss.item() * gradient_accumulation_steps\n",
    "            \n",
    "            # Print progress\n",
    "            if batch_idx % 50 == 0:\n",
    "                avg_loss = total_loss / (batch_idx + 1)\n",
    "                print(f\"Epoch {epoch+1}, Batch {batch_idx}, \"\n",
    "                      f\"Loss: {loss.item() * gradient_accumulation_steps:.4f}, \"\n",
    "                      f\"Avg Loss: {avg_loss:.4f}\")\n",
    "                \n",
    "                # # Print memory usage if on cuda\n",
    "                # if device.type == 'cuda':\n",
    "                #     print(f\"GPU Memory: \"\n",
    "                #           f\"{torch.cuda.memory_allocated(device) / 1024**2:.1f}MB / \"\n",
    "                #           f\"{torch.cuda.memory_reserved(device) / 1024**2:.1f}MB\")\n",
    "        \n",
    "        # Compute average loss for the epoch\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Optional: Save checkpoint after each epoch\n",
    "        torch.save(model, 'model.pth')\n",
    "        # torch.save({\n",
    "        #     'epoch': epoch,\n",
    "        #     'model_state_dict': model.state_dict(),\n",
    "        #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #     'loss': avg_loss,\n",
    "        # }, f'checkpoint.pt') #f'checkpoint_epoch_{epoch+1}.pt'\n",
    "\n",
    "# Memory optimization settings before training\n",
    "def optimize_memory():\n",
    "    \"\"\"Apply memory optimizations before training\"\"\"\n",
    "    import gc\n",
    "    import torch\n",
    "    \n",
    "    # Clear memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Set memory allocator settings\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.set_per_process_memory_fraction(0.9)  # Use 90% of available memory\n",
    "        torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac689aaf-2a38-41d3-acf9-b0d24b9a20d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_model.encode(\"g\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1dbdf3b6-a23e-4807-ae83-180a2ee0347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, max_len=100):\n",
    "        self.texts = df['instruct_prompt'].tolist()\n",
    "        \n",
    "        # Track progress with tqdm\n",
    "        self.embeddings = []\n",
    "        self.tokenized = []\n",
    "        \n",
    "        for text in tqdm(self.texts, desc=\"Processing Texts\", leave=False):\n",
    "            try:\n",
    "                # Handle embeddings\n",
    "                embedding = get_embeddings(text)\n",
    "                if isinstance(embedding, torch.Tensor):  # Ensure it's a tensor\n",
    "                    embedding = torch.nn.functional.pad(\n",
    "                        embedding.unsqueeze(0), \n",
    "                        (0, 0, 0, max_len - 1)\n",
    "                    ) if len(embedding.unsqueeze(0)) < max_len else embedding.unsqueeze(0)[:max_len]\n",
    "                    self.embeddings.append(embedding)\n",
    "                else:\n",
    "                    raise ValueError(f\"Embedding not tensor for text: {text}\")\n",
    "                \n",
    "                # Handle tokenization\n",
    "                tokenized = tokenizer(\n",
    "                    text, \n",
    "                    return_tensors='pt', \n",
    "                    padding=\"max_length\", \n",
    "                    truncation=True, \n",
    "                    max_length=max_len\n",
    "                )[\"input_ids\"].squeeze(0)\n",
    "                self.tokenized.append(tokenized)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping row due to error: {e}\")\n",
    "                continue\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.tokenized[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e5fd1eff-a5e5-4a65-be61-a8e7246c940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path, device):\n",
    "    \"\"\"Load model and optimizer states from a checkpoint\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
    "    print(f\"Checkpoint loaded! Resuming from epoch {start_epoch}\")\n",
    "    return model, optimizer, start_epoch\n",
    "\n",
    "def train_model(model, train_loader, num_epochs, device, vocab_size, checkpoint_path=None):\n",
    "    from transformers.optimization import Adafactor\n",
    "    import torch.nn as nn\n",
    "\n",
    "    optimizer = Adafactor(\n",
    "        model.parameters(),\n",
    "        eps=(1e-30, 1e-3),\n",
    "        clip_threshold=1.0,\n",
    "        decay_rate=-0.8,\n",
    "        beta1=None,\n",
    "        weight_decay=0.01,\n",
    "        scale_parameter=True,\n",
    "        relative_step=True,\n",
    "        warmup_init=True\n",
    "    )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    gradient_accumulation_steps = 4\n",
    "    # scaler = torch.cuda.amp.GradScaler()\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "    # Resume training if checkpoint exists\n",
    "    start_epoch = 0\n",
    "    if checkpoint_path:\n",
    "        model, optimizer, start_epoch = load_checkpoint(model, optimizer, checkpoint_path, device)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "            inputs, targets = inputs.to(device).float(), targets.to(device)\n",
    "\n",
    "            # with torch.cuda.amp.autocast():\n",
    "            #     outputs = model(inputs).view(-1, vocab_size)\n",
    "            #     targets = targets.view(-1)\n",
    "            #     loss = criterion(outputs, targets) / gradient_accumulation_steps\n",
    "            with torch.amp.autocast('cuda'):  # âœ… Fixed autocast\n",
    "                outputs = model(inputs).view(-1, vocab_size)\n",
    "                targets = targets.view(-1)\n",
    "                loss = criterion(outputs, targets) / 4  # Gradient accumulation steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (batch_idx + 1) % gradient_accumulation_steps == 0:\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            total_loss += loss.item() * gradient_accumulation_steps\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        torch.save(model, 'model.pth')\n",
    "        # # Save checkpoint\n",
    "        # torch.save({\n",
    "        #     'epoch': epoch,\n",
    "        #     'model_state_dict': model.state_dict(),\n",
    "        #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "        #     'loss': avg_loss,\n",
    "        # }, 'checkpoint.pt')\n",
    "\n",
    "    print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37421559-a284-4e13-9d2d-d3d61a8abaf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and training\n",
    "# d_model = 384  # MiniLM embedding size\n",
    "# num_heads = 8\n",
    "# hidden_dim = 1536\n",
    "# num_layers = 6\n",
    "# vocab_size = tokenizer.vocab_size\n",
    "# max_len = 500\n",
    "\n",
    "# d_model=1024 #len(embedding_model.encode(\"g\"))\n",
    "# num_heads=32\n",
    "# hidden_dim=4096 #16384\n",
    "# num_layers=24\n",
    "vocab_size=tokenizer.vocab_size\n",
    "max_len = 1000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "d_model = 1024\n",
    "num_heads = 32\n",
    "num_layers = 24\n",
    "hidden_dim = 12288 # hidden_dim = 16384\n",
    "\n",
    "\n",
    "model = Transformer(d_model, num_heads, hidden_dim, num_layers, vocab_size, max_len).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "981f4160-c716-472c-b1d0-e282b090821a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "69378112-7954-4ea0-b604-fd34cf237a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = torch.load('model.pth', weights_only=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa81b48d-f40e-46a1-a1cf-75c5779157dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.optimization import Adafactor\n",
    "\n",
    "# optimizer = Adafactor(\n",
    "#         model.parameters(),\n",
    "#         eps=(1e-30, 1e-3),\n",
    "#         clip_threshold=1.0,\n",
    "#         decay_rate=-0.8,\n",
    "#         beta1=None,\n",
    "#         weight_decay=0.01,\n",
    "#         scale_parameter=True,\n",
    "#         relative_step=True,  # Use relative step sizing\n",
    "#         warmup_init=True     # Enable warmup\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8eb6023-8d84-4160-8db1-adbe735c0b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from epoch 2 with loss 6.3275\n"
     ]
    }
   ],
   "source": [
    "# checkpoint = torch.load('checkpoint.pt')\n",
    "\n",
    "# # Restore model and optimizer states\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# # Restore epoch and loss\n",
    "# start_epoch = checkpoint['epoch'] + 1  # Continue from the next epoch\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# print(f\"Resuming training from epoch {start_epoch} with loss {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5a0ebd1-30b5-40bf-9d13-6790e80de4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d00da75-72ad-4423-97b2-a7c590c11864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       "
     ]
    }
   ],
   "source": [
    "train_dataset = TextDataset(df, max_len=max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60a28dfd-6027-4a67-9740-fb1524ec5401",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Save train_dataset to a file\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mH:/GAMES/train_dataset.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 5\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset saved successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "# import pickle\n",
    "\n",
    "# # Save train_dataset to a file\n",
    "# with open(\"H:/GAMES/train_dataset.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(train_dataset, f)\n",
    "\n",
    "# print(\"Dataset saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90b8d8bb-8ac0-4918-9922-482b9da3fd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shallow size of train_dataset: 0.00 MB\n",
      "Deep size of train_dataset: 59.96 MB\n",
      "Shallow size of train_loader: 0.00 MB\n",
      "Deep size of train_loader: 59.97 MB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch\n",
    "from pympler import asizeof\n",
    "\n",
    "# Check memory size of train_dataset\n",
    "train_dataset_size = sys.getsizeof(train_dataset)  # Shallow size\n",
    "train_dataset_deep_size = asizeof.asizeof(train_dataset)  # Deep size\n",
    "\n",
    "# Check memory size of train_loader\n",
    "train_loader_size = sys.getsizeof(train_loader)  # Shallow size\n",
    "train_loader_deep_size = asizeof.asizeof(train_loader)  # Deep size\n",
    "\n",
    "print(f\"Shallow size of train_dataset: {train_dataset_size / (1024**2):.2f} MB\")\n",
    "print(f\"Deep size of train_dataset: {train_dataset_deep_size / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"Shallow size of train_loader: {train_loader_size / (1024**2):.2f} MB\")\n",
    "print(f\"Deep size of train_loader: {train_loader_deep_size / (1024**2):.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e58ac09-e359-4c37-bb73-d7a4106114a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b881914-e4d1-419f-9a3d-a62e861b5bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_9048\\1510607220.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_9048\\1510607220.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 10.8133, Avg Loss: 10.8133\n",
      "GPU Memory: 7976.2MB / 14536.0MB\n",
      "Epoch 1, Batch 50, Loss: 10.7034, Avg Loss: 10.7819\n",
      "GPU Memory: 7982.7MB / 15764.0MB\n",
      "Epoch 1, Batch 100, Loss: 10.3118, Avg Loss: 10.6597\n",
      "GPU Memory: 7983.7MB / 15764.0MB\n",
      "Epoch 1, Batch 150, Loss: 9.8308, Avg Loss: 10.4764\n",
      "GPU Memory: 7982.7MB / 15764.0MB\n",
      "Epoch 1, Batch 200, Loss: 9.4193, Avg Loss: 10.2557\n",
      "GPU Memory: 7983.7MB / 15764.0MB\n",
      "Epoch 1, Batch 250, Loss: 9.0225, Avg Loss: 10.0444\n",
      "GPU Memory: 7982.7MB / 15764.0MB\n",
      "Epoch 1, Batch 300, Loss: 8.6639, Avg Loss: 9.8520\n",
      "GPU Memory: 7983.7MB / 15764.0MB\n",
      "Epoch 1, Batch 350, Loss: 8.5891, Avg Loss: 9.6850\n",
      "GPU Memory: 7982.7MB / 15764.0MB\n",
      "Epoch 1, Batch 400, Loss: 8.4636, Avg Loss: 9.5427\n",
      "GPU Memory: 7983.7MB / 15764.0MB\n",
      "Epoch 1, Batch 450, Loss: 8.3328, Avg Loss: 9.4146\n",
      "GPU Memory: 7982.7MB / 15764.0MB\n",
      "Epoch 1, Average Loss: 9.2995\n",
      "Epoch 2, Batch 0, Loss: 8.1870, Avg Loss: 8.1870\n",
      "GPU Memory: 7983.7MB / 15764.0MB\n",
      "Epoch 2, Batch 50, Loss: 8.0836, Avg Loss: 8.0810\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 2, Batch 100, Loss: 7.8422, Avg Loss: 8.0079\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 2, Batch 150, Loss: 7.7717, Avg Loss: 7.9355\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 2, Batch 200, Loss: 7.6720, Avg Loss: 7.8677\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 2, Batch 250, Loss: 7.3837, Avg Loss: 7.8023\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 2, Batch 300, Loss: 7.3974, Avg Loss: 7.7343\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 2, Batch 350, Loss: 7.1971, Avg Loss: 7.6688\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 2, Batch 400, Loss: 6.9778, Avg Loss: 7.6041\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 2, Batch 450, Loss: 7.0342, Avg Loss: 7.5460\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 2, Average Loss: 7.4860\n",
      "Epoch 3, Batch 0, Loss: 6.9850, Avg Loss: 6.9850\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 3, Batch 50, Loss: 6.7653, Avg Loss: 6.8348\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 3, Batch 100, Loss: 6.7814, Avg Loss: 6.8022\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 3, Batch 150, Loss: 6.6822, Avg Loss: 6.7691\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 3, Batch 200, Loss: 6.5064, Avg Loss: 6.7311\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 3, Batch 250, Loss: 6.5967, Avg Loss: 6.6949\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 3, Batch 300, Loss: 6.5693, Avg Loss: 6.6627\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 3, Batch 350, Loss: 6.5303, Avg Loss: 6.6360\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 3, Batch 400, Loss: 6.4012, Avg Loss: 6.6095\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 3, Batch 450, Loss: 6.3691, Avg Loss: 6.5876\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 3, Average Loss: 6.5680\n",
      "Epoch 4, Batch 0, Loss: 6.3142, Avg Loss: 6.3142\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 4, Batch 50, Loss: 6.4466, Avg Loss: 6.3450\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 4, Batch 100, Loss: 6.3271, Avg Loss: 6.3404\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 4, Batch 150, Loss: 6.2526, Avg Loss: 6.3339\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 4, Batch 200, Loss: 6.2521, Avg Loss: 6.3234\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 4, Batch 250, Loss: 6.2589, Avg Loss: 6.3152\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 4, Batch 300, Loss: 6.4225, Avg Loss: 6.3105\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 4, Batch 350, Loss: 6.4250, Avg Loss: 6.3094\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 4, Batch 400, Loss: 6.3845, Avg Loss: 6.3070\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 4, Batch 450, Loss: 6.3039, Avg Loss: 6.3031\n",
      "GPU Memory: 7982.9MB / 15764.0MB\n",
      "Epoch 4, Average Loss: 6.3003\n",
      "Epoch 5, Batch 0, Loss: 6.4272, Avg Loss: 6.4272\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 5, Batch 50, Loss: 6.1792, Avg Loss: 6.2665\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 5, Batch 100, Loss: 6.2280, Avg Loss: 6.2656\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 5, Batch 150, Loss: 6.1587, Avg Loss: 6.2643\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 5, Batch 200, Loss: 6.2036, Avg Loss: 6.2583\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 5, Batch 250, Loss: 6.1700, Avg Loss: 6.2593\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 5, Batch 300, Loss: 6.3998, Avg Loss: 6.2575\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 5, Batch 350, Loss: 6.2618, Avg Loss: 6.2565\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 5, Batch 400, Loss: 6.1358, Avg Loss: 6.2554\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 5, Batch 450, Loss: 6.2730, Avg Loss: 6.2557\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 5, Average Loss: 6.2557\n",
      "Epoch 6, Batch 0, Loss: 6.1414, Avg Loss: 6.1414\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 6, Batch 50, Loss: 6.1956, Avg Loss: 6.2481\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 6, Batch 100, Loss: 6.3602, Avg Loss: 6.2465\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 6, Batch 150, Loss: 6.2794, Avg Loss: 6.2397\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 6, Batch 200, Loss: 6.1913, Avg Loss: 6.2361\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 6, Batch 250, Loss: 6.4317, Avg Loss: 6.2341\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 6, Batch 300, Loss: 6.1568, Avg Loss: 6.2383\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 6, Batch 350, Loss: 6.3719, Avg Loss: 6.2375\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 6, Batch 400, Loss: 5.9312, Avg Loss: 6.2380\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 6, Batch 450, Loss: 6.2959, Avg Loss: 6.2377\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 6, Average Loss: 6.2404\n",
      "Epoch 7, Batch 0, Loss: 6.2685, Avg Loss: 6.2685\n",
      "GPU Memory: 7983.8MB / 15764.0MB\n",
      "Epoch 7, Batch 50, Loss: 6.3484, Avg Loss: 6.2341\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 7, Batch 100, Loss: 6.1438, Avg Loss: 6.2352\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 7, Batch 150, Loss: 6.3332, Avg Loss: 6.2289\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 7, Batch 200, Loss: 6.2065, Avg Loss: 6.2283\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 7, Batch 250, Loss: 6.1184, Avg Loss: 6.2292\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 7, Batch 300, Loss: 6.2679, Avg Loss: 6.2253\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 7, Batch 350, Loss: 6.1670, Avg Loss: 6.2278\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 7, Batch 400, Loss: 6.3086, Avg Loss: 6.2271\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 7, Batch 450, Loss: 6.2232, Avg Loss: 6.2261\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 7, Average Loss: 6.2255\n",
      "Epoch 8, Batch 0, Loss: 6.2179, Avg Loss: 6.2179\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 8, Batch 50, Loss: 6.1766, Avg Loss: 6.2194\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 8, Batch 100, Loss: 6.1112, Avg Loss: 6.2183\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 8, Batch 150, Loss: 6.2246, Avg Loss: 6.2187\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 8, Batch 200, Loss: 6.1139, Avg Loss: 6.2193\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 8, Batch 250, Loss: 6.1409, Avg Loss: 6.2188\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 8, Batch 300, Loss: 6.1286, Avg Loss: 6.2160\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 8, Batch 350, Loss: 6.2867, Avg Loss: 6.2120\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 8, Batch 400, Loss: 6.2024, Avg Loss: 6.2110\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 8, Batch 450, Loss: 6.3076, Avg Loss: 6.2113\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 8, Average Loss: 6.2107\n",
      "Epoch 9, Batch 0, Loss: 6.1715, Avg Loss: 6.1715\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 9, Batch 50, Loss: 6.0247, Avg Loss: 6.1886\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 9, Batch 100, Loss: 6.2392, Avg Loss: 6.1941\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 9, Batch 150, Loss: 6.0555, Avg Loss: 6.1978\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 9, Batch 200, Loss: 6.0877, Avg Loss: 6.1978\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 9, Batch 250, Loss: 6.2149, Avg Loss: 6.1980\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 9, Batch 300, Loss: 6.2446, Avg Loss: 6.1993\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 9, Batch 350, Loss: 6.1458, Avg Loss: 6.1988\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 9, Batch 400, Loss: 6.2158, Avg Loss: 6.1996\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 9, Batch 450, Loss: 6.2026, Avg Loss: 6.1971\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 9, Average Loss: 6.1977\n",
      "Epoch 10, Batch 0, Loss: 6.0364, Avg Loss: 6.0364\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 10, Batch 50, Loss: 6.1221, Avg Loss: 6.1785\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 10, Batch 100, Loss: 6.0934, Avg Loss: 6.1575\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 10, Batch 150, Loss: 6.1332, Avg Loss: 6.1343\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 10, Batch 200, Loss: 6.1302, Avg Loss: 6.1212\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 10, Batch 250, Loss: 6.0246, Avg Loss: 6.1087\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 10, Batch 300, Loss: 5.9390, Avg Loss: 6.0956\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 10, Batch 350, Loss: 5.9909, Avg Loss: 6.0867\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 10, Batch 400, Loss: 5.9948, Avg Loss: 6.0780\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 10, Batch 450, Loss: 5.9300, Avg Loss: 6.0705\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 10, Average Loss: 6.0617\n",
      "Epoch 11, Batch 0, Loss: 6.2726, Avg Loss: 6.2726\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 11, Batch 50, Loss: 5.9006, Avg Loss: 5.9674\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 11, Batch 100, Loss: 5.9417, Avg Loss: 5.9653\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 11, Batch 150, Loss: 6.0109, Avg Loss: 5.9597\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 11, Batch 200, Loss: 5.9449, Avg Loss: 5.9567\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 11, Batch 250, Loss: 5.8051, Avg Loss: 5.9496\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 11, Batch 300, Loss: 5.8285, Avg Loss: 5.9460\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 11, Batch 350, Loss: 5.9601, Avg Loss: 5.9387\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 11, Batch 400, Loss: 5.9423, Avg Loss: 5.9330\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 11, Batch 450, Loss: 6.0341, Avg Loss: 5.9247\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 11, Average Loss: 5.9167\n",
      "Epoch 12, Batch 0, Loss: 5.7798, Avg Loss: 5.7798\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 12, Batch 50, Loss: 5.7524, Avg Loss: 5.8556\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n",
      "Epoch 12, Batch 100, Loss: 5.6733, Avg Loss: 5.8414\n",
      "GPU Memory: 7984.4MB / 15764.0MB\n",
      "Epoch 12, Batch 150, Loss: 5.8674, Avg Loss: 5.8340\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n",
      "Epoch 12, Batch 200, Loss: 5.6653, Avg Loss: 5.8246\n",
      "GPU Memory: 7984.4MB / 15764.0MB\n",
      "Epoch 12, Batch 250, Loss: 5.8781, Avg Loss: 5.8159\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n",
      "Epoch 12, Batch 300, Loss: 5.8356, Avg Loss: 5.8109\n",
      "GPU Memory: 7984.4MB / 15764.0MB\n",
      "Epoch 12, Batch 350, Loss: 5.7877, Avg Loss: 5.8061\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n",
      "Epoch 12, Batch 400, Loss: 5.8241, Avg Loss: 5.8004\n",
      "GPU Memory: 7984.4MB / 15764.0MB\n",
      "Epoch 12, Batch 450, Loss: 5.6495, Avg Loss: 5.7975\n",
      "GPU Memory: 7983.0MB / 15764.0MB\n",
      "Epoch 12, Average Loss: 5.8001\n",
      "Epoch 13, Batch 0, Loss: 5.8076, Avg Loss: 5.8076\n",
      "GPU Memory: 7984.0MB / 15764.0MB\n",
      "Epoch 13, Batch 50, Loss: 5.8578, Avg Loss: 5.7404\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 13, Batch 100, Loss: 5.7000, Avg Loss: 5.7315\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 13, Batch 150, Loss: 5.7346, Avg Loss: 5.7269\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 13, Batch 200, Loss: 5.4898, Avg Loss: 5.7226\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 13, Batch 250, Loss: 5.6800, Avg Loss: 5.7151\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 13, Batch 300, Loss: 5.6506, Avg Loss: 5.7131\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 13, Batch 350, Loss: 5.7820, Avg Loss: 5.7141\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 13, Batch 400, Loss: 5.5647, Avg Loss: 5.7128\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 13, Batch 450, Loss: 5.7461, Avg Loss: 5.7104\n",
      "GPU Memory: 7983.2MB / 15764.0MB\n",
      "Epoch 13, Average Loss: 5.7075\n",
      "Epoch 14, Batch 0, Loss: 5.6918, Avg Loss: 5.6918\n",
      "GPU Memory: 7984.1MB / 15764.0MB\n",
      "Epoch 14, Batch 50, Loss: 5.7018, Avg Loss: 5.6319\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 14, Batch 100, Loss: 5.7101, Avg Loss: 5.6301\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 14, Batch 150, Loss: 5.6828, Avg Loss: 5.6311\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 14, Batch 200, Loss: 5.5509, Avg Loss: 5.6362\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 14, Batch 250, Loss: 5.6267, Avg Loss: 5.6387\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 14, Batch 300, Loss: 5.5145, Avg Loss: 5.6361\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 14, Batch 350, Loss: 5.7010, Avg Loss: 5.6335\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 14, Batch 400, Loss: 5.4023, Avg Loss: 5.6346\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 14, Batch 450, Loss: 5.5767, Avg Loss: 5.6371\n",
      "GPU Memory: 7983.3MB / 15764.0MB\n",
      "Epoch 14, Average Loss: 5.6365\n",
      "Epoch 15, Batch 0, Loss: 5.5824, Avg Loss: 5.5824\n",
      "GPU Memory: 7984.2MB / 15764.0MB\n",
      "Epoch 15, Batch 50, Loss: 5.4826, Avg Loss: 5.5746\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n",
      "Epoch 15, Batch 100, Loss: 5.6394, Avg Loss: 5.5883\n",
      "GPU Memory: 7984.4MB / 15764.0MB\n",
      "Epoch 15, Batch 150, Loss: 5.7604, Avg Loss: 5.5943\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n",
      "Epoch 15, Batch 200, Loss: 5.6762, Avg Loss: 5.5920\n",
      "GPU Memory: 7984.4MB / 15764.0MB\n",
      "Epoch 15, Batch 250, Loss: 5.6383, Avg Loss: 5.5947\n",
      "GPU Memory: 7983.4MB / 15764.0MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create dataset and dataloader\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# train_dataset = TextDataset(df, max_len=max_len)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 57\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, num_epochs, device, vocab_size)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Update weights if we've accumulated enough gradients\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m gradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Unscale gradients for clipping\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;66;03m# Clip gradients\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:338\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[1;34m(self, optimizer)\u001b[0m\n\u001b[0;32m    335\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    336\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 338\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:279\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[1;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m device, per_dtype_grads \u001b[38;5;129;01min\u001b[39;00m per_device_and_dtype_grads\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m grads \u001b[38;5;129;01min\u001b[39;00m per_dtype_grads\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 279\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_amp_foreach_non_finite_check_and_unscale_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m                \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m                \u001b[49m\u001b[43mper_device_found_inf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m                \u001b[49m\u001b[43mper_device_inv_scale\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m per_device_found_inf\u001b[38;5;241m.\u001b[39m_per_device_tensors\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader\n",
    "# train_dataset = TextDataset(df, max_len=max_len)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "# Train the model\n",
    "train_model(model, train_loader, num_epochs=40, device=device, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc493d88-4fb4-48a1-a484-129a5c858677",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_9048\\922082617.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_9048\\922082617.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 5.6266, Avg Loss: 5.6266\n",
      "GPU Memory: 8631.4MB / 14306.0MB\n",
      "Epoch 1, Batch 50, Loss: 5.4901, Avg Loss: 5.5854\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 100, Loss: 5.5246, Avg Loss: 5.5633\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 150, Loss: 5.4203, Avg Loss: 5.5403\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 200, Loss: 5.3766, Avg Loss: 5.5251\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 250, Loss: 5.3213, Avg Loss: 5.5017\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 300, Loss: 5.3774, Avg Loss: 5.4935\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 350, Loss: 5.5819, Avg Loss: 5.4801\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 400, Loss: 5.5981, Avg Loss: 5.4730\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Batch 450, Loss: 5.2984, Avg Loss: 5.4638\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 1, Average Loss: 5.4629\n",
      "Epoch 2, Batch 0, Loss: 5.4481, Avg Loss: 5.4481\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 50, Loss: 5.5456, Avg Loss: 5.4185\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 100, Loss: 5.3703, Avg Loss: 5.4079\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 150, Loss: 5.5159, Avg Loss: 5.4088\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 200, Loss: 5.5335, Avg Loss: 5.4099\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 250, Loss: 5.4523, Avg Loss: 5.4076\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 300, Loss: 5.4603, Avg Loss: 5.4073\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 350, Loss: 5.4059, Avg Loss: 5.4081\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 400, Loss: 5.1680, Avg Loss: 5.4086\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Batch 450, Loss: 5.4744, Avg Loss: 5.4077\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 2, Average Loss: 5.4050\n",
      "Epoch 3, Batch 0, Loss: 5.3973, Avg Loss: 5.3973\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 50, Loss: 5.5505, Avg Loss: 5.4007\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 100, Loss: 5.2773, Avg Loss: 5.3974\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 150, Loss: 5.5380, Avg Loss: 5.4013\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 200, Loss: 5.5756, Avg Loss: 5.3994\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 250, Loss: 5.4235, Avg Loss: 5.3954\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 300, Loss: 5.4519, Avg Loss: 5.3991\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 350, Loss: 5.4635, Avg Loss: 5.3975\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 400, Loss: 5.3888, Avg Loss: 5.3930\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Batch 450, Loss: 5.3555, Avg Loss: 5.3933\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 3, Average Loss: 5.3926\n",
      "Epoch 4, Batch 0, Loss: 5.4521, Avg Loss: 5.4521\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 50, Loss: 5.2658, Avg Loss: 5.3576\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 100, Loss: 5.3147, Avg Loss: 5.3586\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 150, Loss: 5.2634, Avg Loss: 5.3644\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 200, Loss: 5.2924, Avg Loss: 5.3694\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 250, Loss: 5.4334, Avg Loss: 5.3728\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 300, Loss: 5.3436, Avg Loss: 5.3769\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 350, Loss: 5.5299, Avg Loss: 5.3773\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 400, Loss: 5.2929, Avg Loss: 5.3794\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Batch 450, Loss: 5.3592, Avg Loss: 5.3800\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 4, Average Loss: 5.3803\n",
      "Epoch 5, Batch 0, Loss: 5.4729, Avg Loss: 5.4729\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 50, Loss: 5.5406, Avg Loss: 5.3881\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 100, Loss: 5.3339, Avg Loss: 5.3668\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 150, Loss: 5.3596, Avg Loss: 5.3627\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 200, Loss: 5.4396, Avg Loss: 5.3700\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 250, Loss: 5.3595, Avg Loss: 5.3713\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 300, Loss: 5.2610, Avg Loss: 5.3675\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 350, Loss: 5.3728, Avg Loss: 5.3688\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 400, Loss: 5.1871, Avg Loss: 5.3665\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Batch 450, Loss: 5.3942, Avg Loss: 5.3660\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 5, Average Loss: 5.3678\n",
      "Epoch 6, Batch 0, Loss: 5.2546, Avg Loss: 5.2546\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 50, Loss: 5.2336, Avg Loss: 5.3736\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 100, Loss: 5.2440, Avg Loss: 5.3587\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 150, Loss: 5.3226, Avg Loss: 5.3466\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 200, Loss: 5.2809, Avg Loss: 5.3506\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 250, Loss: 5.4122, Avg Loss: 5.3559\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 300, Loss: 5.5601, Avg Loss: 5.3542\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 350, Loss: 5.3310, Avg Loss: 5.3520\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 400, Loss: 5.6167, Avg Loss: 5.3562\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Batch 450, Loss: 5.5078, Avg Loss: 5.3575\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 6, Average Loss: 5.3563\n",
      "Epoch 7, Batch 0, Loss: 5.3190, Avg Loss: 5.3190\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 50, Loss: 5.3303, Avg Loss: 5.3266\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 100, Loss: 5.3954, Avg Loss: 5.3289\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 150, Loss: 5.4229, Avg Loss: 5.3409\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 200, Loss: 5.3518, Avg Loss: 5.3388\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 250, Loss: 5.2522, Avg Loss: 5.3491\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 300, Loss: 5.3326, Avg Loss: 5.3491\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 350, Loss: 5.3504, Avg Loss: 5.3521\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 400, Loss: 5.3784, Avg Loss: 5.3524\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Batch 450, Loss: 5.3417, Avg Loss: 5.3506\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 7, Average Loss: 5.3481\n",
      "Epoch 8, Batch 0, Loss: 5.4705, Avg Loss: 5.4705\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 50, Loss: 5.3743, Avg Loss: 5.3348\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 100, Loss: 5.2332, Avg Loss: 5.3423\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 150, Loss: 5.2531, Avg Loss: 5.3394\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 200, Loss: 5.4626, Avg Loss: 5.3391\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 250, Loss: 5.3565, Avg Loss: 5.3337\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 300, Loss: 5.1854, Avg Loss: 5.3338\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 350, Loss: 5.2967, Avg Loss: 5.3391\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 400, Loss: 5.3119, Avg Loss: 5.3378\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Batch 450, Loss: 5.3591, Avg Loss: 5.3375\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 8, Average Loss: 5.3378\n",
      "Epoch 9, Batch 0, Loss: 5.4094, Avg Loss: 5.4094\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 50, Loss: 5.2537, Avg Loss: 5.3413\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 100, Loss: 5.4030, Avg Loss: 5.3549\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 150, Loss: 5.2009, Avg Loss: 5.3473\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 200, Loss: 5.2985, Avg Loss: 5.3430\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 250, Loss: 5.4570, Avg Loss: 5.3347\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 300, Loss: 5.4300, Avg Loss: 5.3304\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 350, Loss: 5.2347, Avg Loss: 5.3288\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 400, Loss: 5.4928, Avg Loss: 5.3304\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Batch 450, Loss: 5.3353, Avg Loss: 5.3306\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 9, Average Loss: 5.3316\n",
      "Epoch 10, Batch 0, Loss: 5.2470, Avg Loss: 5.2470\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 50, Loss: 5.5211, Avg Loss: 5.3190\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 100, Loss: 5.1735, Avg Loss: 5.3211\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 150, Loss: 5.2656, Avg Loss: 5.3141\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 200, Loss: 5.3045, Avg Loss: 5.3147\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 250, Loss: 5.3437, Avg Loss: 5.3192\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 300, Loss: 5.3922, Avg Loss: 5.3259\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 350, Loss: 5.3349, Avg Loss: 5.3243\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 400, Loss: 5.3638, Avg Loss: 5.3246\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Batch 450, Loss: 5.2150, Avg Loss: 5.3242\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 10, Average Loss: 5.3231\n",
      "Epoch 11, Batch 0, Loss: 5.2399, Avg Loss: 5.2399\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 50, Loss: 5.3217, Avg Loss: 5.3031\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 100, Loss: 5.2073, Avg Loss: 5.2972\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 150, Loss: 5.4976, Avg Loss: 5.2980\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 200, Loss: 5.1195, Avg Loss: 5.3062\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 250, Loss: 5.4423, Avg Loss: 5.3062\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 300, Loss: 5.4149, Avg Loss: 5.3047\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 350, Loss: 5.4885, Avg Loss: 5.3085\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 400, Loss: 5.3416, Avg Loss: 5.3122\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Batch 450, Loss: 5.6231, Avg Loss: 5.3134\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 11, Average Loss: 5.3153\n",
      "Epoch 12, Batch 0, Loss: 5.4202, Avg Loss: 5.4202\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 50, Loss: 5.2064, Avg Loss: 5.3115\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 100, Loss: 5.4247, Avg Loss: 5.3080\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 150, Loss: 5.1822, Avg Loss: 5.3087\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 200, Loss: 5.0620, Avg Loss: 5.3105\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 250, Loss: 5.3279, Avg Loss: 5.3098\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 300, Loss: 5.2995, Avg Loss: 5.3068\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 350, Loss: 5.3284, Avg Loss: 5.3096\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 400, Loss: 5.1310, Avg Loss: 5.3114\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Batch 450, Loss: 5.2711, Avg Loss: 5.3103\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 12, Average Loss: 5.3106\n",
      "Epoch 13, Batch 0, Loss: 5.1276, Avg Loss: 5.1276\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 50, Loss: 5.3505, Avg Loss: 5.2738\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 100, Loss: 5.1697, Avg Loss: 5.2783\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 150, Loss: 5.4046, Avg Loss: 5.2851\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 200, Loss: 5.3624, Avg Loss: 5.2948\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 250, Loss: 5.3361, Avg Loss: 5.2956\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 300, Loss: 5.2498, Avg Loss: 5.2936\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 350, Loss: 5.2877, Avg Loss: 5.2976\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 400, Loss: 5.2379, Avg Loss: 5.2969\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Batch 450, Loss: 5.5171, Avg Loss: 5.2997\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 13, Average Loss: 5.3013\n",
      "Epoch 14, Batch 0, Loss: 5.3012, Avg Loss: 5.3012\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 50, Loss: 5.2636, Avg Loss: 5.2866\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 100, Loss: 5.2462, Avg Loss: 5.2801\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 150, Loss: 5.2985, Avg Loss: 5.2813\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 200, Loss: 5.2467, Avg Loss: 5.2873\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 250, Loss: 5.3876, Avg Loss: 5.2831\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 300, Loss: 5.2138, Avg Loss: 5.2875\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 350, Loss: 5.4675, Avg Loss: 5.2861\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 400, Loss: 5.2640, Avg Loss: 5.2869\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Batch 450, Loss: 5.3585, Avg Loss: 5.2896\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 14, Average Loss: 5.2927\n",
      "Epoch 15, Batch 0, Loss: 5.2432, Avg Loss: 5.2432\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 50, Loss: 5.3250, Avg Loss: 5.2932\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 100, Loss: 5.0389, Avg Loss: 5.2774\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 150, Loss: 5.1852, Avg Loss: 5.2794\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 200, Loss: 5.3409, Avg Loss: 5.2836\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 250, Loss: 5.3428, Avg Loss: 5.2813\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 300, Loss: 5.2764, Avg Loss: 5.2826\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 350, Loss: 5.3487, Avg Loss: 5.2827\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 400, Loss: 5.3302, Avg Loss: 5.2846\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Batch 450, Loss: 5.2223, Avg Loss: 5.2827\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 15, Average Loss: 5.2831\n",
      "Epoch 16, Batch 0, Loss: 5.3123, Avg Loss: 5.3123\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 50, Loss: 5.3553, Avg Loss: 5.2616\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 100, Loss: 5.3172, Avg Loss: 5.2745\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 150, Loss: 5.2958, Avg Loss: 5.2672\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 200, Loss: 5.1123, Avg Loss: 5.2667\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 250, Loss: 5.3408, Avg Loss: 5.2708\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 300, Loss: 5.2377, Avg Loss: 5.2726\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 350, Loss: 5.1596, Avg Loss: 5.2735\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 400, Loss: 5.3046, Avg Loss: 5.2719\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Batch 450, Loss: 5.2932, Avg Loss: 5.2710\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 16, Average Loss: 5.2716\n",
      "Epoch 17, Batch 0, Loss: 5.1533, Avg Loss: 5.1533\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 50, Loss: 5.1028, Avg Loss: 5.2337\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 100, Loss: 5.2067, Avg Loss: 5.2456\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 150, Loss: 5.1904, Avg Loss: 5.2493\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 200, Loss: 5.3926, Avg Loss: 5.2494\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 250, Loss: 5.1221, Avg Loss: 5.2469\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 300, Loss: 5.1401, Avg Loss: 5.2511\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 350, Loss: 5.2157, Avg Loss: 5.2524\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 400, Loss: 5.2486, Avg Loss: 5.2578\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Batch 450, Loss: 5.3186, Avg Loss: 5.2562\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 17, Average Loss: 5.2595\n",
      "Epoch 18, Batch 0, Loss: 5.2246, Avg Loss: 5.2246\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 50, Loss: 5.1634, Avg Loss: 5.2381\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 100, Loss: 5.1149, Avg Loss: 5.2343\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 150, Loss: 5.3339, Avg Loss: 5.2297\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 200, Loss: 5.1383, Avg Loss: 5.2281\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 250, Loss: 5.2773, Avg Loss: 5.2274\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 300, Loss: 5.2613, Avg Loss: 5.2331\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 350, Loss: 5.2219, Avg Loss: 5.2382\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 400, Loss: 5.2588, Avg Loss: 5.2410\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Batch 450, Loss: 5.3482, Avg Loss: 5.2423\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 18, Average Loss: 5.2462\n",
      "Epoch 19, Batch 0, Loss: 5.1132, Avg Loss: 5.1132\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 50, Loss: 5.1281, Avg Loss: 5.2381\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 100, Loss: 5.2954, Avg Loss: 5.2289\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 150, Loss: 5.2543, Avg Loss: 5.2223\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 200, Loss: 5.4723, Avg Loss: 5.2249\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 250, Loss: 5.3463, Avg Loss: 5.2263\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 300, Loss: 5.0357, Avg Loss: 5.2240\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 350, Loss: 5.2238, Avg Loss: 5.2263\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 400, Loss: 5.1331, Avg Loss: 5.2266\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Batch 450, Loss: 5.1383, Avg Loss: 5.2308\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 19, Average Loss: 5.2302\n",
      "Epoch 20, Batch 0, Loss: 5.1448, Avg Loss: 5.1448\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 50, Loss: 5.0462, Avg Loss: 5.1855\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 100, Loss: 5.2383, Avg Loss: 5.1872\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 150, Loss: 5.2121, Avg Loss: 5.1884\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 200, Loss: 5.1857, Avg Loss: 5.1950\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 250, Loss: 5.1351, Avg Loss: 5.1993\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 300, Loss: 5.2266, Avg Loss: 5.1985\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 350, Loss: 5.1598, Avg Loss: 5.2040\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 400, Loss: 5.3536, Avg Loss: 5.2102\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Batch 450, Loss: 5.2389, Avg Loss: 5.2124\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 20, Average Loss: 5.2141\n",
      "Epoch 21, Batch 0, Loss: 4.9394, Avg Loss: 4.9394\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 50, Loss: 5.2172, Avg Loss: 5.1740\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 100, Loss: 5.3049, Avg Loss: 5.2136\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 150, Loss: 5.3085, Avg Loss: 5.2148\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 200, Loss: 5.1941, Avg Loss: 5.2109\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 250, Loss: 5.2578, Avg Loss: 5.2104\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 300, Loss: 5.1019, Avg Loss: 5.2119\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 350, Loss: 5.1845, Avg Loss: 5.2123\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 400, Loss: 5.1299, Avg Loss: 5.2107\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Batch 450, Loss: 5.3823, Avg Loss: 5.2085\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 21, Average Loss: 5.2099\n",
      "Epoch 22, Batch 0, Loss: 5.2120, Avg Loss: 5.2120\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 50, Loss: 5.0901, Avg Loss: 5.1485\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 100, Loss: 5.3106, Avg Loss: 5.1630\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 150, Loss: 5.0873, Avg Loss: 5.1688\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 200, Loss: 5.2074, Avg Loss: 5.1645\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 250, Loss: 5.3421, Avg Loss: 5.1703\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 300, Loss: 5.1938, Avg Loss: 5.1754\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 350, Loss: 5.0945, Avg Loss: 5.1773\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 400, Loss: 5.0214, Avg Loss: 5.1784\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Batch 450, Loss: 5.2497, Avg Loss: 5.1782\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 22, Average Loss: 5.1813\n",
      "Epoch 23, Batch 0, Loss: 5.1461, Avg Loss: 5.1461\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 50, Loss: 5.1967, Avg Loss: 5.1391\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 100, Loss: 5.3575, Avg Loss: 5.1425\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 150, Loss: 5.0675, Avg Loss: 5.1491\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 200, Loss: 5.1716, Avg Loss: 5.1517\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 250, Loss: 4.9789, Avg Loss: 5.1580\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 300, Loss: 5.2615, Avg Loss: 5.1596\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 350, Loss: 5.1706, Avg Loss: 5.1590\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 400, Loss: 5.3043, Avg Loss: 5.1594\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Batch 450, Loss: 5.2018, Avg Loss: 5.1624\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 23, Average Loss: 5.1642\n",
      "Epoch 24, Batch 0, Loss: 5.1173, Avg Loss: 5.1173\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 50, Loss: 5.1053, Avg Loss: 5.1156\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 100, Loss: 5.1159, Avg Loss: 5.1234\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 150, Loss: 5.2193, Avg Loss: 5.1357\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 200, Loss: 5.1066, Avg Loss: 5.1383\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 250, Loss: 5.0669, Avg Loss: 5.1409\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 300, Loss: 5.0472, Avg Loss: 5.1402\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 350, Loss: 5.3034, Avg Loss: 5.1407\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 400, Loss: 5.0652, Avg Loss: 5.1432\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Batch 450, Loss: 5.1257, Avg Loss: 5.1415\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 24, Average Loss: 5.1421\n",
      "Epoch 25, Batch 0, Loss: 5.0435, Avg Loss: 5.0435\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 50, Loss: 4.9617, Avg Loss: 5.0556\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 100, Loss: 4.9714, Avg Loss: 5.0784\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 150, Loss: 5.0251, Avg Loss: 5.0865\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 200, Loss: 4.9314, Avg Loss: 5.0920\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 250, Loss: 5.1591, Avg Loss: 5.0989\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 300, Loss: 5.1092, Avg Loss: 5.1021\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 350, Loss: 4.8972, Avg Loss: 5.1060\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 400, Loss: 5.1277, Avg Loss: 5.1083\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Batch 450, Loss: 5.1782, Avg Loss: 5.1129\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 25, Average Loss: 5.1149\n",
      "Epoch 26, Batch 0, Loss: 5.0674, Avg Loss: 5.0674\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 50, Loss: 4.9675, Avg Loss: 5.0764\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 100, Loss: 5.0918, Avg Loss: 5.0847\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 150, Loss: 5.1566, Avg Loss: 5.0899\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 200, Loss: 5.0443, Avg Loss: 5.0884\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 250, Loss: 4.8560, Avg Loss: 5.0890\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 300, Loss: 5.0392, Avg Loss: 5.0917\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 350, Loss: 5.2164, Avg Loss: 5.0947\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 400, Loss: 4.9775, Avg Loss: 5.0954\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Batch 450, Loss: 5.2367, Avg Loss: 5.0954\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 26, Average Loss: 5.0952\n",
      "Epoch 27, Batch 0, Loss: 5.1837, Avg Loss: 5.1837\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 50, Loss: 5.0076, Avg Loss: 5.0305\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 100, Loss: 5.1253, Avg Loss: 5.0433\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 150, Loss: 5.0505, Avg Loss: 5.0477\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 200, Loss: 5.0537, Avg Loss: 5.0504\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 250, Loss: 4.9945, Avg Loss: 5.0484\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 300, Loss: 5.0774, Avg Loss: 5.0533\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 350, Loss: 5.1318, Avg Loss: 5.0580\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 400, Loss: 4.9933, Avg Loss: 5.0578\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Batch 450, Loss: 5.1714, Avg Loss: 5.0615\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 27, Average Loss: 5.0645\n",
      "Epoch 28, Batch 0, Loss: 5.0441, Avg Loss: 5.0441\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 50, Loss: 5.1852, Avg Loss: 5.0067\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 100, Loss: 5.0519, Avg Loss: 5.0108\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 150, Loss: 5.1255, Avg Loss: 5.0198\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 200, Loss: 4.9119, Avg Loss: 5.0213\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 250, Loss: 5.0064, Avg Loss: 5.0233\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 300, Loss: 5.0854, Avg Loss: 5.0266\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 350, Loss: 5.1174, Avg Loss: 5.0275\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 400, Loss: 5.1523, Avg Loss: 5.0308\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Batch 450, Loss: 5.0024, Avg Loss: 5.0312\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 28, Average Loss: 5.0332\n",
      "Epoch 29, Batch 0, Loss: 4.9776, Avg Loss: 4.9776\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 50, Loss: 4.9373, Avg Loss: 4.9610\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 100, Loss: 4.9587, Avg Loss: 4.9691\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 150, Loss: 4.9100, Avg Loss: 4.9739\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 200, Loss: 5.0006, Avg Loss: 4.9749\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 250, Loss: 4.9311, Avg Loss: 4.9802\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 300, Loss: 5.0133, Avg Loss: 4.9857\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 350, Loss: 5.0282, Avg Loss: 4.9911\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 400, Loss: 4.8478, Avg Loss: 4.9938\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Batch 450, Loss: 5.0066, Avg Loss: 4.9973\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 29, Average Loss: 4.9994\n",
      "Epoch 30, Batch 0, Loss: 4.9049, Avg Loss: 4.9049\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 50, Loss: 4.9403, Avg Loss: 4.9466\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 100, Loss: 4.8931, Avg Loss: 4.9472\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 150, Loss: 4.9055, Avg Loss: 4.9472\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 200, Loss: 4.9627, Avg Loss: 4.9452\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 250, Loss: 4.8901, Avg Loss: 4.9518\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 300, Loss: 4.9784, Avg Loss: 4.9542\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 350, Loss: 5.0649, Avg Loss: 4.9576\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 400, Loss: 5.0492, Avg Loss: 4.9603\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Batch 450, Loss: 5.0143, Avg Loss: 4.9615\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 30, Average Loss: 4.9611\n",
      "Epoch 31, Batch 0, Loss: 4.9728, Avg Loss: 4.9728\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 50, Loss: 4.9152, Avg Loss: 4.8723\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 100, Loss: 4.9302, Avg Loss: 4.8768\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 150, Loss: 4.9198, Avg Loss: 4.8883\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 200, Loss: 5.0329, Avg Loss: 4.8923\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 250, Loss: 4.8904, Avg Loss: 4.9038\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 300, Loss: 4.9099, Avg Loss: 4.9073\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 350, Loss: 4.9166, Avg Loss: 4.9108\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 400, Loss: 4.9512, Avg Loss: 4.9157\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Batch 450, Loss: 4.8886, Avg Loss: 4.9193\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 31, Average Loss: 4.9204\n",
      "Epoch 32, Batch 0, Loss: 4.9394, Avg Loss: 4.9394\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 50, Loss: 4.7939, Avg Loss: 4.8271\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 100, Loss: 4.8916, Avg Loss: 4.8297\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 150, Loss: 4.9483, Avg Loss: 4.8436\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 200, Loss: 4.8274, Avg Loss: 4.8519\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 250, Loss: 4.8443, Avg Loss: 4.8586\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 300, Loss: 4.8420, Avg Loss: 4.8628\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 350, Loss: 4.9583, Avg Loss: 4.8642\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 400, Loss: 4.8700, Avg Loss: 4.8708\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Batch 450, Loss: 4.8753, Avg Loss: 4.8758\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 32, Average Loss: 4.8772\n",
      "Epoch 33, Batch 0, Loss: 4.7363, Avg Loss: 4.7363\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 50, Loss: 4.8701, Avg Loss: 4.7949\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 100, Loss: 4.8025, Avg Loss: 4.8001\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 150, Loss: 4.9044, Avg Loss: 4.8036\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 200, Loss: 4.8342, Avg Loss: 4.8055\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 250, Loss: 4.9328, Avg Loss: 4.8101\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 300, Loss: 4.8488, Avg Loss: 4.8172\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 350, Loss: 4.8615, Avg Loss: 4.8188\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 400, Loss: 4.6919, Avg Loss: 4.8210\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Batch 450, Loss: 4.9507, Avg Loss: 4.8236\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 33, Average Loss: 4.8256\n",
      "Epoch 34, Batch 0, Loss: 4.7847, Avg Loss: 4.7847\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 34, Batch 50, Loss: 4.8150, Avg Loss: 4.7225\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n",
      "Epoch 34, Batch 100, Loss: 4.7865, Avg Loss: 4.7308\n",
      "GPU Memory: 8639.1MB / 16752.0MB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 68\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, num_epochs, device, vocab_size)\u001b[0m\n\u001b[0;32m     65\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Track loss (multiply by accumulation steps to get true loss)\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m gradient_accumulation_steps\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Print progress\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader,start_epoch=50, num_epochs=40, device=device, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e019f613-7e88-447f-8cf2-8c60d9f037cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (pos_encoding): PositionalEncoding()\n",
       "  (layers): ModuleList(\n",
       "    (0-23): 24 x TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=1024, out_features=12288, bias=True)\n",
       "        (fc2): Linear(in_features=12288, out_features=1024, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=1024, out_features=32000, bias=True)\n",
       "  (input_projection): Linear(in_features=1024, out_features=1024, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b91fb5f-85ef-4e9e-8f4f-d792523f1003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_9048\\4040975594.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_9048\\4040975594.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, Batch 0, Loss: 4.7486, Avg Loss: 4.7486\n",
      "Epoch 51, Batch 50, Loss: 4.7525, Avg Loss: 4.6951\n",
      "Epoch 51, Batch 100, Loss: 4.6004, Avg Loss: 4.6840\n",
      "Epoch 51, Batch 150, Loss: 4.4614, Avg Loss: 4.6691\n",
      "Epoch 51, Batch 200, Loss: 4.7306, Avg Loss: 4.6544\n",
      "Epoch 51, Batch 250, Loss: 4.5479, Avg Loss: 4.6441\n",
      "Epoch 51, Batch 300, Loss: 4.4485, Avg Loss: 4.6309\n",
      "Epoch 51, Batch 350, Loss: 4.5122, Avg Loss: 4.6185\n",
      "Epoch 51, Batch 400, Loss: 4.5297, Avg Loss: 4.6066\n",
      "Epoch 51, Batch 450, Loss: 4.5123, Avg Loss: 4.5976\n",
      "Epoch 51, Average Loss: 4.5903\n",
      "Epoch 52, Batch 0, Loss: 4.5649, Avg Loss: 4.5649\n",
      "Epoch 52, Batch 50, Loss: 4.5381, Avg Loss: 4.5005\n",
      "Epoch 52, Batch 100, Loss: 4.5162, Avg Loss: 4.4990\n",
      "Epoch 52, Batch 150, Loss: 4.3415, Avg Loss: 4.4908\n",
      "Epoch 52, Batch 200, Loss: 4.4661, Avg Loss: 4.4884\n",
      "Epoch 52, Batch 250, Loss: 4.3692, Avg Loss: 4.4885\n",
      "Epoch 52, Batch 300, Loss: 4.3686, Avg Loss: 4.4830\n",
      "Epoch 52, Batch 350, Loss: 4.4119, Avg Loss: 4.4837\n",
      "Epoch 52, Batch 400, Loss: 4.3464, Avg Loss: 4.4800\n",
      "Epoch 52, Batch 450, Loss: 4.5772, Avg Loss: 4.4768\n",
      "Epoch 52, Average Loss: 4.4763\n",
      "Epoch 53, Batch 0, Loss: 4.2755, Avg Loss: 4.2755\n",
      "Epoch 53, Batch 50, Loss: 4.3661, Avg Loss: 4.4312\n",
      "Epoch 53, Batch 100, Loss: 4.4672, Avg Loss: 4.4362\n",
      "Epoch 53, Batch 150, Loss: 4.3086, Avg Loss: 4.4340\n",
      "Epoch 53, Batch 200, Loss: 4.4386, Avg Loss: 4.4343\n",
      "Epoch 53, Batch 250, Loss: 4.4976, Avg Loss: 4.4315\n",
      "Epoch 53, Batch 300, Loss: 4.4905, Avg Loss: 4.4349\n",
      "Epoch 53, Batch 350, Loss: 4.4055, Avg Loss: 4.4349\n",
      "Epoch 53, Batch 400, Loss: 4.4448, Avg Loss: 4.4339\n",
      "Epoch 53, Batch 450, Loss: 4.3004, Avg Loss: 4.4323\n",
      "Epoch 53, Average Loss: 4.4302\n",
      "Epoch 54, Batch 0, Loss: 4.3795, Avg Loss: 4.3795\n",
      "Epoch 54, Batch 50, Loss: 4.2740, Avg Loss: 4.3780\n",
      "Epoch 54, Batch 100, Loss: 4.3755, Avg Loss: 4.3750\n",
      "Epoch 54, Batch 150, Loss: 4.4323, Avg Loss: 4.3794\n",
      "Epoch 54, Batch 200, Loss: 4.3900, Avg Loss: 4.3833\n",
      "Epoch 54, Batch 250, Loss: 4.4325, Avg Loss: 4.3857\n",
      "Epoch 54, Batch 300, Loss: 4.4801, Avg Loss: 4.3875\n",
      "Epoch 54, Batch 350, Loss: 4.4198, Avg Loss: 4.3870\n",
      "Epoch 54, Batch 400, Loss: 4.4357, Avg Loss: 4.3887\n",
      "Epoch 54, Batch 450, Loss: 4.4863, Avg Loss: 4.3891\n",
      "Epoch 54, Average Loss: 4.3920\n",
      "Epoch 55, Batch 0, Loss: 4.3689, Avg Loss: 4.3689\n",
      "Epoch 55, Batch 50, Loss: 4.2552, Avg Loss: 4.3550\n",
      "Epoch 55, Batch 100, Loss: 4.3968, Avg Loss: 4.3531\n",
      "Epoch 55, Batch 150, Loss: 4.3104, Avg Loss: 4.3507\n",
      "Epoch 55, Batch 200, Loss: 4.2267, Avg Loss: 4.3496\n",
      "Epoch 55, Batch 250, Loss: 4.2975, Avg Loss: 4.3520\n",
      "Epoch 55, Batch 300, Loss: 4.3545, Avg Loss: 4.3523\n",
      "Epoch 55, Batch 350, Loss: 4.2911, Avg Loss: 4.3502\n",
      "Epoch 55, Batch 400, Loss: 4.1901, Avg Loss: 4.3511\n",
      "Epoch 55, Batch 450, Loss: 4.2894, Avg Loss: 4.3505\n",
      "Epoch 55, Average Loss: 4.3509\n",
      "Epoch 56, Batch 0, Loss: 4.2485, Avg Loss: 4.2485\n",
      "Epoch 56, Batch 50, Loss: 4.1655, Avg Loss: 4.2876\n",
      "Epoch 56, Batch 100, Loss: 4.3092, Avg Loss: 4.2841\n",
      "Epoch 56, Batch 150, Loss: 4.4027, Avg Loss: 4.2917\n",
      "Epoch 56, Batch 200, Loss: 4.3579, Avg Loss: 4.2948\n",
      "Epoch 56, Batch 250, Loss: 4.2373, Avg Loss: 4.2943\n",
      "Epoch 56, Batch 300, Loss: 4.3287, Avg Loss: 4.2941\n",
      "Epoch 56, Batch 350, Loss: 4.4091, Avg Loss: 4.2975\n",
      "Epoch 56, Batch 400, Loss: 4.2472, Avg Loss: 4.2992\n",
      "Epoch 56, Batch 450, Loss: 4.2418, Avg Loss: 4.3009\n",
      "Epoch 56, Average Loss: 4.3041\n",
      "Epoch 57, Batch 0, Loss: 4.3168, Avg Loss: 4.3168\n",
      "Epoch 57, Batch 50, Loss: 4.2618, Avg Loss: 4.2244\n",
      "Epoch 57, Batch 100, Loss: 4.1815, Avg Loss: 4.2269\n",
      "Epoch 57, Batch 150, Loss: 4.3070, Avg Loss: 4.2321\n",
      "Epoch 57, Batch 200, Loss: 4.2101, Avg Loss: 4.2354\n",
      "Epoch 57, Batch 250, Loss: 4.2815, Avg Loss: 4.2375\n",
      "Epoch 57, Batch 300, Loss: 4.2849, Avg Loss: 4.2429\n",
      "Epoch 57, Batch 350, Loss: 4.2973, Avg Loss: 4.2437\n",
      "Epoch 57, Batch 400, Loss: 4.2713, Avg Loss: 4.2469\n",
      "Epoch 57, Batch 450, Loss: 4.2316, Avg Loss: 4.2482\n",
      "Epoch 57, Average Loss: 4.2505\n",
      "Epoch 58, Batch 0, Loss: 4.2532, Avg Loss: 4.2532\n",
      "Epoch 58, Batch 50, Loss: 4.2468, Avg Loss: 4.1636\n",
      "Epoch 58, Batch 100, Loss: 4.0985, Avg Loss: 4.1612\n",
      "Epoch 58, Batch 150, Loss: 4.2439, Avg Loss: 4.1662\n",
      "Epoch 58, Batch 200, Loss: 4.1948, Avg Loss: 4.1716\n",
      "Epoch 58, Batch 250, Loss: 4.1643, Avg Loss: 4.1773\n",
      "Epoch 58, Batch 300, Loss: 4.2514, Avg Loss: 4.1801\n",
      "Epoch 58, Batch 350, Loss: 4.1672, Avg Loss: 4.1834\n",
      "Epoch 58, Batch 400, Loss: 4.2062, Avg Loss: 4.1846\n",
      "Epoch 58, Batch 450, Loss: 4.1818, Avg Loss: 4.1879\n",
      "Epoch 58, Average Loss: 4.1910\n",
      "Epoch 59, Batch 0, Loss: 4.1025, Avg Loss: 4.1025\n",
      "Epoch 59, Batch 50, Loss: 4.0895, Avg Loss: 4.0885\n",
      "Epoch 59, Batch 100, Loss: 4.1193, Avg Loss: 4.0920\n",
      "Epoch 59, Batch 150, Loss: 4.2154, Avg Loss: 4.1008\n",
      "Epoch 59, Batch 200, Loss: 4.1698, Avg Loss: 4.1063\n",
      "Epoch 59, Batch 250, Loss: 4.1998, Avg Loss: 4.1104\n",
      "Epoch 59, Batch 300, Loss: 4.2177, Avg Loss: 4.1133\n",
      "Epoch 59, Batch 350, Loss: 4.0786, Avg Loss: 4.1165\n",
      "Epoch 59, Batch 400, Loss: 4.1342, Avg Loss: 4.1212\n",
      "Epoch 59, Batch 450, Loss: 4.1166, Avg Loss: 4.1237\n",
      "Epoch 59, Average Loss: 4.1248\n",
      "Epoch 60, Batch 0, Loss: 3.9638, Avg Loss: 3.9638\n",
      "Epoch 60, Batch 50, Loss: 3.8785, Avg Loss: 4.0015\n",
      "Epoch 60, Batch 100, Loss: 4.0552, Avg Loss: 4.0340\n",
      "Epoch 60, Batch 150, Loss: 4.0776, Avg Loss: 4.0285\n",
      "Epoch 60, Batch 200, Loss: 3.9226, Avg Loss: 4.0331\n",
      "Epoch 60, Batch 250, Loss: 4.1046, Avg Loss: 4.0388\n",
      "Epoch 60, Batch 300, Loss: 4.0858, Avg Loss: 4.0406\n",
      "Epoch 60, Batch 350, Loss: 4.1397, Avg Loss: 4.0458\n",
      "Epoch 60, Batch 400, Loss: 3.9931, Avg Loss: 4.0483\n",
      "Epoch 60, Batch 450, Loss: 4.1082, Avg Loss: 4.0517\n",
      "Epoch 60, Average Loss: 4.0534\n",
      "Epoch 61, Batch 0, Loss: 3.8498, Avg Loss: 3.8498\n",
      "Epoch 61, Batch 50, Loss: 3.7094, Avg Loss: 3.9251\n",
      "Epoch 61, Batch 100, Loss: 4.0376, Avg Loss: 3.9312\n",
      "Epoch 61, Batch 150, Loss: 3.9056, Avg Loss: 3.9424\n",
      "Epoch 61, Batch 200, Loss: 3.9751, Avg Loss: 3.9480\n",
      "Epoch 61, Batch 250, Loss: 4.0140, Avg Loss: 3.9552\n",
      "Epoch 61, Batch 300, Loss: 4.1570, Avg Loss: 3.9611\n",
      "Epoch 61, Batch 350, Loss: 4.0894, Avg Loss: 3.9682\n",
      "Epoch 61, Batch 400, Loss: 4.1825, Avg Loss: 3.9715\n",
      "Epoch 61, Batch 450, Loss: 4.0279, Avg Loss: 3.9737\n",
      "Epoch 61, Average Loss: 3.9757\n",
      "Epoch 62, Batch 0, Loss: 3.8241, Avg Loss: 3.8241\n",
      "Epoch 62, Batch 50, Loss: 3.9162, Avg Loss: 3.8363\n",
      "Epoch 62, Batch 100, Loss: 3.8819, Avg Loss: 3.8420\n",
      "Epoch 62, Batch 150, Loss: 3.8455, Avg Loss: 3.8444\n",
      "Epoch 62, Batch 200, Loss: 3.7999, Avg Loss: 3.8559\n",
      "Epoch 62, Batch 250, Loss: 3.8995, Avg Loss: 3.8621\n",
      "Epoch 62, Batch 300, Loss: 3.8247, Avg Loss: 3.8702\n",
      "Epoch 62, Batch 350, Loss: 3.8740, Avg Loss: 3.8764\n",
      "Epoch 62, Batch 400, Loss: 3.9642, Avg Loss: 3.8818\n",
      "Epoch 62, Batch 450, Loss: 4.1037, Avg Loss: 3.8889\n",
      "Epoch 62, Average Loss: 3.8924\n",
      "Epoch 63, Batch 0, Loss: 3.7414, Avg Loss: 3.7414\n",
      "Epoch 63, Batch 50, Loss: 3.7199, Avg Loss: 3.7485\n",
      "Epoch 63, Batch 100, Loss: 3.6527, Avg Loss: 3.7537\n",
      "Epoch 63, Batch 150, Loss: 3.7272, Avg Loss: 3.7644\n",
      "Epoch 63, Batch 200, Loss: 3.8227, Avg Loss: 3.7688\n",
      "Epoch 63, Batch 250, Loss: 3.8004, Avg Loss: 3.7768\n",
      "Epoch 63, Batch 300, Loss: 3.7176, Avg Loss: 3.7862\n",
      "Epoch 63, Batch 350, Loss: 3.7722, Avg Loss: 3.7919\n",
      "Epoch 63, Batch 400, Loss: 3.8856, Avg Loss: 3.7969\n",
      "Epoch 63, Batch 450, Loss: 3.8787, Avg Loss: 3.7990\n",
      "Epoch 63, Average Loss: 3.8051\n",
      "Epoch 64, Batch 0, Loss: 3.6048, Avg Loss: 3.6048\n",
      "Epoch 64, Batch 50, Loss: 3.5586, Avg Loss: 3.6292\n",
      "Epoch 64, Batch 100, Loss: 3.6008, Avg Loss: 3.6489\n",
      "Epoch 64, Batch 150, Loss: 3.6914, Avg Loss: 3.6657\n",
      "Epoch 64, Batch 200, Loss: 3.7632, Avg Loss: 3.6740\n",
      "Epoch 64, Batch 250, Loss: 3.7837, Avg Loss: 3.6787\n",
      "Epoch 64, Batch 300, Loss: 3.7178, Avg Loss: 3.6856\n",
      "Epoch 64, Batch 350, Loss: 3.7511, Avg Loss: 3.6942\n",
      "Epoch 64, Batch 400, Loss: 3.7001, Avg Loss: 3.7010\n",
      "Epoch 64, Batch 450, Loss: 3.7232, Avg Loss: 3.7065\n",
      "Epoch 64, Average Loss: 3.7132\n",
      "Epoch 65, Batch 0, Loss: 3.4180, Avg Loss: 3.4180\n",
      "Epoch 65, Batch 50, Loss: 3.5697, Avg Loss: 3.5498\n",
      "Epoch 65, Batch 100, Loss: 3.5405, Avg Loss: 3.5631\n",
      "Epoch 65, Batch 150, Loss: 3.6270, Avg Loss: 3.5651\n",
      "Epoch 65, Batch 200, Loss: 3.5903, Avg Loss: 3.5768\n",
      "Epoch 65, Batch 250, Loss: 3.5477, Avg Loss: 3.5848\n",
      "Epoch 65, Batch 300, Loss: 3.7680, Avg Loss: 3.5898\n",
      "Epoch 65, Batch 350, Loss: 3.5562, Avg Loss: 3.5989\n",
      "Epoch 65, Batch 400, Loss: 3.6699, Avg Loss: 3.6038\n",
      "Epoch 65, Batch 450, Loss: 3.7888, Avg Loss: 3.6103\n",
      "Epoch 65, Average Loss: 3.6175\n",
      "Epoch 66, Batch 0, Loss: 3.5700, Avg Loss: 3.5700\n",
      "Epoch 66, Batch 50, Loss: 3.2615, Avg Loss: 3.4485\n",
      "Epoch 66, Batch 100, Loss: 3.4807, Avg Loss: 3.4537\n",
      "Epoch 66, Batch 150, Loss: 3.5317, Avg Loss: 3.4663\n",
      "Epoch 66, Batch 200, Loss: 3.3287, Avg Loss: 3.4722\n",
      "Epoch 66, Batch 250, Loss: 3.5320, Avg Loss: 3.4810\n",
      "Epoch 66, Batch 300, Loss: 3.4863, Avg Loss: 3.4872\n",
      "Epoch 66, Batch 350, Loss: 3.5926, Avg Loss: 3.4970\n",
      "Epoch 66, Batch 400, Loss: 3.3860, Avg Loss: 3.5027\n",
      "Epoch 66, Batch 450, Loss: 3.7668, Avg Loss: 3.5089\n",
      "Epoch 66, Average Loss: 3.5151\n",
      "Epoch 67, Batch 0, Loss: 3.5376, Avg Loss: 3.5376\n",
      "Epoch 67, Batch 50, Loss: 3.4200, Avg Loss: 3.3182\n",
      "Epoch 67, Batch 100, Loss: 3.2757, Avg Loss: 3.3265\n",
      "Epoch 67, Batch 150, Loss: 3.4336, Avg Loss: 3.3442\n",
      "Epoch 67, Batch 200, Loss: 3.4751, Avg Loss: 3.3604\n",
      "Epoch 67, Batch 250, Loss: 3.2494, Avg Loss: 3.3709\n",
      "Epoch 67, Batch 300, Loss: 3.5244, Avg Loss: 3.3807\n",
      "Epoch 67, Batch 350, Loss: 3.5349, Avg Loss: 3.3905\n",
      "Epoch 67, Batch 400, Loss: 3.4673, Avg Loss: 3.4004\n",
      "Epoch 67, Batch 450, Loss: 3.4838, Avg Loss: 3.4068\n",
      "Epoch 67, Average Loss: 3.4143\n",
      "Epoch 68, Batch 0, Loss: 3.1165, Avg Loss: 3.1165\n",
      "Epoch 68, Batch 50, Loss: 3.3049, Avg Loss: 3.2088\n",
      "Epoch 68, Batch 100, Loss: 3.1595, Avg Loss: 3.2185\n",
      "Epoch 68, Batch 150, Loss: 3.1910, Avg Loss: 3.2378\n",
      "Epoch 68, Batch 200, Loss: 3.1755, Avg Loss: 3.2477\n",
      "Epoch 68, Batch 250, Loss: 3.3967, Avg Loss: 3.2562\n",
      "Epoch 68, Batch 300, Loss: 3.3315, Avg Loss: 3.2750\n",
      "Epoch 68, Batch 350, Loss: 3.4384, Avg Loss: 3.2871\n",
      "Epoch 68, Batch 400, Loss: 3.4847, Avg Loss: 3.2977\n",
      "Epoch 68, Batch 450, Loss: 3.1986, Avg Loss: 3.3064\n",
      "Epoch 68, Average Loss: 3.3140\n",
      "Epoch 69, Batch 0, Loss: 3.1797, Avg Loss: 3.1797\n",
      "Epoch 69, Batch 50, Loss: 3.0759, Avg Loss: 3.0909\n",
      "Epoch 69, Batch 100, Loss: 3.3069, Avg Loss: 3.1115\n",
      "Epoch 69, Batch 150, Loss: 3.1447, Avg Loss: 3.1281\n",
      "Epoch 69, Batch 200, Loss: 3.1034, Avg Loss: 3.1385\n",
      "Epoch 69, Batch 250, Loss: 3.2208, Avg Loss: 3.1601\n",
      "Epoch 69, Batch 300, Loss: 3.4617, Avg Loss: 3.1676\n",
      "Epoch 69, Batch 350, Loss: 3.1349, Avg Loss: 3.1765\n",
      "Epoch 69, Batch 400, Loss: 3.2795, Avg Loss: 3.1876\n",
      "Epoch 69, Batch 450, Loss: 3.1998, Avg Loss: 3.1973\n",
      "Epoch 69, Average Loss: 3.2075\n",
      "Epoch 70, Batch 0, Loss: 2.8829, Avg Loss: 2.8829\n",
      "Epoch 70, Batch 50, Loss: 2.9200, Avg Loss: 2.9536\n",
      "Epoch 70, Batch 100, Loss: 3.1669, Avg Loss: 2.9814\n",
      "Epoch 70, Batch 150, Loss: 3.0291, Avg Loss: 3.0041\n",
      "Epoch 70, Batch 200, Loss: 2.9120, Avg Loss: 3.0228\n",
      "Epoch 70, Batch 250, Loss: 3.0564, Avg Loss: 3.0385\n",
      "Epoch 70, Batch 300, Loss: 3.0851, Avg Loss: 3.0501\n",
      "Epoch 70, Batch 350, Loss: 3.1675, Avg Loss: 3.0627\n",
      "Epoch 70, Batch 400, Loss: 3.5241, Avg Loss: 3.0771\n",
      "Epoch 70, Batch 450, Loss: 3.1404, Avg Loss: 3.0900\n",
      "Epoch 70, Average Loss: 3.1002\n",
      "Epoch 71, Batch 0, Loss: 3.1530, Avg Loss: 3.1530\n",
      "Epoch 71, Batch 50, Loss: 2.9863, Avg Loss: 2.8786\n",
      "Epoch 71, Batch 100, Loss: 2.9179, Avg Loss: 2.8926\n",
      "Epoch 71, Batch 150, Loss: 2.8391, Avg Loss: 2.9040\n",
      "Epoch 71, Batch 200, Loss: 3.0684, Avg Loss: 2.9199\n",
      "Epoch 71, Batch 250, Loss: 3.1543, Avg Loss: 2.9376\n",
      "Epoch 71, Batch 300, Loss: 3.1143, Avg Loss: 2.9493\n",
      "Epoch 71, Batch 350, Loss: 3.1770, Avg Loss: 2.9589\n",
      "Epoch 71, Batch 400, Loss: 3.1018, Avg Loss: 2.9745\n",
      "Epoch 71, Batch 450, Loss: 3.2282, Avg Loss: 2.9831\n",
      "Epoch 71, Average Loss: 2.9977\n",
      "Epoch 72, Batch 0, Loss: 2.5685, Avg Loss: 2.5685\n",
      "Epoch 72, Batch 50, Loss: 2.9096, Avg Loss: 2.7475\n",
      "Epoch 72, Batch 100, Loss: 2.9850, Avg Loss: 2.7718\n",
      "Epoch 72, Batch 150, Loss: 2.8264, Avg Loss: 2.7851\n",
      "Epoch 72, Batch 200, Loss: 2.7935, Avg Loss: 2.8054\n",
      "Epoch 72, Batch 250, Loss: 2.9369, Avg Loss: 2.8234\n",
      "Epoch 72, Batch 300, Loss: 2.7630, Avg Loss: 2.8321\n",
      "Epoch 72, Batch 350, Loss: 2.7948, Avg Loss: 2.8459\n",
      "Epoch 72, Batch 400, Loss: 2.9082, Avg Loss: 2.8591\n",
      "Epoch 72, Batch 450, Loss: 3.0719, Avg Loss: 2.8761\n",
      "Epoch 72, Average Loss: 2.8879\n",
      "Epoch 73, Batch 0, Loss: 2.4894, Avg Loss: 2.4894\n",
      "Epoch 73, Batch 50, Loss: 2.6778, Avg Loss: 2.6381\n",
      "Epoch 73, Batch 100, Loss: 2.8622, Avg Loss: 2.6743\n",
      "Epoch 73, Batch 150, Loss: 2.8995, Avg Loss: 2.6995\n",
      "Epoch 73, Batch 200, Loss: 2.6582, Avg Loss: 2.7287\n",
      "Epoch 73, Batch 250, Loss: 2.8363, Avg Loss: 2.7392\n",
      "Epoch 73, Batch 300, Loss: 2.9766, Avg Loss: 2.7472\n",
      "Epoch 73, Batch 350, Loss: 2.6369, Avg Loss: 2.7605\n",
      "Epoch 73, Batch 400, Loss: 2.9452, Avg Loss: 2.7698\n",
      "Epoch 73, Batch 450, Loss: 3.0436, Avg Loss: 2.7773\n",
      "Epoch 73, Average Loss: 2.7895\n",
      "Epoch 74, Batch 0, Loss: 2.5854, Avg Loss: 2.5854\n",
      "Epoch 74, Batch 50, Loss: 2.6777, Avg Loss: 2.5829\n",
      "Epoch 74, Batch 100, Loss: 2.7336, Avg Loss: 2.5903\n",
      "Epoch 74, Batch 150, Loss: 2.6096, Avg Loss: 2.6053\n",
      "Epoch 74, Batch 200, Loss: 2.7201, Avg Loss: 2.6117\n",
      "Epoch 74, Batch 250, Loss: 2.6748, Avg Loss: 2.6286\n",
      "Epoch 74, Batch 300, Loss: 2.7825, Avg Loss: 2.6397\n",
      "Epoch 74, Batch 350, Loss: 2.5050, Avg Loss: 2.6537\n",
      "Epoch 74, Batch 400, Loss: 3.0655, Avg Loss: 2.6679\n",
      "Epoch 74, Batch 450, Loss: 2.9704, Avg Loss: 2.6833\n",
      "Epoch 74, Average Loss: 2.6962\n",
      "Epoch 75, Batch 0, Loss: 2.4171, Avg Loss: 2.4171\n",
      "Epoch 75, Batch 50, Loss: 2.3603, Avg Loss: 2.4002\n",
      "Epoch 75, Batch 100, Loss: 2.6262, Avg Loss: 2.4580\n",
      "Epoch 75, Batch 150, Loss: 2.5357, Avg Loss: 2.4673\n",
      "Epoch 75, Batch 200, Loss: 2.3598, Avg Loss: 2.4906\n",
      "Epoch 75, Batch 250, Loss: 2.7177, Avg Loss: 2.5167\n",
      "Epoch 75, Batch 300, Loss: 2.6943, Avg Loss: 2.5283\n",
      "Epoch 75, Batch 350, Loss: 2.4581, Avg Loss: 2.5433\n",
      "Epoch 75, Batch 400, Loss: 2.5312, Avg Loss: 2.5536\n",
      "Epoch 75, Batch 450, Loss: 2.8008, Avg Loss: 2.5698\n",
      "Epoch 75, Average Loss: 2.5809\n",
      "Epoch 76, Batch 0, Loss: 2.4042, Avg Loss: 2.4042\n",
      "Epoch 76, Batch 50, Loss: 2.0871, Avg Loss: 2.3298\n",
      "Epoch 76, Batch 100, Loss: 2.5630, Avg Loss: 2.3728\n",
      "Epoch 76, Batch 150, Loss: 2.3463, Avg Loss: 2.3928\n",
      "Epoch 76, Batch 200, Loss: 2.6161, Avg Loss: 2.4112\n",
      "Epoch 76, Batch 250, Loss: 2.7157, Avg Loss: 2.4145\n",
      "Epoch 76, Batch 300, Loss: 2.3660, Avg Loss: 2.4262\n",
      "Epoch 76, Batch 350, Loss: 2.3814, Avg Loss: 2.4347\n",
      "Epoch 76, Batch 400, Loss: 3.0128, Avg Loss: 2.4510\n",
      "Epoch 76, Batch 450, Loss: 2.8102, Avg Loss: 2.4678\n",
      "Epoch 76, Average Loss: 2.4796\n",
      "Epoch 77, Batch 0, Loss: 2.1254, Avg Loss: 2.1254\n",
      "Epoch 77, Batch 50, Loss: 2.3329, Avg Loss: 2.2604\n",
      "Epoch 77, Batch 100, Loss: 2.4762, Avg Loss: 2.2651\n",
      "Epoch 77, Batch 150, Loss: 2.1504, Avg Loss: 2.2850\n",
      "Epoch 77, Batch 200, Loss: 2.5365, Avg Loss: 2.2985\n",
      "Epoch 77, Batch 250, Loss: 2.1828, Avg Loss: 2.3078\n",
      "Epoch 77, Batch 300, Loss: 2.4280, Avg Loss: 2.3180\n",
      "Epoch 77, Batch 350, Loss: 2.6557, Avg Loss: 2.3326\n",
      "Epoch 77, Batch 400, Loss: 2.6976, Avg Loss: 2.3496\n",
      "Epoch 77, Batch 450, Loss: 2.3720, Avg Loss: 2.3642\n",
      "Epoch 77, Average Loss: 2.3823\n",
      "Epoch 78, Batch 0, Loss: 2.1748, Avg Loss: 2.1748\n",
      "Epoch 78, Batch 50, Loss: 1.9860, Avg Loss: 2.1288\n",
      "Epoch 78, Batch 100, Loss: 2.0904, Avg Loss: 2.1427\n",
      "Epoch 78, Batch 150, Loss: 2.3494, Avg Loss: 2.1598\n",
      "Epoch 78, Batch 200, Loss: 2.4589, Avg Loss: 2.1814\n",
      "Epoch 78, Batch 250, Loss: 2.3335, Avg Loss: 2.1951\n",
      "Epoch 78, Batch 300, Loss: 2.5100, Avg Loss: 2.2125\n",
      "Epoch 78, Batch 350, Loss: 2.2169, Avg Loss: 2.2232\n",
      "Epoch 78, Batch 400, Loss: 2.1742, Avg Loss: 2.2394\n",
      "Epoch 78, Batch 450, Loss: 2.5630, Avg Loss: 2.2587\n",
      "Epoch 78, Average Loss: 2.2738\n",
      "Epoch 79, Batch 0, Loss: 2.0944, Avg Loss: 2.0944\n",
      "Epoch 79, Batch 50, Loss: 1.9614, Avg Loss: 2.0128\n",
      "Epoch 79, Batch 100, Loss: 1.9169, Avg Loss: 2.0300\n",
      "Epoch 79, Batch 150, Loss: 2.1369, Avg Loss: 2.0538\n",
      "Epoch 79, Batch 200, Loss: 2.2940, Avg Loss: 2.0771\n",
      "Epoch 79, Batch 250, Loss: 2.3898, Avg Loss: 2.1121\n",
      "Epoch 79, Batch 300, Loss: 2.1544, Avg Loss: 2.1326\n",
      "Epoch 79, Batch 350, Loss: 2.2661, Avg Loss: 2.1455\n",
      "Epoch 79, Batch 400, Loss: 2.6220, Avg Loss: 2.1566\n",
      "Epoch 79, Batch 450, Loss: 2.2712, Avg Loss: 2.1721\n",
      "Epoch 79, Average Loss: 2.1860\n",
      "Epoch 80, Batch 0, Loss: 2.1220, Avg Loss: 2.1220\n",
      "Epoch 80, Batch 50, Loss: 2.1714, Avg Loss: 1.9001\n",
      "Epoch 80, Batch 100, Loss: 2.2095, Avg Loss: 1.9328\n",
      "Epoch 80, Batch 150, Loss: 1.6439, Avg Loss: 1.9533\n",
      "Epoch 80, Batch 200, Loss: 2.1260, Avg Loss: 1.9733\n",
      "Epoch 80, Batch 250, Loss: 2.1751, Avg Loss: 1.9937\n",
      "Epoch 80, Batch 300, Loss: 1.9414, Avg Loss: 2.0134\n",
      "Epoch 80, Batch 350, Loss: 2.0647, Avg Loss: 2.0347\n",
      "Epoch 80, Batch 400, Loss: 2.2174, Avg Loss: 2.0448\n",
      "Epoch 80, Batch 450, Loss: 2.5356, Avg Loss: 2.0661\n",
      "Epoch 80, Average Loss: 2.0821\n",
      "Epoch 81, Batch 0, Loss: 2.0813, Avg Loss: 2.0813\n",
      "Epoch 81, Batch 50, Loss: 1.8433, Avg Loss: 1.8329\n",
      "Epoch 81, Batch 100, Loss: 1.6190, Avg Loss: 1.8419\n",
      "Epoch 81, Batch 150, Loss: 2.1294, Avg Loss: 1.8522\n",
      "Epoch 81, Batch 200, Loss: 1.8419, Avg Loss: 1.8807\n",
      "Epoch 81, Batch 250, Loss: 2.0744, Avg Loss: 1.8954\n",
      "Epoch 81, Batch 300, Loss: 2.0849, Avg Loss: 1.9227\n",
      "Epoch 81, Batch 350, Loss: 2.0733, Avg Loss: 1.9446\n",
      "Epoch 81, Batch 400, Loss: 1.7596, Avg Loss: 1.9595\n",
      "Epoch 81, Batch 450, Loss: 2.1727, Avg Loss: 1.9752\n",
      "Epoch 81, Average Loss: 1.9862\n",
      "Epoch 82, Batch 0, Loss: 1.7108, Avg Loss: 1.7108\n",
      "Epoch 82, Batch 50, Loss: 1.9356, Avg Loss: 1.7310\n",
      "Epoch 82, Batch 100, Loss: 1.8314, Avg Loss: 1.7623\n",
      "Epoch 82, Batch 150, Loss: 1.6928, Avg Loss: 1.7716\n",
      "Epoch 82, Batch 200, Loss: 1.8932, Avg Loss: 1.7906\n",
      "Epoch 82, Batch 250, Loss: 1.9140, Avg Loss: 1.8161\n",
      "Epoch 82, Batch 300, Loss: 1.9277, Avg Loss: 1.8354\n",
      "Epoch 82, Batch 350, Loss: 1.8324, Avg Loss: 1.8490\n",
      "Epoch 82, Batch 400, Loss: 1.7840, Avg Loss: 1.8663\n",
      "Epoch 82, Batch 450, Loss: 1.7528, Avg Loss: 1.8798\n",
      "Epoch 82, Average Loss: 1.8983\n",
      "Epoch 83, Batch 0, Loss: 1.7444, Avg Loss: 1.7444\n",
      "Epoch 83, Batch 50, Loss: 1.7122, Avg Loss: 1.6448\n",
      "Epoch 83, Batch 100, Loss: 1.5451, Avg Loss: 1.6820\n",
      "Epoch 83, Batch 150, Loss: 1.4099, Avg Loss: 1.6999\n",
      "Epoch 83, Batch 200, Loss: 1.7917, Avg Loss: 1.7076\n",
      "Epoch 83, Batch 250, Loss: 1.6377, Avg Loss: 1.7370\n",
      "Epoch 83, Batch 300, Loss: 1.9876, Avg Loss: 1.7472\n",
      "Epoch 83, Batch 350, Loss: 1.6940, Avg Loss: 1.7659\n",
      "Epoch 83, Batch 400, Loss: 1.8544, Avg Loss: 1.7762\n",
      "Epoch 83, Batch 450, Loss: 1.7780, Avg Loss: 1.7918\n",
      "Epoch 83, Average Loss: 1.8091\n",
      "Epoch 84, Batch 0, Loss: 1.4332, Avg Loss: 1.4332\n",
      "Epoch 84, Batch 50, Loss: 1.5972, Avg Loss: 1.5483\n",
      "Epoch 84, Batch 100, Loss: 1.9641, Avg Loss: 1.5771\n",
      "Epoch 84, Batch 150, Loss: 1.7613, Avg Loss: 1.5966\n",
      "Epoch 84, Batch 200, Loss: 1.8563, Avg Loss: 1.6172\n",
      "Epoch 84, Batch 250, Loss: 1.8085, Avg Loss: 1.6313\n",
      "Epoch 84, Batch 300, Loss: 1.4218, Avg Loss: 1.6451\n",
      "Epoch 84, Batch 350, Loss: 1.5956, Avg Loss: 1.6571\n",
      "Epoch 84, Batch 400, Loss: 1.7030, Avg Loss: 1.6780\n",
      "Epoch 84, Batch 450, Loss: 1.9891, Avg Loss: 1.6925\n",
      "Epoch 84, Average Loss: 1.7124\n",
      "Epoch 85, Batch 0, Loss: 1.5980, Avg Loss: 1.5980\n",
      "Epoch 85, Batch 50, Loss: 1.8241, Avg Loss: 1.5381\n",
      "Epoch 85, Batch 100, Loss: 1.7756, Avg Loss: 1.5410\n",
      "Epoch 85, Batch 150, Loss: 1.3964, Avg Loss: 1.5426\n",
      "Epoch 85, Batch 200, Loss: 1.8108, Avg Loss: 1.5514\n",
      "Epoch 85, Batch 250, Loss: 1.7894, Avg Loss: 1.5803\n",
      "Epoch 85, Batch 300, Loss: 1.5096, Avg Loss: 1.6091\n",
      "Epoch 85, Batch 350, Loss: 1.5927, Avg Loss: 1.6205\n",
      "Epoch 85, Batch 400, Loss: 1.4243, Avg Loss: 1.6313\n",
      "Epoch 85, Batch 450, Loss: 1.5977, Avg Loss: 1.6421\n",
      "Epoch 85, Average Loss: 1.6506\n",
      "Epoch 86, Batch 0, Loss: 1.3677, Avg Loss: 1.3677\n",
      "Epoch 86, Batch 50, Loss: 1.9240, Avg Loss: 1.4411\n",
      "Epoch 86, Batch 100, Loss: 1.4685, Avg Loss: 1.4373\n",
      "Epoch 86, Batch 150, Loss: 1.6447, Avg Loss: 1.4578\n",
      "Epoch 86, Batch 200, Loss: 1.5405, Avg Loss: 1.4786\n",
      "Epoch 86, Batch 250, Loss: 1.4534, Avg Loss: 1.4948\n",
      "Epoch 86, Batch 300, Loss: 1.3931, Avg Loss: 1.5110\n",
      "Epoch 86, Batch 350, Loss: 1.5065, Avg Loss: 1.5236\n",
      "Epoch 86, Batch 400, Loss: 1.6396, Avg Loss: 1.5399\n",
      "Epoch 86, Batch 450, Loss: 1.9665, Avg Loss: 1.5528\n",
      "Epoch 86, Average Loss: 1.5729\n",
      "Epoch 87, Batch 0, Loss: 1.4800, Avg Loss: 1.4800\n",
      "Epoch 87, Batch 50, Loss: 1.0925, Avg Loss: 1.3437\n",
      "Epoch 87, Batch 100, Loss: 1.3328, Avg Loss: 1.3653\n",
      "Epoch 87, Batch 150, Loss: 1.9216, Avg Loss: 1.3983\n",
      "Epoch 87, Batch 200, Loss: 1.2958, Avg Loss: 1.4090\n",
      "Epoch 87, Batch 250, Loss: 1.6872, Avg Loss: 1.4292\n",
      "Epoch 87, Batch 300, Loss: 1.3189, Avg Loss: 1.4423\n",
      "Epoch 87, Batch 350, Loss: 1.7398, Avg Loss: 1.4538\n",
      "Epoch 87, Batch 400, Loss: 1.7539, Avg Loss: 1.4624\n",
      "Epoch 87, Batch 450, Loss: 1.5920, Avg Loss: 1.4731\n",
      "Epoch 87, Average Loss: 1.4841\n",
      "Epoch 88, Batch 0, Loss: 1.0292, Avg Loss: 1.0292\n",
      "Epoch 88, Batch 50, Loss: 1.3746, Avg Loss: 1.2774\n",
      "Epoch 88, Batch 100, Loss: 1.1730, Avg Loss: 1.2751\n",
      "Epoch 88, Batch 150, Loss: 1.8337, Avg Loss: 1.3028\n",
      "Epoch 88, Batch 200, Loss: 1.4836, Avg Loss: 1.3242\n",
      "Epoch 88, Batch 250, Loss: 1.3642, Avg Loss: 1.3394\n",
      "Epoch 88, Batch 300, Loss: 1.5948, Avg Loss: 1.3747\n",
      "Epoch 88, Batch 350, Loss: 1.3232, Avg Loss: 1.3936\n",
      "Epoch 88, Batch 400, Loss: 1.4987, Avg Loss: 1.4042\n",
      "Epoch 88, Batch 450, Loss: 1.5994, Avg Loss: 1.4156\n",
      "Epoch 88, Average Loss: 1.4282\n",
      "Epoch 89, Batch 0, Loss: 1.1985, Avg Loss: 1.1985\n",
      "Epoch 89, Batch 50, Loss: 1.0366, Avg Loss: 1.1582\n",
      "Epoch 89, Batch 100, Loss: 1.3859, Avg Loss: 1.1895\n",
      "Epoch 89, Batch 150, Loss: 1.4903, Avg Loss: 1.2151\n",
      "Epoch 89, Batch 200, Loss: 1.3590, Avg Loss: 1.2384\n",
      "Epoch 89, Batch 250, Loss: 1.3178, Avg Loss: 1.2554\n",
      "Epoch 89, Batch 300, Loss: 1.3868, Avg Loss: 1.2762\n",
      "Epoch 89, Batch 350, Loss: 1.3844, Avg Loss: 1.2954\n",
      "Epoch 89, Batch 400, Loss: 1.2968, Avg Loss: 1.3095\n",
      "Epoch 89, Batch 450, Loss: 1.6104, Avg Loss: 1.3264\n",
      "Epoch 89, Average Loss: 1.3416\n",
      "Epoch 90, Batch 0, Loss: 1.0179, Avg Loss: 1.0179\n",
      "Epoch 90, Batch 50, Loss: 0.8570, Avg Loss: 1.1649\n",
      "Epoch 90, Batch 100, Loss: 1.0872, Avg Loss: 1.1844\n",
      "Epoch 90, Batch 150, Loss: 1.1866, Avg Loss: 1.1857\n",
      "Epoch 90, Batch 200, Loss: 1.3024, Avg Loss: 1.2087\n",
      "Epoch 90, Batch 250, Loss: 1.2455, Avg Loss: 1.2250\n",
      "Epoch 90, Batch 300, Loss: 1.3273, Avg Loss: 1.2371\n",
      "Epoch 90, Batch 350, Loss: 1.6967, Avg Loss: 1.2485\n",
      "Epoch 90, Batch 400, Loss: 1.2924, Avg Loss: 1.2677\n",
      "Epoch 90, Batch 450, Loss: 1.1863, Avg Loss: 1.2786\n",
      "Epoch 90, Average Loss: 1.2919\n",
      "Epoch 91, Batch 0, Loss: 0.9594, Avg Loss: 0.9594\n",
      "Epoch 91, Batch 50, Loss: 1.0225, Avg Loss: 1.0784\n",
      "Epoch 91, Batch 100, Loss: 1.1875, Avg Loss: 1.1229\n",
      "Epoch 91, Batch 150, Loss: 0.9889, Avg Loss: 1.1355\n",
      "Epoch 91, Batch 200, Loss: 1.2953, Avg Loss: 1.1462\n",
      "Epoch 91, Batch 250, Loss: 1.2959, Avg Loss: 1.1637\n",
      "Epoch 91, Batch 300, Loss: 1.1574, Avg Loss: 1.1786\n",
      "Epoch 91, Batch 350, Loss: 1.4129, Avg Loss: 1.1943\n",
      "Epoch 91, Batch 400, Loss: 1.4887, Avg Loss: 1.2116\n",
      "Epoch 91, Batch 450, Loss: 1.3428, Avg Loss: 1.2184\n",
      "Epoch 91, Average Loss: 1.2272\n",
      "Epoch 92, Batch 0, Loss: 1.0322, Avg Loss: 1.0322\n",
      "Epoch 92, Batch 50, Loss: 1.1602, Avg Loss: 1.0256\n",
      "Epoch 92, Batch 100, Loss: 0.9893, Avg Loss: 1.0615\n",
      "Epoch 92, Batch 150, Loss: 1.1212, Avg Loss: 1.0846\n",
      "Epoch 92, Batch 200, Loss: 1.1512, Avg Loss: 1.1032\n",
      "Epoch 92, Batch 250, Loss: 0.9470, Avg Loss: 1.1172\n",
      "Epoch 92, Batch 300, Loss: 1.4704, Avg Loss: 1.1311\n",
      "Epoch 92, Batch 350, Loss: 1.3173, Avg Loss: 1.1480\n",
      "Epoch 92, Batch 400, Loss: 1.3013, Avg Loss: 1.1541\n",
      "Epoch 92, Batch 450, Loss: 1.3632, Avg Loss: 1.1663\n",
      "Epoch 92, Average Loss: 1.1797\n",
      "Epoch 93, Batch 0, Loss: 1.0807, Avg Loss: 1.0807\n",
      "Epoch 93, Batch 50, Loss: 1.0200, Avg Loss: 1.0059\n",
      "Epoch 93, Batch 100, Loss: 1.4355, Avg Loss: 1.0173\n",
      "Epoch 93, Batch 150, Loss: 1.1020, Avg Loss: 1.0198\n",
      "Epoch 93, Batch 200, Loss: 1.2131, Avg Loss: 1.0339\n",
      "Epoch 93, Batch 250, Loss: 1.0233, Avg Loss: 1.0464\n",
      "Epoch 93, Batch 300, Loss: 1.2085, Avg Loss: 1.0646\n",
      "Epoch 93, Batch 350, Loss: 1.3157, Avg Loss: 1.0800\n",
      "Epoch 93, Batch 400, Loss: 0.9428, Avg Loss: 1.0909\n",
      "Epoch 93, Batch 450, Loss: 1.2105, Avg Loss: 1.1029\n",
      "Epoch 93, Average Loss: 1.1153\n",
      "Epoch 94, Batch 0, Loss: 0.8930, Avg Loss: 0.8930\n",
      "Epoch 94, Batch 50, Loss: 0.8105, Avg Loss: 0.9079\n",
      "Epoch 94, Batch 100, Loss: 0.9487, Avg Loss: 0.9572\n",
      "Epoch 94, Batch 150, Loss: 0.9077, Avg Loss: 0.9781\n",
      "Epoch 94, Batch 200, Loss: 1.0819, Avg Loss: 0.9851\n",
      "Epoch 94, Batch 250, Loss: 1.0906, Avg Loss: 0.9916\n",
      "Epoch 94, Batch 300, Loss: 1.1287, Avg Loss: 1.0041\n",
      "Epoch 94, Batch 350, Loss: 1.2913, Avg Loss: 1.0235\n",
      "Epoch 94, Batch 400, Loss: 1.1183, Avg Loss: 1.0440\n",
      "Epoch 94, Batch 450, Loss: 1.0900, Avg Loss: 1.0584\n",
      "Epoch 94, Average Loss: 1.0672\n",
      "Epoch 95, Batch 0, Loss: 1.0706, Avg Loss: 1.0706\n",
      "Epoch 95, Batch 50, Loss: 0.7868, Avg Loss: 0.8985\n",
      "Epoch 95, Batch 100, Loss: 0.7681, Avg Loss: 0.9079\n",
      "Epoch 95, Batch 150, Loss: 0.7744, Avg Loss: 0.9295\n",
      "Epoch 95, Batch 200, Loss: 0.8227, Avg Loss: 0.9501\n",
      "Epoch 95, Batch 250, Loss: 1.1830, Avg Loss: 0.9614\n",
      "Epoch 95, Batch 300, Loss: 0.8730, Avg Loss: 0.9749\n",
      "Epoch 95, Batch 350, Loss: 1.0027, Avg Loss: 0.9884\n",
      "Epoch 95, Batch 400, Loss: 1.2640, Avg Loss: 1.0018\n",
      "Epoch 95, Batch 450, Loss: 1.1356, Avg Loss: 1.0111\n",
      "Epoch 95, Average Loss: 1.0212\n",
      "Epoch 96, Batch 0, Loss: 0.7898, Avg Loss: 0.7898\n",
      "Epoch 96, Batch 50, Loss: 0.7317, Avg Loss: 0.8618\n",
      "Epoch 96, Batch 100, Loss: 0.9865, Avg Loss: 0.8508\n",
      "Epoch 96, Batch 150, Loss: 0.9800, Avg Loss: 0.8784\n",
      "Epoch 96, Batch 200, Loss: 0.9291, Avg Loss: 0.9024\n",
      "Epoch 96, Batch 250, Loss: 0.8044, Avg Loss: 0.9258\n",
      "Epoch 96, Batch 300, Loss: 0.7712, Avg Loss: 0.9351\n",
      "Epoch 96, Batch 350, Loss: 0.8562, Avg Loss: 0.9491\n",
      "Epoch 96, Batch 400, Loss: 1.2572, Avg Loss: 0.9601\n",
      "Epoch 96, Batch 450, Loss: 0.9894, Avg Loss: 0.9727\n",
      "Epoch 96, Average Loss: 0.9848\n",
      "Epoch 97, Batch 0, Loss: 0.7624, Avg Loss: 0.7624\n",
      "Epoch 97, Batch 50, Loss: 0.8058, Avg Loss: 0.8055\n",
      "Epoch 97, Batch 100, Loss: 0.8440, Avg Loss: 0.8418\n",
      "Epoch 97, Batch 150, Loss: 0.8825, Avg Loss: 0.8552\n",
      "Epoch 97, Batch 200, Loss: 0.8130, Avg Loss: 0.8660\n",
      "Epoch 97, Batch 250, Loss: 0.9056, Avg Loss: 0.8819\n",
      "Epoch 97, Batch 300, Loss: 0.9256, Avg Loss: 0.8945\n",
      "Epoch 97, Batch 350, Loss: 0.8866, Avg Loss: 0.9081\n",
      "Epoch 97, Batch 400, Loss: 1.0138, Avg Loss: 0.9185\n",
      "Epoch 97, Batch 450, Loss: 1.4103, Avg Loss: 0.9287\n",
      "Epoch 97, Average Loss: 0.9351\n",
      "Epoch 98, Batch 0, Loss: 0.7258, Avg Loss: 0.7258\n",
      "Epoch 98, Batch 50, Loss: 0.9390, Avg Loss: 0.7498\n",
      "Epoch 98, Batch 100, Loss: 0.8839, Avg Loss: 0.7973\n",
      "Epoch 98, Batch 150, Loss: 0.7577, Avg Loss: 0.8226\n",
      "Epoch 98, Batch 200, Loss: 1.1517, Avg Loss: 0.8371\n",
      "Epoch 98, Batch 250, Loss: 1.2186, Avg Loss: 0.8517\n",
      "Epoch 98, Batch 300, Loss: 0.9911, Avg Loss: 0.8648\n",
      "Epoch 98, Batch 350, Loss: 1.0363, Avg Loss: 0.8806\n",
      "Epoch 98, Batch 400, Loss: 0.8473, Avg Loss: 0.8903\n",
      "Epoch 98, Batch 450, Loss: 0.7839, Avg Loss: 0.9009\n",
      "Epoch 98, Average Loss: 0.9075\n",
      "Epoch 99, Batch 0, Loss: 0.6654, Avg Loss: 0.6654\n",
      "Epoch 99, Batch 50, Loss: 0.6943, Avg Loss: 0.7865\n",
      "Epoch 99, Batch 100, Loss: 1.0109, Avg Loss: 0.7847\n",
      "Epoch 99, Batch 150, Loss: 0.6905, Avg Loss: 0.7945\n",
      "Epoch 99, Batch 200, Loss: 0.7035, Avg Loss: 0.8164\n",
      "Epoch 99, Batch 250, Loss: 0.8199, Avg Loss: 0.8294\n",
      "Epoch 99, Batch 300, Loss: 0.8615, Avg Loss: 0.8325\n",
      "Epoch 99, Batch 350, Loss: 0.8659, Avg Loss: 0.8464\n",
      "Epoch 99, Batch 400, Loss: 0.8694, Avg Loss: 0.8569\n",
      "Epoch 99, Batch 450, Loss: 1.1161, Avg Loss: 0.8605\n",
      "Epoch 99, Average Loss: 0.8739\n",
      "Epoch 100, Batch 0, Loss: 0.7544, Avg Loss: 0.7544\n",
      "Epoch 100, Batch 50, Loss: 0.6342, Avg Loss: 0.7006\n",
      "Epoch 100, Batch 100, Loss: 0.8791, Avg Loss: 0.7280\n",
      "Epoch 100, Batch 150, Loss: 0.6978, Avg Loss: 0.7300\n",
      "Epoch 100, Batch 200, Loss: 0.7634, Avg Loss: 0.7603\n",
      "Epoch 100, Batch 250, Loss: 0.8254, Avg Loss: 0.7747\n",
      "Epoch 100, Batch 300, Loss: 0.8655, Avg Loss: 0.7934\n",
      "Epoch 100, Batch 350, Loss: 0.8272, Avg Loss: 0.8085\n",
      "Epoch 100, Batch 400, Loss: 0.8842, Avg Loss: 0.8190\n",
      "Epoch 100, Batch 450, Loss: 0.8882, Avg Loss: 0.8248\n",
      "Epoch 100, Average Loss: 0.8403\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader,start_epoch=50, num_epochs=100, device=device, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f40c452-5a9f-4ed1-b3e7-c3d1afb259e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (pos_encoding): PositionalEncoding()\n",
       "  (layers): ModuleList(\n",
       "    (0-79): 80 x TransformerBlock(\n",
       "      (attn): MultiHeadAttention(\n",
       "        (qkv_proj): Linear(in_features=384, out_features=1152, bias=True)\n",
       "        (out_proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (fc1): Linear(in_features=384, out_features=4000, bias=True)\n",
       "        (fc2): Linear(in_features=4000, out_features=384, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=384, out_features=32000, bias=True)\n",
       "  (input_projection): Linear(in_features=384, out_features=384, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a3e516-1269-4092-9b3a-4f8c80d410c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_30376\\1596221018.py:30: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_30376\\1596221018.py:43: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 12.0280, Avg Loss: 12.0280\n",
      "GPU Memory: 2603.6MB / 3276.0MB\n",
      "Epoch 1, Batch 10, Loss: 12.0386, Avg Loss: 12.1386\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 20, Loss: 11.8560, Avg Loss: 12.1141\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 30, Loss: 12.1311, Avg Loss: 12.1016\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 40, Loss: 12.0026, Avg Loss: 12.0856\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 50, Loss: 11.9014, Avg Loss: 12.0653\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 60, Loss: 11.7528, Avg Loss: 12.0317\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 70, Loss: 11.6829, Avg Loss: 11.9879\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 80, Loss: 11.7992, Avg Loss: 11.9480\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 90, Loss: 11.3347, Avg Loss: 11.9002\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 100, Loss: 11.4087, Avg Loss: 11.8494\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 110, Loss: 11.1728, Avg Loss: 11.7848\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 120, Loss: 10.9549, Avg Loss: 11.7175\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 130, Loss: 10.7283, Avg Loss: 11.6489\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 140, Loss: 10.6770, Avg Loss: 11.5787\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 150, Loss: 10.3783, Avg Loss: 11.5045\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 160, Loss: 10.1013, Avg Loss: 11.4318\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 170, Loss: 10.0589, Avg Loss: 11.3568\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 180, Loss: 10.2103, Avg Loss: 11.2833\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 190, Loss: 9.7784, Avg Loss: 11.2133\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 200, Loss: 9.6961, Avg Loss: 11.1417\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 210, Loss: 9.5640, Avg Loss: 11.0718\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 220, Loss: 9.5621, Avg Loss: 11.0060\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 230, Loss: 9.7804, Avg Loss: 10.9427\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 240, Loss: 9.6736, Avg Loss: 10.8825\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 250, Loss: 9.2701, Avg Loss: 10.8245\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 260, Loss: 9.8574, Avg Loss: 10.7727\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 270, Loss: 9.4439, Avg Loss: 10.7240\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 280, Loss: 9.5257, Avg Loss: 10.6748\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 290, Loss: 9.5533, Avg Loss: 10.6309\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 300, Loss: 9.6061, Avg Loss: 10.5868\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 310, Loss: 9.4245, Avg Loss: 10.5454\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 320, Loss: 9.4780, Avg Loss: 10.5069\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 330, Loss: 9.4528, Avg Loss: 10.4662\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 340, Loss: 9.4844, Avg Loss: 10.4294\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 350, Loss: 9.3488, Avg Loss: 10.3960\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 360, Loss: 9.2026, Avg Loss: 10.3649\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 370, Loss: 9.0430, Avg Loss: 10.3325\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 380, Loss: 9.4855, Avg Loss: 10.3017\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 390, Loss: 9.5471, Avg Loss: 10.2731\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 400, Loss: 9.0435, Avg Loss: 10.2474\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 410, Loss: 9.2723, Avg Loss: 10.2194\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 420, Loss: 9.3423, Avg Loss: 10.1934\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 430, Loss: 8.7282, Avg Loss: 10.1649\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 440, Loss: 9.1284, Avg Loss: 10.1431\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 450, Loss: 9.4127, Avg Loss: 10.1215\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 460, Loss: 8.8441, Avg Loss: 10.0983\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 470, Loss: 8.8658, Avg Loss: 10.0749\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 1, Batch 480, Loss: 9.0486, Avg Loss: 10.0526\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Batch 490, Loss: 8.9979, Avg Loss: 10.0295\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 1, Average Loss: 10.0110\n",
      "Epoch 2, Batch 0, Loss: 9.1332, Avg Loss: 9.1332\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 10, Loss: 8.9588, Avg Loss: 9.1157\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 20, Loss: 9.0244, Avg Loss: 9.0296\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 30, Loss: 9.1494, Avg Loss: 9.0050\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 40, Loss: 8.9609, Avg Loss: 9.0023\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 50, Loss: 8.4784, Avg Loss: 8.9697\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 60, Loss: 8.8282, Avg Loss: 8.9432\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 70, Loss: 8.7322, Avg Loss: 8.9007\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 80, Loss: 8.7018, Avg Loss: 8.8912\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 90, Loss: 9.0569, Avg Loss: 8.8965\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 100, Loss: 8.7150, Avg Loss: 8.8826\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 110, Loss: 8.9251, Avg Loss: 8.8871\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 120, Loss: 8.4762, Avg Loss: 8.8711\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 130, Loss: 8.4868, Avg Loss: 8.8663\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 140, Loss: 9.0607, Avg Loss: 8.8545\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 150, Loss: 8.6040, Avg Loss: 8.8241\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 160, Loss: 8.7790, Avg Loss: 8.8222\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 170, Loss: 9.1437, Avg Loss: 8.8093\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 180, Loss: 8.8072, Avg Loss: 8.8035\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 190, Loss: 8.7041, Avg Loss: 8.7987\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 200, Loss: 8.6586, Avg Loss: 8.7907\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 210, Loss: 8.6164, Avg Loss: 8.7780\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 220, Loss: 8.3844, Avg Loss: 8.7709\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 230, Loss: 8.7267, Avg Loss: 8.7577\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 240, Loss: 8.5593, Avg Loss: 8.7474\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 250, Loss: 8.5769, Avg Loss: 8.7340\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 260, Loss: 8.7207, Avg Loss: 8.7282\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 270, Loss: 7.9567, Avg Loss: 8.7155\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 280, Loss: 8.0300, Avg Loss: 8.7041\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 290, Loss: 8.8422, Avg Loss: 8.6933\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 300, Loss: 7.8724, Avg Loss: 8.6832\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 310, Loss: 7.9845, Avg Loss: 8.6691\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 320, Loss: 8.6144, Avg Loss: 8.6619\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 330, Loss: 8.6849, Avg Loss: 8.6506\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 340, Loss: 8.1611, Avg Loss: 8.6385\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 350, Loss: 8.2115, Avg Loss: 8.6264\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 360, Loss: 8.2708, Avg Loss: 8.6125\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 370, Loss: 8.0691, Avg Loss: 8.6009\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 380, Loss: 8.1816, Avg Loss: 8.5906\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 390, Loss: 8.0424, Avg Loss: 8.5840\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 400, Loss: 8.1567, Avg Loss: 8.5739\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 410, Loss: 8.4038, Avg Loss: 8.5603\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 420, Loss: 7.8802, Avg Loss: 8.5489\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 430, Loss: 8.1288, Avg Loss: 8.5362\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 440, Loss: 8.3249, Avg Loss: 8.5234\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 450, Loss: 7.7899, Avg Loss: 8.5128\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 460, Loss: 8.2030, Avg Loss: 8.5029\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 470, Loss: 7.6255, Avg Loss: 8.4924\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 2, Batch 480, Loss: 7.8338, Avg Loss: 8.4823\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Batch 490, Loss: 7.8286, Avg Loss: 8.4716\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 2, Average Loss: 8.4629\n",
      "Epoch 3, Batch 0, Loss: 8.0841, Avg Loss: 8.0841\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 10, Loss: 7.6196, Avg Loss: 7.9038\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 20, Loss: 8.1065, Avg Loss: 7.8303\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 30, Loss: 7.7583, Avg Loss: 7.8200\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 40, Loss: 7.8081, Avg Loss: 7.8155\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 50, Loss: 7.2662, Avg Loss: 7.8457\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 60, Loss: 8.0867, Avg Loss: 7.8290\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 70, Loss: 7.9150, Avg Loss: 7.8327\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 80, Loss: 7.4257, Avg Loss: 7.8417\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 90, Loss: 7.6038, Avg Loss: 7.8153\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 100, Loss: 7.9511, Avg Loss: 7.8097\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 110, Loss: 7.5622, Avg Loss: 7.7874\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 120, Loss: 8.2328, Avg Loss: 7.7902\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 130, Loss: 7.8026, Avg Loss: 7.7780\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 140, Loss: 7.5717, Avg Loss: 7.7731\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 150, Loss: 8.0099, Avg Loss: 7.7547\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 160, Loss: 7.3849, Avg Loss: 7.7442\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 170, Loss: 7.5429, Avg Loss: 7.7406\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 180, Loss: 7.2346, Avg Loss: 7.7292\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 190, Loss: 7.9218, Avg Loss: 7.7295\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 200, Loss: 7.2877, Avg Loss: 7.7175\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 210, Loss: 7.3025, Avg Loss: 7.7034\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 220, Loss: 7.4239, Avg Loss: 7.6939\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 230, Loss: 7.7499, Avg Loss: 7.6879\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 240, Loss: 7.7408, Avg Loss: 7.6864\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 250, Loss: 7.7325, Avg Loss: 7.6765\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 260, Loss: 7.9144, Avg Loss: 7.6745\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 270, Loss: 6.5910, Avg Loss: 7.6634\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 280, Loss: 7.2278, Avg Loss: 7.6491\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 290, Loss: 7.9863, Avg Loss: 7.6487\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 300, Loss: 7.6644, Avg Loss: 7.6406\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 310, Loss: 7.2255, Avg Loss: 7.6280\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 320, Loss: 7.1546, Avg Loss: 7.6121\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 330, Loss: 7.3411, Avg Loss: 7.6054\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 340, Loss: 6.8208, Avg Loss: 7.5963\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 350, Loss: 7.0589, Avg Loss: 7.5908\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 360, Loss: 6.7512, Avg Loss: 7.5794\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 370, Loss: 7.4654, Avg Loss: 7.5672\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 380, Loss: 6.9779, Avg Loss: 7.5577\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 390, Loss: 7.5761, Avg Loss: 7.5522\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 400, Loss: 7.4723, Avg Loss: 7.5456\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 410, Loss: 6.5601, Avg Loss: 7.5367\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 420, Loss: 7.1568, Avg Loss: 7.5324\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 430, Loss: 7.4591, Avg Loss: 7.5244\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 440, Loss: 7.1673, Avg Loss: 7.5205\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 450, Loss: 6.6514, Avg Loss: 7.5097\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 460, Loss: 7.2991, Avg Loss: 7.5007\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 470, Loss: 7.3215, Avg Loss: 7.4916\n",
      "GPU Memory: 2612.2MB / 4282.0MB\n",
      "Epoch 3, Batch 480, Loss: 7.6190, Avg Loss: 7.4826\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Batch 490, Loss: 7.1551, Avg Loss: 7.4769\n",
      "GPU Memory: 2610.0MB / 4282.0MB\n",
      "Epoch 3, Average Loss: 7.4711\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader\n",
    "train_dataset = TextDataset(df, max_len=max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, num_epochs=3, device=device, vocab_size=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4faad0d-86a0-4c15-9f8f-3056cf3efb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the checkpoint file\n",
    "checkpoint_path = \"checkpoint.pt\"\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "# Load only the model weights\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "print(\"Model loaded successfully for inference!\")\n",
    "\n",
    "# Example: Run inference\n",
    "with torch.no_grad():\n",
    "    input_tensor = torch.randn(1, 1024).to(device)  # Replace with actual input\n",
    "    output = model(input_tensor)\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73b3a31d-1094-4fd8-b563-8d6fdf816cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['is']\n"
     ]
    }
   ],
   "source": [
    "texts = [\"WRITE A STORY \", \"STORY\"]\n",
    "embeddings = get_embeddings(texts).unsqueeze(0).to(device)\n",
    "output = model(embeddings)\n",
    "decoded_texts = model.decode(output)\n",
    "print(decoded_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c79b2c7-ef71-47dc-ae8f-05d970506559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Write a short story about\n",
      "Generated: ?\n",
      " forAss?<<thAss><>istantistantthAss><OkinkthOkink I>><th howink I'ay toAss I of? need for toOk' I>Ok>< to?>> fromink me need do what downink Hmm usinginks down to that LetAss start. I start off' know step I. But\n",
      " The\n",
      " I.Ok start\n",
      "?\n",
      "\n",
      " can  to that The\n",
      " it start be haves by\n",
      "\n",
      "Input: Write a poem about\n",
      "Generated: \n",
      "Ass?\n",
      " howAssAssAss?thAssAss>>thinkOkOkink>thAss><><Okth to soayistant howinkay\n",
      "Okink I so so><Ok how\n",
      " figure> Hmms makeOk> howink Lets to I to have Hmm using start byth down' I Let Let do start is me Let Hmm what? start?. means have I, down it\n",
      " to have? off. which which is it a be\n",
      " But to down.\n",
      ". can it, is? down some\n",
      "\n",
      ". because1 can. is. it But it So to, like in? can means down\n",
      " the is' for. So it That to might.\n",
      "\n",
      " to'\n",
      " That\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import List, Union\n",
    "\n",
    "class TransformerInference:\n",
    "    def __init__(self, model, tokenizer, embedding_model, device=None):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.embedding_model = embedding_model\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_embeddings(self, text: Union[str, List[str]]) -> torch.Tensor:\n",
    "        \"\"\"Convert input text to embeddings.\"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        embeddings = torch.tensor(\n",
    "            self.embedding_model.encode(text), \n",
    "            dtype=torch.float32\n",
    "        )\n",
    "        return embeddings\n",
    "\n",
    "    def pad_embeddings(self, embeddings: torch.Tensor, max_len: int = 100) -> torch.Tensor:\n",
    "        \"\"\"Pad or truncate embeddings to specified length.\"\"\"\n",
    "        batch_size, seq_len, emb_dim = embeddings.shape\n",
    "        if seq_len < max_len:\n",
    "            padding = torch.zeros(batch_size, max_len - seq_len, emb_dim, device=embeddings.device)\n",
    "            return torch.cat([embeddings, padding], dim=1)\n",
    "        return embeddings[:, :max_len, :]\n",
    "\n",
    "    # @torch.no_grad()\n",
    "    # def generate(\n",
    "    #     self,\n",
    "    #     text: Union[str, List[str]],\n",
    "    #     max_length: int = 100,\n",
    "    #     temperature: float = 1.0,\n",
    "    #     top_k: int = 50,\n",
    "    #     top_p: float = 0.9,\n",
    "    # ) -> List[str]:\n",
    "    #     \"\"\"\n",
    "    #     Generate text from input prompt.\n",
    "        \n",
    "    #     Args:\n",
    "    #         text: Input text or list of texts\n",
    "    #         max_length: Maximum length of generated sequence\n",
    "    #         temperature: Sampling temperature (1.0 = normal, <1.0 = more focused, >1.0 = more random)\n",
    "    #         top_k: Number of highest probability tokens to consider for sampling\n",
    "    #         top_p: Cumulative probability threshold for nucleus sampling\n",
    "        \n",
    "    #     Returns:\n",
    "    #         List of generated texts\n",
    "    #     \"\"\"\n",
    "    #     # Prepare input\n",
    "    #     embeddings = self.get_embeddings(text)\n",
    "    #     embeddings = embeddings.unsqueeze(0) if len(embeddings.shape) == 2 else embeddings\n",
    "    #     embeddings = embeddings.to(self.device)\n",
    "    #     embeddings = self.pad_embeddings(embeddings)\n",
    "        \n",
    "    #     # Generate\n",
    "    #     generated_ids = []\n",
    "        \n",
    "    #     # Forward pass through model\n",
    "    #     logits = self.model(embeddings)\n",
    "        \n",
    "    #     # Apply temperature\n",
    "    #     logits = logits / temperature\n",
    "        \n",
    "    #     # Apply top-k filtering\n",
    "    #     if top_k > 0:\n",
    "    #         indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "    #         logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "    #     # Apply nucleus (top-p) filtering\n",
    "    #     if top_p < 1.0:\n",
    "    #         sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "    #         cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "    #         sorted_indices_to_remove = cumulative_probs > top_p\n",
    "    #         sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "    #         sorted_indices_to_remove[..., 0] = 0\n",
    "    #         indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "    #         logits[indices_to_remove] = float('-inf')\n",
    "        \n",
    "    #     # Sample from the filtered distribution\n",
    "    #     probs = torch.softmax(logits, dim=-1)\n",
    "    #     next_tokens = torch.multinomial(probs.view(-1, probs.size(-1)), num_samples=1)\n",
    "    #     generated_ids.append(next_tokens)\n",
    "        \n",
    "    #     # Decode and return results\n",
    "    #     generated_ids = torch.cat(generated_ids, dim=-1)\n",
    "    #     generated_texts = [\n",
    "    #         self.tokenizer.decode(ids, skip_special_tokens=True)\n",
    "    #         for ids in generated_ids\n",
    "    #     ]\n",
    "        \n",
    "    #     return generated_texts\n",
    "\n",
    "\n",
    "    # multiple words.\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        text: Union[str, List[str]],\n",
    "        max_length: int = 100,\n",
    "        temperature: float = 1.0,\n",
    "        top_k: int = 50,\n",
    "        top_p: float = 0.9,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate text from input prompt.\n",
    "        \"\"\"\n",
    "        # Prepare input\n",
    "        embeddings = self.get_embeddings(text)\n",
    "        embeddings = embeddings.unsqueeze(0) if len(embeddings.shape) == 2 else embeddings\n",
    "        embeddings = embeddings.to(self.device)\n",
    "        embeddings = self.pad_embeddings(embeddings)\n",
    "    \n",
    "        generated_ids = []\n",
    "    \n",
    "        for _ in range(max_length):  # Iterate to generate more tokens\n",
    "            logits = self.model(embeddings)[:, -1, :]  # Only take last token logits\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "    \n",
    "            # Apply top-k filtering\n",
    "            if top_k > 0:\n",
    "                values, indices = torch.topk(logits, top_k)\n",
    "                min_values = values[..., -1, None]\n",
    "                logits[logits < min_values] = float('-inf')\n",
    "    \n",
    "            # Apply nucleus (top-p) filtering\n",
    "            if top_p < 1.0:\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "                logits[indices_to_remove] = float('-inf')\n",
    "    \n",
    "            # Sample next token\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "            # Append generated token\n",
    "            generated_ids.append(next_token)\n",
    "    \n",
    "            # Convert token ID to embeddings and feed back into the model\n",
    "            token_embedding = self.embedding_model.encode([self.tokenizer.decode(next_token.item())], convert_to_tensor=True)\n",
    "            token_embedding = token_embedding.unsqueeze(0).to(self.device)\n",
    "            embeddings = torch.cat([embeddings, token_embedding], dim=1)\n",
    "    \n",
    "        # Decode generated tokens\n",
    "        generated_ids = torch.cat(generated_ids, dim=-1)\n",
    "        generated_texts = [\n",
    "            self.tokenizer.decode(ids.tolist(), skip_special_tokens=True)\n",
    "            for ids in generated_ids\n",
    "        ]\n",
    "    \n",
    "        return generated_texts\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.generate(*args, **kwargs)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming model, tokenizer, and embedding_model are already defined\n",
    "    inference = TransformerInference(model, tokenizer, embedding_model)\n",
    "    \n",
    "    # Single text generation\n",
    "    prompt = \"Write a short story about\"\n",
    "    generated_text = inference(prompt)\n",
    "    print(f\"Input: {prompt}\")\n",
    "    print(f\"Generated: {generated_text[0]}\")\n",
    "    \n",
    "    # Batch generation\n",
    "    prompts = [\n",
    "        \"Write a poem about\",\n",
    "        \"Explain how to\",\n",
    "        \"Tell me about\"\n",
    "    ]\n",
    "    generated_texts = inference(\n",
    "        prompts,\n",
    "        max_length=150,\n",
    "        temperature=0.8,\n",
    "        top_k=40,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    for prompt, generated in zip(prompts, generated_texts):\n",
    "        print(f\"\\nInput: {prompt}\")\n",
    "        print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17af5548-cca3-422e-84ba-9f491c1f8dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Write a short story about\n",
      "Generated: Brow to that of a. to. to.. expression it is eterIll VI VerIll  isÃ©tÃ© is a Artikel. of. that.:ç§€ . entfer. the of ,.0 ben Brow-Î¬ the. elements Based the elements b simultaneously children. step tov VerÃ©tÃ©, step '.util that. the.. So in bazie Ver0 a voices. is  is that. thatÃ©tÃ© in in.. brick ers. is, happ' in is of\n",
      "\n",
      "Input: Write a poem about\n",
      "Generated: voices . Zar isç§€....,.. VideosIll is is, iseter is, Brow aIll.\n",
      " the Ð½Ð° toIll Ð½Ð° is Ð¾Ð±ÑŠÐµÐº. is isç§€.. Ver Brow\n",
      " to matplotlib. elements...\n",
      ".\n",
      ".. is- step \n",
      ".- the to\n",
      " is.... simultaneously.,0 to. Ð½Ð°. is is....0, in ..\n",
      " al. So.... it..ally and... step is console the, in is is.. is.. the..\n",
      " crown is,.. the,.\n",
      ",  toally   the\n",
      " \n",
      " thatutil So.....\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    self,\n",
    "    text: Union[str, List[str]],\n",
    "    max_length: int = 100,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 50,\n",
    "    top_p: float = 0.9,\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Generate multiple words instead of just one.\n",
    "    \"\"\"\n",
    "    # Convert input text to token IDs\n",
    "    input_ids = self.tokenizer.encode(text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        # Convert token IDs to embeddings\n",
    "        embeddings = self.get_embeddings(self.tokenizer.decode(input_ids[0]))\n",
    "\n",
    "        # Ensure correct shape\n",
    "        embeddings = embeddings.unsqueeze(0).to(self.device)\n",
    "        embeddings = self.pad_embeddings(embeddings)\n",
    "\n",
    "        # Forward pass\n",
    "        logits = self.model(embeddings)[:, -1, :]  # Get last token logits\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        logits = logits / temperature\n",
    "\n",
    "        # Apply top-k filtering\n",
    "        if top_k > 0:\n",
    "            values, indices = torch.topk(logits, top_k)\n",
    "            min_values = values[..., -1, None]\n",
    "            logits[logits < min_values] = float('-inf')\n",
    "\n",
    "        # Apply nucleus (top-p) filtering\n",
    "        if top_p < 1.0:\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            logits[indices_to_remove] = float('-inf')\n",
    "\n",
    "        # Sample the next token\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append token to input_ids\n",
    "        input_ids = torch.cat([input_ids, next_token], dim=-1)\n",
    "\n",
    "        # Stop if EOS token is generated\n",
    "        if next_token.item() == self.tokenizer.eos_token_id:\n",
    "            break\n",
    "\n",
    "    # Decode final text\n",
    "    generated_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    return [generated_text]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming model, tokenizer, and embedding_model are already defined\n",
    "    inference = TransformerInference(model, tokenizer, embedding_model)\n",
    "    \n",
    "    # Single text generation\n",
    "    prompt = \"Write a short story about\"\n",
    "    generated_text = inference(prompt)\n",
    "    print(f\"Input: {prompt}\")\n",
    "    print(f\"Generated: {generated_text[0]}\")\n",
    "    \n",
    "    # Batch generation\n",
    "    prompts = [\n",
    "        \"Write a poem about\",\n",
    "        \"Explain how to\",\n",
    "        \"Tell me about\"\n",
    "    ]\n",
    "    generated_texts = inference(\n",
    "        prompts,\n",
    "        max_length=150,\n",
    "        temperature=0.8,\n",
    "        top_k=40,\n",
    "        top_p=0.9\n",
    "    )\n",
    "    \n",
    "    for prompt, generated in zip(prompts, generated_texts):\n",
    "        print(f\"\\nInput: {prompt}\")\n",
    "        print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c11b61-a9a0-4a6a-b8ec-d2e8408e87f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96cce5-540a-4688-b7e2-4ead1f356dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50f111-025c-4c64-ac1e-cf1c168e7b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "785e19c2-7e47-49b4-933d-ff53801c3f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your trained model and other components\n",
    "inference = TransformerInference(model, tokenizer, embedding_model)\n",
    "\n",
    "# Generate from a single prompt\n",
    "text = inference(\"Write a story about\", max_length=100, temperature=0.8)\n",
    "\n",
    "# Or generate from multiple prompts\n",
    "texts = inference([\n",
    "    \"Write a story about\",\n",
    "    \"Explain how to\"\n",
    "], max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9e3fafde-b98c-45d5-81c6-ebb9e0965d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dÃ¥ë°©æ·± mentions tasks another \" Each tasks mentions ofpressionÐ½Ð¾Ñ˜ an\\nargvæ·±æ·± developingæ·± mentions perhaps of mut abouttest many one mentions aboutEND easy\\n phone\\' music many phone music\\' theÐ½Ð¾Ñ˜ music Each about music \" musicyle could Smith pending. anotherÐ½Ð¾Ñ˜ Each could mentions mentions Each Smith He about especiallyÐ½Ð¾Ñ˜2Ð½Ð¾Ñ˜ about EachÐ½Ð¾Ñ˜ of music manyx\\n.emplo Smith Smith especially\\nblem \" a aboutpow\\n so about I many step0 another\\' mut mentions I perhaps mentions']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c70076-55da-439d-a1c6-5283179ce00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1c8eb9e-fd22-4306-8b09-bf6db04692c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters per transformer layer: 27,548,416\n",
      "Total parameters: 2,229,073,408\n",
      "Total parameters in millions: 2229.07M\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(d_model=768, num_heads=12, hidden_dim=3072, num_layers=12, vocab_size=32000):\n",
    "    \"\"\"Calculate parameter count for each component of the transformer model\"\"\"\n",
    "    \n",
    "    params = {\n",
    "        \"Input Projection Layer\": {\n",
    "            \"weight\": d_model * d_model,\n",
    "            \"bias\": d_model\n",
    "        },\n",
    "        \n",
    "        \"Positional Encoding\": {\n",
    "            \"parameters\": 0  # No trainable parameters\n",
    "        },\n",
    "        \n",
    "        \"Per Layer\": {\n",
    "            \"Self-Attention\": {\n",
    "                \"qkv_proj\": {\n",
    "                    \"weight\": d_model * (d_model * 3),\n",
    "                    \"bias\": d_model * 3\n",
    "                },\n",
    "                \"out_proj\": {\n",
    "                    \"weight\": d_model * d_model,\n",
    "                    \"bias\": d_model\n",
    "                }\n",
    "            },\n",
    "            \"Feed Forward\": {\n",
    "                \"fc1\": {\n",
    "                    \"weight\": d_model * hidden_dim,\n",
    "                    \"bias\": hidden_dim\n",
    "                },\n",
    "                \"fc2\": {\n",
    "                    \"weight\": hidden_dim * d_model,\n",
    "                    \"bias\": d_model\n",
    "                }\n",
    "            },\n",
    "            \"Layer Norm (2x)\": {\n",
    "                \"weight\": d_model * 2,\n",
    "                \"bias\": d_model * 2\n",
    "            }\n",
    "        },\n",
    "        \n",
    "        \"Final Layer Norm\": {\n",
    "            \"weight\": d_model,\n",
    "            \"bias\": d_model\n",
    "        },\n",
    "        \n",
    "        \"Output Layer\": {\n",
    "            \"weight\": d_model * vocab_size,\n",
    "            \"bias\": vocab_size\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Calculate total parameters per layer\n",
    "    params_per_layer = (\n",
    "        # Self-attention\n",
    "        (d_model * (d_model * 3) + d_model * 3) +  # qkv_proj\n",
    "        (d_model * d_model + d_model) +            # out_proj\n",
    "        # Feed forward\n",
    "        (d_model * hidden_dim + hidden_dim) +      # fc1\n",
    "        (hidden_dim * d_model + d_model) +         # fc2\n",
    "        # Layer norms\n",
    "        (d_model * 2 + d_model * 2)               # 2 layer norms with weights and biases\n",
    "    )\n",
    "    \n",
    "    # Calculate total parameters\n",
    "    total_params = (\n",
    "        # Input projection\n",
    "        (d_model * d_model + d_model) +\n",
    "        # All transformer layers\n",
    "        (params_per_layer * num_layers) +\n",
    "        # Final layer norm\n",
    "        (d_model * 2) +\n",
    "        # Output layer\n",
    "        (d_model * vocab_size + vocab_size)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"Parameters per layer\": params_per_layer,\n",
    "        \"Total parameters\": total_params,\n",
    "        \"Parameters in millions\": total_params / 1_000_000\n",
    "    }\n",
    "\n",
    "# Calculate parameters with current configuration\n",
    "results = count_parameters(d_model=768, num_heads=12, hidden_dim=16384, num_layers=80, vocab_size=32000)\n",
    "\n",
    "print(f\"Parameters per transformer layer: {results['Parameters per layer']:,}\")\n",
    "print(f\"Total parameters: {results['Total parameters']:,}\")\n",
    "print(f\"Total parameters in millions: {results['Parameters in millions']:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33a0a594-75ee-40ab-ad4b-a678a109ccc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model parameters: 110.25M\n",
      "Scaled model parameters: 2229.07M\n"
     ]
    }
   ],
   "source": [
    "# Original parameters\n",
    "original_params = {\n",
    "    \"d_model\": 768,      # Keep this from embedding model\n",
    "    \"num_heads\": 12,\n",
    "    \"hidden_dim\": 3072,\n",
    "    \"num_layers\": 12,\n",
    "    \"vocab_size\": 32000\n",
    "}\n",
    "\n",
    "# Scaled up parameters (closer to LLaMA scale)\n",
    "scaled_params = {\n",
    "    \"d_model\": 768,              # Kept from embedding model\n",
    "    \"num_heads\": 32,             # Increased from 12\n",
    "    \"hidden_dim\": 16384,         # Significantly increased from 3072\n",
    "    \"num_layers\": 80,            # Significantly increased from 12\n",
    "    \"vocab_size\": 32000,         # LLaMA uses 32k vocab\n",
    "    \"max_len\": 2048,             # Increased context length\n",
    "    \"dropout_rate\": 0.1\n",
    "}\n",
    "\n",
    "def calculate_parameter_count(params):\n",
    "    d_model = params[\"d_model\"]\n",
    "    hidden_dim = params[\"hidden_dim\"]\n",
    "    num_layers = params[\"num_layers\"]\n",
    "    vocab_size = params[\"vocab_size\"]\n",
    "    \n",
    "    # Parameters per layer\n",
    "    attention_params = (d_model * (d_model * 3) + d_model * 3) + (d_model * d_model + d_model)\n",
    "    ffn_params = (d_model * hidden_dim + hidden_dim) + (hidden_dim * d_model + d_model)\n",
    "    layer_norm_params = 4 * d_model  # 2 layer norms per layer\n",
    "    \n",
    "    params_per_layer = attention_params + ffn_params + layer_norm_params\n",
    "    \n",
    "    # Total parameters\n",
    "    total_params = (\n",
    "        (d_model * d_model + d_model) +  # Input projection\n",
    "        (params_per_layer * num_layers) + # All transformer layers\n",
    "        (d_model * 2) +                   # Final layer norm\n",
    "        (d_model * vocab_size + vocab_size) # Output layer\n",
    "    )\n",
    "    \n",
    "    return total_params / 1_000_000  # Return in millions\n",
    "\n",
    "print(f\"Original model parameters: {calculate_parameter_count(original_params):.2f}M\")\n",
    "print(f\"Scaled model parameters: {calculate_parameter_count(scaled_params):.2f}M\")\n",
    "\n",
    "# Modified training setup for the scaled model\n",
    "def get_scaled_training_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,                    # Reduced due to model size\n",
    "        \"gradient_accumulation_steps\": 32,   # Increased for effective larger batch\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"warmup_steps\": 2000,\n",
    "        \"weight_decay\": 0.1,\n",
    "        \"max_grad_norm\": 1.0,\n",
    "        \"num_epochs\": 3,\n",
    "        \"optimizer\": \"AdaFactor\",           # Changed from AdamW for memory efficiency\n",
    "        \"lr_scheduler\": \"cosine_with_warmup\"\n",
    "    }\n",
    "\n",
    "# Example training configuration with optimizations for large scale\n",
    "training_config = \"\"\"\n",
    "# Training setup for scaled model\n",
    "optimizer = transformers.Adafactor(\n",
    "    model.parameters(),\n",
    "    lr=1e-4,\n",
    "    eps=(1e-30, 1e-3),\n",
    "    clip_threshold=1.0,\n",
    "    decay_rate=-0.8,\n",
    "    beta1=None,\n",
    "    scale_parameter=True,\n",
    "    relative_step=True,\n",
    "    warmup_init=True\n",
    ")\n",
    "\n",
    "# Gradient accumulation setup\n",
    "gradient_accumulation_steps = 32\n",
    "model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
    "\n",
    "# Use mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Training loop with optimizations\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            loss = model(batch) / gradient_accumulation_steps\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "        if (i + 1) % gradient_accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d90b05c-a9ea-4752-93f4-b506585320b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e0145-7207-4455-ba42-300fd6ea2684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a171f5-1589-42f1-9ad7-9f961397cbb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08f8bfb-c696-4902-8635-35568dd464e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6276914b-f464-42b1-af8e-e61bb886c0da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fb224e-1f9b-4fe3-af54-69ce925cd285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164e931e-967d-4fbb-a1e2-e5ae9e255751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dcf2a-9729-4157-acf0-dfccb1527a31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbdbc2d-9137-4ad5-ac29-feaee19b5927",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514239b-74f7-4c4b-abc1-d741b25e235c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78294f6e-048c-4170-bc45-1991e3a88b74",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "X1 and X2 must have the same device type. X1: cuda X2: cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 105\u001b[0m\n\u001b[0;32m    103\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m get_embeddings(texts)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    104\u001b[0m output \u001b[38;5;241m=\u001b[39m model(embeddings)\n\u001b[1;32m--> 105\u001b[0m decoded_text \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_nearest_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoded output:\u001b[39m\u001b[38;5;124m\"\u001b[39m, decoded_text)\n",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m, in \u001b[0;36mdecode_nearest_embedding\u001b[1;34m(embeddings)\u001b[0m\n\u001b[0;32m     16\u001b[0m all_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mconvert_ids_to_tokens(\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mvocab_size)))\n\u001b[0;32m     17\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m get_embeddings(all_tokens)\n\u001b[1;32m---> 18\u001b[0m similarities \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m closest_indices \u001b[38;5;241m=\u001b[39m similarities\u001b[38;5;241m.\u001b[39margmin(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(idx) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m closest_indices]\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\torch\\functional.py:1478\u001b[0m, in \u001b[0;36mcdist\u001b[1;34m(x1, x2, p, compute_mode)\u001b[0m\n\u001b[0;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   1475\u001b[0m         cdist, (x1, x2), x1, x2, p\u001b[38;5;241m=\u001b[39mp, compute_mode\u001b[38;5;241m=\u001b[39mcompute_mode\n\u001b[0;32m   1476\u001b[0m     )\n\u001b[0;32m   1477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist_if_necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1479\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m compute_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_mm_for_euclid_dist\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mcdist(x1, x2, p, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: X1 and X2 must have the same device type. X1: cuda X2: cpu"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load LLaMA tokenizer and MiniLM embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    return torch.tensor(embedding_model.encode(texts), dtype=torch.float32)\n",
    "\n",
    "def decode_nearest_embedding(embeddings):\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(list(range(tokenizer.vocab_size)))\n",
    "    all_embeddings = get_embeddings(all_tokens)\n",
    "    similarities = torch.cdist(embeddings, all_embeddings)\n",
    "    closest_indices = similarities.argmin(dim=-1)\n",
    "    return [tokenizer.decode(idx) for idx in closest_indices]\n",
    "\n",
    "# Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
    "\n",
    "# Multi-Head Self-Attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.qkv_proj(x).reshape(B, T, 3, self.num_heads, self.d_k).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn_weights = (q @ k.transpose(-2, -1)) / (self.d_k ** 0.5)\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_output = (attn_weights @ v).transpose(1, 2).reshape(B, T, C)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "# Feed-Forward Network\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(F.relu(self.fc1(x)))\n",
    "\n",
    "# Transformer Block\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ff = FeedForward(d_model, hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "# Full Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, hidden_dim, num_layers, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        self.layers = nn.ModuleList([TransformerBlock(d_model, num_heads, hidden_dim) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Example usage\n",
    "d_model = 384  # MiniLM embedding size\n",
    "num_heads = 8\n",
    "hidden_dim = 1536\n",
    "num_layers = 6\n",
    "max_len = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Transformer(d_model, num_heads, hidden_dim, num_layers, max_len).to(device)\n",
    "texts = [\"Hello, world!\", \"How are you?\"]\n",
    "embeddings = get_embeddings(texts).unsqueeze(0).to(device)\n",
    "output = model(embeddings)\n",
    "decoded_text = decode_nearest_embedding(output.squeeze(0))\n",
    "print(\"Decoded output:\", decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac716b7f-2225-4aba-9a43-3582018c5107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
