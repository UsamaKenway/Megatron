{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "863e5462-1f92-4989-a98a-0edec609e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting speech emotion recognition model training...\n",
      "Using device: cuda\n",
      "Train dataset: Dataset({\n",
      "    features: ['name', 'path', 'emotion', 'text'],\n",
      "    num_rows: 768\n",
      "})\n",
      "Evaluation dataset: Dataset({\n",
      "    features: ['name', 'path', 'emotion', 'text'],\n",
      "    num_rows: 192\n",
      "})\n",
      "Classification problem with 5 classes: ['anger', 'disgust', 'fear', 'happiness', 'sadness']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a01734b7ea42588c6bb0857457a670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/262 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d275f235dbc1429f9750a8f9d9281414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a742677b6aeb40df8aaf49f6db4ac26f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/300 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "188c459d5fe2436084cee6864abd0abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target sampling rate: 16000\n",
      "Preprocessing train dataset...\n",
      "Preprocessing evaluation dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSpeechClassification were not initialized from the model checkpoint at jonatasgrosman/wav2vec2-large-xlsr-53-english and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\usama\\AppData\\Local\\Temp\\ipykernel_2128\\299534398.py:411: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CTCTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CTCTrainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 07:33, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.473500</td>\n",
       "      <td>1.304830</td>\n",
       "      <td>0.468750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.981000</td>\n",
       "      <td>0.889456</td>\n",
       "      <td>0.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.640100</td>\n",
       "      <td>0.729066</td>\n",
       "      <td>0.729167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.603500</td>\n",
       "      <td>0.369323</td>\n",
       "      <td>0.911458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.102800</td>\n",
       "      <td>0.315033</td>\n",
       "      <td>0.921875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.800200</td>\n",
       "      <td>0.301259</td>\n",
       "      <td>0.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.548600</td>\n",
       "      <td>0.248176</td>\n",
       "      <td>0.942708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Training metrics: {'train_runtime': 457.1167, 'train_samples_per_second': 13.441, 'train_steps_per_second': 0.333, 'total_flos': 7.734316525162577e+17, 'train_loss': 2.384810395538807, 'epoch': 7.623376623376624}\n",
      "Evaluating model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics: {'eval_loss': 0.26280614733695984, 'eval_accuracy': 0.9427083134651184, 'eval_runtime': 10.5812, 'eval_samples_per_second': 18.145, 'eval_steps_per_second': 1.89, 'epoch': 7.623376623376624}\n",
      "Saving model to ./emotion_recognition_model\n",
      "Model training and saving completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Complete script for speech emotion recognition training\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union, Any, Tuple\n",
    "from torch import nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "from packaging import version\n",
    "import torchaudio\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoConfig,\n",
    "    EvalPrediction,\n",
    "    is_apex_available\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "# Check for APEX\n",
    "if is_apex_available():\n",
    "    from apex import amp\n",
    "\n",
    "# Check for native AMP\n",
    "if version.parse(torch.__version__) >= version.parse(\"1.6\"):\n",
    "    _is_native_amp_available = True\n",
    "    from torch.cuda.amp import autocast\n",
    "\n",
    "# Define SpeechClassifierOutput class\n",
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "# Define the classification head\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "# Define the model for speech classification\n",
    "class Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "\n",
    "# Data collator for padding\n",
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    \"\"\"\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        label_features = [feature[\"labels\"] for feature in features]\n",
    "\n",
    "        d_type = torch.long if isinstance(label_features[0], int) else torch.float\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch[\"labels\"] = torch.tensor(label_features, dtype=d_type)\n",
    "\n",
    "        return batch\n",
    "\n",
    "# # Custom trainer class\n",
    "# class CTCTrainer(Trainer):\n",
    "#     def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Perform a training step on a batch of inputs.\n",
    "#         \"\"\"\n",
    "#         model.train()\n",
    "#         inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "#         if self.use_amp:\n",
    "#             with autocast():\n",
    "#                 loss = self.compute_loss(model, inputs)\n",
    "#         else:\n",
    "#             loss = self.compute_loss(model, inputs)\n",
    "\n",
    "#         if self.args.gradient_accumulation_steps > 1:\n",
    "#             loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "#         if self.use_amp:\n",
    "#             self.scaler.scale(loss).backward()\n",
    "#         elif self.use_apex:\n",
    "#             with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "#                 scaled_loss.backward()\n",
    "#         elif self.deepspeed:\n",
    "#             self.deepspeed.backward(loss)\n",
    "#         else:\n",
    "#             loss.backward()\n",
    "\n",
    "#         return loss.detach()\n",
    "\n",
    "# class CTCTrainer(Trainer):\n",
    "#     def training_step(\n",
    "#         self, \n",
    "#         model: nn.Module, \n",
    "#         inputs: Dict[str, Union[torch.Tensor, Any]], \n",
    "#         num_items_in_batch: Optional[int] = None  # Add the new argument\n",
    "#     ) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Perform a training step on a batch of inputs.\n",
    "#         \"\"\"\n",
    "#         model.train()\n",
    "#         inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "#         if self.use_amp:\n",
    "#             with autocast():\n",
    "#                 loss = self.compute_loss(model, inputs)\n",
    "#         else:\n",
    "#             loss = self.compute_loss(model, inputs)\n",
    "\n",
    "#         if self.args.gradient_accumulation_steps > 1:\n",
    "#             loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "#         if self.use_amp:\n",
    "#             self.scaler.scale(loss).backward()\n",
    "#         elif self.use_apex:\n",
    "#             with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "#                 scaled_loss.backward()\n",
    "#         elif self.deepspeed:\n",
    "#             self.deepspeed.backward(loss)\n",
    "#         else:\n",
    "#             loss.backward()\n",
    "\n",
    "#         return loss.detach()\n",
    "class CTCTrainer(Trainer):\n",
    "    pass\n",
    "\n",
    "# Main training function\n",
    "def train_emotion_recognition_model():\n",
    "    print(\"Starting speech emotion recognition model training...\")\n",
    "    \n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    data_files = {\n",
    "        \"train\": \"dataset/train.csv\",\n",
    "        \"validation\": \"dataset/test.csv\",\n",
    "    }\n",
    "    \n",
    "    dataset = load_dataset(\"csv\", data_files=data_files, delimiter=\"\\t\")\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"validation\"]\n",
    "    \n",
    "    print(f\"Train dataset: {train_dataset}\")\n",
    "    print(f\"Evaluation dataset: {eval_dataset}\")\n",
    "    \n",
    "    # Define input and output columns\n",
    "    input_column = \"path\"\n",
    "    output_column = \"emotion\"\n",
    "    \n",
    "    # Get unique labels\n",
    "    label_list = train_dataset.unique(output_column)\n",
    "    label_list.sort()  # Sort for determinism\n",
    "    num_labels = len(label_list)\n",
    "    print(f\"Classification problem with {num_labels} classes: {label_list}\")\n",
    "    \n",
    "    # Setup model configuration\n",
    "    model_name_or_path = \"jonatasgrosman/wav2vec2-large-xlsr-53-english\"\n",
    "    pooling_mode = \"mean\"\n",
    "    \n",
    "    # Create config\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        num_labels=num_labels,\n",
    "        label2id={label: i for i, label in enumerate(label_list)},\n",
    "        id2label={i: label for i, label in enumerate(label_list)},\n",
    "        finetuning_task=\"wav2vec2_clf\",\n",
    "    )\n",
    "    setattr(config, 'pooling_mode', pooling_mode)\n",
    "    \n",
    "    # Create processor\n",
    "    processor = Wav2Vec2Processor.from_pretrained(model_name_or_path, force_download=True)\n",
    "    target_sampling_rate = processor.feature_extractor.sampling_rate\n",
    "    print(f\"Target sampling rate: {target_sampling_rate}\")\n",
    "    \n",
    "    # # Define preprocessing functions\n",
    "    # def speech_file_to_array_fn(path):\n",
    "    #     import torch\n",
    "    #     speech_array, sampling_rate = torchaudio.load(path)\n",
    "    #     resampler = torchaudio.transforms.Resample(sampling_rate, target_sampling_rate)\n",
    "    #     speech = resampler(speech_array).squeeze().numpy()\n",
    "    #     return speech\n",
    "    def speech_file_to_array_fn(path):\n",
    "        import torchaudio\n",
    "        import torch \n",
    "    \n",
    "        try:\n",
    "            speech_array, sampling_rate = torchaudio.load(path)\n",
    "    \n",
    "            # --- Convert to mono by averaging channels if necessary ---\n",
    "            if speech_array.shape[0] > 1:  # Check if number of channels > 1\n",
    "                print(f\"Warning: Converting stereo audio to mono: {path}\") # Optional: log this\n",
    "                speech_array = torch.mean(speech_array, dim=0, keepdim=True)\n",
    "            # --------------------------------------------------------\n",
    "    \n",
    "            # Ensure sampling rate matches target\n",
    "            if sampling_rate != target_sampling_rate:\n",
    "                 # Initialize resampler only if needed\n",
    "                resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=target_sampling_rate)\n",
    "                speech_array = resampler(speech_array) # Resample\n",
    "    \n",
    "            # Squeeze and convert to numpy\n",
    "            # Ensure it's float32 for consistency\n",
    "            speech = speech_array.squeeze().to(torch.float32).numpy()\n",
    "    \n",
    "            # Optional: Check for empty arrays after processing\n",
    "            if speech.size == 0:\n",
    "                print(f\"Warning: Empty audio array after processing: {path}\")\n",
    "                # Return a small non-empty array to avoid downstream errors,\n",
    "                # though this might impact training slightly.\n",
    "                # A better approach might be to filter these files beforehand.\n",
    "                return np.zeros(1, dtype=np.float32)\n",
    "    \n",
    "            return speech\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {path}: {e}\")\n",
    "            # Return a placeholder or raise the exception depending on desired behavior\n",
    "            # Returning a dummy array might be safer for .map() not to fail entirely\n",
    "            return np.zeros(1, dtype=np.float32) # Or handle more gracefully\n",
    "    \n",
    "    def label_to_id(label, label_list):\n",
    "        if len(label_list) > 0:\n",
    "            return label_list.index(label) if label in label_list else -1\n",
    "        return label\n",
    "    \n",
    "    def preprocess_function(examples):\n",
    "        speech_list = [speech_file_to_array_fn(path) for path in examples[input_column]]\n",
    "        target_list = [label_to_id(label, label_list) for label in examples[output_column]]\n",
    "        \n",
    "        result = processor(speech_list, sampling_rate=target_sampling_rate)\n",
    "        result[\"labels\"] = list(target_list)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    # Preprocess datasets\n",
    "    print(\"Preprocessing train dataset...\")\n",
    "    train_dataset = train_dataset.map(\n",
    "        preprocess_function,\n",
    "        batch_size=100,\n",
    "        batched=True,\n",
    "        num_proc=4\n",
    "    )\n",
    "    \n",
    "    print(\"Preprocessing evaluation dataset...\")\n",
    "    eval_dataset = eval_dataset.map(\n",
    "        preprocess_function,\n",
    "        batch_size=100,\n",
    "        batched=True,\n",
    "        num_proc=4\n",
    "    )\n",
    "    \n",
    "    # Create data collator\n",
    "    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "    \n",
    "    # Define compute metrics function\n",
    "    def compute_metrics(p: EvalPrediction):\n",
    "        preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "    \n",
    "    # Load pretrained model\n",
    "    model = Wav2Vec2ForSpeechClassification.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        config=config,\n",
    "    )\n",
    "    \n",
    "    # Freeze feature extractor\n",
    "    model.freeze_feature_extractor()\n",
    "    \n",
    "    # Define training arguments\n",
    "    output_dir = \"./emotion_recognition_model\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=10,\n",
    "        per_device_eval_batch_size=10,\n",
    "        gradient_accumulation_steps=4,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        num_train_epochs=8.0,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        # save_steps=100,\n",
    "        eval_steps=20,\n",
    "        logging_steps=10,\n",
    "        learning_rate=1e-4,\n",
    "        save_total_limit=2,\n",
    "        # dataloader_num_workers=2,\n",
    "        report_to=\"none\",  # Disable wandb or other reporting\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = CTCTrainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting model training...\")\n",
    "    train_result = trainer.train()\n",
    "    print(f\"Training completed. Training metrics: {train_result.metrics}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"Evaluating model...\")\n",
    "    eval_result = trainer.evaluate()\n",
    "    print(f\"Evaluation metrics: {eval_result}\")\n",
    "    \n",
    "    # Save the model\n",
    "    print(f\"Saving model to {output_dir}\")\n",
    "    model.save_pretrained(output_dir)\n",
    "    processor.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save training args and other configurations\n",
    "    with open(f\"{output_dir}/training_args.json\", \"w\") as f:\n",
    "        json.dump(training_args.to_dict(), f)\n",
    "    \n",
    "    # Save label mappings\n",
    "    with open(f\"{output_dir}/label_mappings.json\", \"w\") as f:\n",
    "        json.dump({\n",
    "            \"label_list\": label_list,\n",
    "            \"num_labels\": num_labels,\n",
    "            \"label2id\": {label: i for i, label in enumerate(label_list)},\n",
    "            \"id2label\": {i: label for i, label in enumerate(label_list)}\n",
    "        }, f)\n",
    "    \n",
    "    print(\"Model training and saving completed successfully!\")\n",
    "    return output_dir\n",
    "\n",
    "# Run the training function\n",
    "if __name__ == \"__main__\":\n",
    "    train_emotion_recognition_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e6b11d2-90c6-4fa5-92e5-e0e5063133c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in to Huggingface Hub...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your Huggingface token:  路路路路路路路路\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repository usamakenway/wav2vec2-large-xlsr-53-english-ravdess is ready\n",
      "Uploading model files from ./emotion_recognition_model to usamakenway/wav2vec2-large-xlsr-53-english-ravdess...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53f2828fcf4245fd8ed4028b13c1f5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.27G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been uploaded to https://huggingface.co/usamakenway/wav2vec2-large-xlsr-53-english-ravdess\n",
      "Creating model card...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid metadata in README.md.\n- \"model-index[0].results[0].metrics[0].type\" is required\n- \"model-index[0].results[0].metrics[1].type\" is required\n- \"model-index[0].results[0].metrics[2].type\" is required\n- \"model-index[0].results[0].metrics[3].type\" is required",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:406\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://huggingface.co/api/validate-yaml",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\huggingface_hub\\hf_api.py:9238\u001b[0m, in \u001b[0;36mHfApi._validate_yaml\u001b[1;34m(self, content, repo_type, token)\u001b[0m\n\u001b[0;32m   9237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 9238\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   9239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:460\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    457\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    458\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m     )\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(BadRequestError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m403\u001b[39m:\n",
      "\u001b[1;31mBadRequestError\u001b[0m: (Request ID: Root=1-67ea6271-54b74cc31895333a71db0863;ea4024fb-5263-4ee6-8e7e-ada870849f48)\n\nBad request:\n\"model-index[0].results[0].metrics[0].type\" is required\n\"model-index[0].results[0].metrics[1].type\" is required\n\"model-index[0].results[0].metrics[2].type\" is required\n\"model-index[0].results[0].metrics[3].type\" is required",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 162\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# To use this script in a notebook:\u001b[39;00m\n\u001b[0;32m    161\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./emotion_recognition_model\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Path to your trained model\u001b[39;00m\n\u001b[1;32m--> 162\u001b[0m \u001b[43mupload_model_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 149\u001b[0m, in \u001b[0;36mupload_model_to_hub\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mREADME.md\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    147\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(model_card)\n\u001b[1;32m--> 149\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_fileobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mREADME.md\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_in_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mREADME.md\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdd model card\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    155\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel card has been uploaded\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour model is now available at: https://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\huggingface_hub\\hf_api.py:1518\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[1;32m-> 1518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\huggingface_hub\\hf_api.py:4439\u001b[0m, in \u001b[0;36mHfApi.upload_file\u001b[1;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[0m\n\u001b[0;32m   4431\u001b[0m commit_message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4432\u001b[0m     commit_message \u001b[38;5;28;01mif\u001b[39;00m commit_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpload \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_in_repo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with huggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   4433\u001b[0m )\n\u001b[0;32m   4434\u001b[0m operation \u001b[38;5;241m=\u001b[39m CommitOperationAdd(\n\u001b[0;32m   4435\u001b[0m     path_or_fileobj\u001b[38;5;241m=\u001b[39mpath_or_fileobj,\n\u001b[0;32m   4436\u001b[0m     path_in_repo\u001b[38;5;241m=\u001b[39mpath_in_repo,\n\u001b[0;32m   4437\u001b[0m )\n\u001b[1;32m-> 4439\u001b[0m commit_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_commit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4442\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43moperation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcommit_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_pr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_pr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_commit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_commit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4449\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_info\u001b[38;5;241m.\u001b[39mpr_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4452\u001b[0m     revision \u001b[38;5;241m=\u001b[39m quote(_parse_revision_from_pr_url(commit_info\u001b[38;5;241m.\u001b[39mpr_url), safe\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\huggingface_hub\\hf_api.py:1518\u001b[0m, in \u001b[0;36mfuture_compatible.<locals>._inner\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_as_future(fn, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;66;03m# Otherwise, call the function normally\u001b[39;00m\n\u001b[1;32m-> 1518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\huggingface_hub\\hf_api.py:3998\u001b[0m, in \u001b[0;36mHfApi.create_commit\u001b[1;34m(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\u001b[0m\n\u001b[0;32m   3996\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m addition\u001b[38;5;241m.\u001b[39mas_file() \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m   3997\u001b[0m     content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode()\n\u001b[1;32m-> 3998\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_yaml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3999\u001b[0m \u001b[38;5;66;03m# Skip other additions after `README.md` has been processed\u001b[39;00m\n\u001b[0;32m   4000\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python\\py311env\\Lib\\site-packages\\huggingface_hub\\hf_api.py:9242\u001b[0m, in \u001b[0;36mHfApi._validate_yaml\u001b[1;34m(self, content, repo_type, token)\u001b[0m\n\u001b[0;32m   9240\u001b[0m errors \u001b[38;5;241m=\u001b[39m response_content\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrors\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[0;32m   9241\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m error \u001b[38;5;129;01min\u001b[39;00m errors])\n\u001b[1;32m-> 9242\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid metadata in README.md.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid metadata in README.md.\n- \"model-index[0].results[0].metrics[0].type\" is required\n- \"model-index[0].results[0].metrics[1].type\" is required\n- \"model-index[0].results[0].metrics[2].type\" is required\n- \"model-index[0].results[0].metrics[3].type\" is required"
     ]
    }
   ],
   "source": [
    "# Code to upload your trained model to Huggingface Hub\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from getpass import getpass\n",
    "import os\n",
    "\n",
    "def upload_model_to_hub(model_path):\n",
    "    \"\"\"\n",
    "    Upload a trained model to the Huggingface Hub\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the saved model directory\n",
    "    \"\"\"\n",
    "\n",
    "    # Set repository name\n",
    "    model_name = \"wav2vec2-emotion-recognition-for-ravdness\"\n",
    "    repo_name = \"usamakenway/wav2vec2-large-xlsr-53-english-ravdess\"# \n",
    "    \n",
    "    # Login to Huggingface\n",
    "    print(\"Logging in to Huggingface Hub...\")\n",
    "    token = getpass(\"Enter your Huggingface token: \")\n",
    "    HfFolder.save_token(token)\n",
    "    api = HfApi()\n",
    "    \n",
    "    # Create repository if it doesn't exist\n",
    "    try:\n",
    "        api.create_repo(repo_id=repo_name, exist_ok=True)\n",
    "        print(f\"Repository {repo_name} is ready\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating repository: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Upload model files\n",
    "    print(f\"Uploading model files from {model_path} to {repo_name}...\")\n",
    "    api.upload_folder(\n",
    "        folder_path=model_path,\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Upload emotion recognition model\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Model has been uploaded to https://huggingface.co/{repo_name}\")\n",
    "    \n",
    "    # Extract label information for the model card\n",
    "    import json\n",
    "    try:\n",
    "        with open(os.path.join(model_path, \"label_mappings.json\"), \"r\") as f:\n",
    "            label_info = json.load(f)\n",
    "            label_list = label_info.get(\"label_list\", ['happiness', 'disgust', 'anger', 'fear', 'sadness'])\n",
    "    except:\n",
    "        label_list = ['happiness', 'disgust', 'anger', 'fear', 'sadness']\n",
    "    \n",
    "    # Create model card\n",
    "    print(\"Creating model card...\")\n",
    "    model_card = f\"\"\"---\n",
    "language: en\n",
    "license: apache-2.0\n",
    "tags:\n",
    "  - audio\n",
    "  - speech\n",
    "  - emotion-recognition\n",
    "  - wav2vec2\n",
    "datasets:\n",
    "  - RAVDESS\n",
    "model-index:\n",
    "  - name: {model_name}\n",
    "    results:\n",
    "      - task:\n",
    "          name: Speech Emotion Recognition\n",
    "          type: audio-classification\n",
    "        metrics:\n",
    "          - name: Training Accuracy\n",
    "            value: 0.9427  # Training accuracy from last epoch\n",
    "          - name: Validation Accuracy\n",
    "            value: 0.9427 \n",
    "          - name: Training Loss\n",
    "            value: 2.38  \n",
    "          - name: Validation Loss\n",
    "            value: 0.26  \n",
    "---\n",
    "\n",
    "# Speech Emotion Recognition Model\n",
    "\n",
    "This model is fine-tuned for speech emotion recognition. It can detect emotions such as happiness, sadness, anger, fear, disgust, etc. in speech.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- Model type: Fine-tuned Wav2Vec2\n",
    "- Base model: lighteternal/wav2vec2-large-xlsr-53-english\n",
    "- Training data: RAVDESS dataset\n",
    "- Supported emotions: {', '.join(label_list)}\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import Wav2Vec2Processor, AutoModelForAudioClassification\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "# Load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"{repo_name}\")\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"{repo_name}\")\n",
    "\n",
    "# Function to predict emotion from audio file\n",
    "def predict_emotion(audio_path):\n",
    "    # Load audio\n",
    "    speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "    \n",
    "    # Resample if needed\n",
    "    if sampling_rate != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(sampling_rate, 16000)\n",
    "        speech = resampler(speech_array).squeeze().numpy()\n",
    "    else:\n",
    "        speech = speech_array.squeeze().numpy()\n",
    "    \n",
    "    # Process audio\n",
    "    inputs = processor(speech, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_id = torch.argmax(logits, dim=1).item()\n",
    "        predicted_emotion = model.config.id2label[predicted_class_id]\n",
    "    \n",
    "    return predicted_emotion\n",
    "\n",
    "# Example usage\n",
    "emotion = predict_emotion(\"path/to/audio.wav\")\n",
    "print(f\"Detected emotion: sadness\")\n",
    "```\n",
    "\n",
    "## Training Details\n",
    "\n",
    "The model was trained for 8 epochs using the following parameters:\n",
    "- Learning rate: 1e-4\n",
    "- Batch size: 20\n",
    "- Gradient accumulation steps: 4\n",
    "\n",
    "## Limitations\n",
    "\n",
    "This model works best with clear speech recordings in quiet environments. Performance may vary with different accents, languages, or noisy backgrounds.\n",
    "\"\"\"\n",
    "\n",
    "    # Save and upload the model card\n",
    "    with open(\"README.md\", \"w\") as f:\n",
    "        f.write(model_card)\n",
    "        \n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"README.md\",\n",
    "        path_in_repo=\"README.md\",\n",
    "        repo_id=repo_name,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=\"Add model card\"\n",
    "    )\n",
    "    \n",
    "    print(\"Model card has been uploaded\")\n",
    "    print(f\"Your model is now available at: https://huggingface.co/{repo_name}\")\n",
    "\n",
    "# To use this script in a notebook:\n",
    "model_path = \"./emotion_recognition_model\"  # Path to your trained model\n",
    "upload_model_to_hub(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600580d-b287-49c5-81d7-a6144e120a0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
