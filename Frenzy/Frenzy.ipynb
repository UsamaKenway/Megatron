{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62234197-1c41-4553-94f5-1094bdc328af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Union\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import Wav2Vec2Processor\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "import torch\n",
    "from transformers.file_utils import ModelOutput\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "from transformers import AutoConfig, Wav2Vec2Processor\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e33fa4c-6f4b-4fe5-874e-c39021f4b867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2420d33cd5747aca97d263b70f10851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.93k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c0c2d29-86d5-4a62-bb20-7b713acf6b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SpeechClassifierOutput(ModelOutput):\n",
    "    loss: Optional[torch.FloatTensor] = None\n",
    "    logits: torch.FloatTensor = None\n",
    "    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n",
    "    attentions: Optional[Tuple[torch.FloatTensor]] = None\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
    "\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2Model\n",
    ")\n",
    "\n",
    "\n",
    "class Wav2Vec2ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for wav2vec classification task.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Frenzy(Wav2Vec2PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.pooling_mode = config.pooling_mode\n",
    "        self.config = config\n",
    "\n",
    "        self.wav2vec2 = Wav2Vec2Model(config)\n",
    "        self.classifier = Wav2Vec2ClassificationHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def freeze_feature_extractor(self):\n",
    "        self.wav2vec2.feature_extractor._freeze_parameters()\n",
    "\n",
    "    def merged_strategy(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            mode=\"mean\"\n",
    "    ):\n",
    "        if mode == \"mean\":\n",
    "            outputs = torch.mean(hidden_states, dim=1)\n",
    "        elif mode == \"sum\":\n",
    "            outputs = torch.sum(hidden_states, dim=1)\n",
    "        elif mode == \"max\":\n",
    "            outputs = torch.max(hidden_states, dim=1)[0]\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "            attention_mask=None,\n",
    "            output_attentions=None,\n",
    "            output_hidden_states=None,\n",
    "            return_dict=None,\n",
    "            labels=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        outputs = self.wav2vec2(\n",
    "            input_values,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SpeechClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3591cf44-33dd-4fbd-8698-c77b9f068acd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Frenzy(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): Wav2Vec2ClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Frenzy(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf784e3-86a7-4a3f-aee3-41ad488dae4f",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2758a2a-cfda-41b6-9612-53ab472f7975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized with 5 emotion classes\n",
      "Emotion to Index mapping: {'happiness': 0, 'anger': 1, 'disgust': 2, 'sadness': 3, 'fear': 4}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import librosa\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, \n",
    "    Wav2Vec2Model, \n",
    "    BertModel, \n",
    "    BertTokenizer\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Custom Dataset\n",
    "class MultimodalEmotionDataset(Dataset):\n",
    "    def __init__(self, csv_path, processor, tokenizer, max_length=128):\n",
    "        # Read the CSV\n",
    "        self.data = pd.read_csv(csv_path, sep='\\t')\n",
    "        \n",
    "        # Add random text column if not exists\n",
    "        if 'text' not in self.data.columns:\n",
    "            self.data['text'] = [f\"Random text for {name}\" for name in self.data['name']]\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Mapping emotions to indices\n",
    "        self.emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.data['emotion'].unique())}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Process Audio\n",
    "        speech_array, sampling_rate = torchaudio.load(row['path'])\n",
    "        speech_array = speech_array.squeeze().numpy()\n",
    "        speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, self.processor.feature_extractor.sampling_rate)\n",
    "        \n",
    "        audio_inputs = self.processor(\n",
    "            speech_array, \n",
    "            sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Process Text\n",
    "        text_inputs = self.tokenizer(\n",
    "            row['text'], \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get emotion label\n",
    "        label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "        return {\n",
    "            'audio_input': audio_inputs.input_values.squeeze(),\n",
    "            'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "            'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "            'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "# Multimodal Fusion Model\n",
    "class MultimodalEmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, audio_model_path, text_model_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Audio Encoder (Wav2Vec2)\n",
    "        self.audio_encoder = Wav2Vec2Model.from_pretrained(audio_model_path)\n",
    "        \n",
    "        # Text Encoder (BERT)\n",
    "        self.text_encoder = BertModel.from_pretrained(text_model_path)\n",
    "        \n",
    "        # Freeze pretrained encoders (optional)\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Fusion Layer\n",
    "        audio_feature_dim = self.audio_encoder.config.hidden_size\n",
    "        text_feature_dim = self.text_encoder.config.hidden_size\n",
    "        fusion_dim = audio_feature_dim + text_feature_dim\n",
    "        \n",
    "        self.fusion_layers = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, fusion_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(fusion_dim // 2, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "        # Extract audio features\n",
    "        audio_outputs = self.audio_encoder(\n",
    "            audio_input, \n",
    "            attention_mask=audio_mask\n",
    "        )\n",
    "        audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)\n",
    "        \n",
    "        # Extract text features\n",
    "        text_outputs = self.text_encoder(\n",
    "            text_input_ids, \n",
    "            attention_mask=text_attention_mask\n",
    "        )\n",
    "        text_features = text_outputs.pooler_output\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([audio_features, text_features], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.fusion_layers(combined_features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Training Setup\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Paths and Configurations\n",
    "    audio_model_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "    text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "    # Initialize Processors\n",
    "    audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "    text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "    \n",
    "    # Create Datasets\n",
    "    train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "    test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "    \n",
    "    # Get number of labels\n",
    "    num_labels = len(train_dataset.emotion_to_idx)\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model Initialized with {num_labels} emotion classes\")\n",
    "    print(\"Emotion to Index mapping:\", train_dataset.emotion_to_idx)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #main() \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Paths and Configurations\n",
    "    audio_model_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "    text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "    # Initialize Processors\n",
    "    audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "    text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "    \n",
    "    # Create Datasets\n",
    "    train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "    test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "    \n",
    "    # Get number of labels\n",
    "    num_labels = len(train_dataset.emotion_to_idx)\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model Initialized with {num_labels} emotion classes\")\n",
    "    print(\"Emotion to Index mapping:\", train_dataset.emotion_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbebea92-a587-48b5-9e40-fc3520cb77a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultimodalEmotionClassifier(\n",
       "  (audio_encoder): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1024, 1024, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (text_encoder): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fusion_layers): Sequential(\n",
       "    (0): Linear(in_features=1792, out_features=896, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=896, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18e105-dc98-4393-b5b7-45d42ad7c962",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f9e3db9-ab71-45dd-892d-7f006bf3525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "C:\\Python\\py311env\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Initialized with 5 emotion classes\n",
      "Emotion to Index mapping: {'happiness': 0, 'anger': 1, 'disgust': 2, 'sadness': 3, 'fear': 4}\n",
      "Epoch 1/100\n",
      "Train Loss: 1.2823, Train Accuracy: 0.5648\n",
      "Val Loss: 0.8227, Val Accuracy: 0.7526\n",
      "Saved new best model with validation accuracy: 0.7526\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.47      0.57        17\n",
      "           1       0.80      0.73      0.76        22\n",
      "           2       0.95      0.76      0.84        25\n",
      "           3       0.67      0.95      0.78        19\n",
      "           4       0.63      0.86      0.73        14\n",
      "\n",
      "    accuracy                           0.75        97\n",
      "   macro avg       0.76      0.75      0.74        97\n",
      "weighted avg       0.78      0.75      0.75        97\n",
      "\n",
      "Epoch 2/100\n",
      "Train Loss: 1.0334, Train Accuracy: 0.6399\n",
      "Val Loss: 0.7040, Val Accuracy: 0.7320\n",
      "Epoch 3/100\n",
      "Train Loss: 0.9594, Train Accuracy: 0.6373\n",
      "Val Loss: 0.6502, Val Accuracy: 0.7526\n",
      "Epoch 4/100\n",
      "Train Loss: 0.9132, Train Accuracy: 0.6580\n",
      "Val Loss: 0.7053, Val Accuracy: 0.7526\n",
      "Epoch 5/100\n",
      "Train Loss: 0.9302, Train Accuracy: 0.6451\n",
      "Val Loss: 0.6682, Val Accuracy: 0.7423\n",
      "Epoch 6/100\n",
      "Train Loss: 0.9071, Train Accuracy: 0.6477\n",
      "Val Loss: 0.6955, Val Accuracy: 0.7216\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.29      0.38        17\n",
      "           1       0.84      0.73      0.78        22\n",
      "           2       0.90      0.76      0.83        25\n",
      "           3       0.67      0.95      0.78        19\n",
      "           4       0.57      0.86      0.69        14\n",
      "\n",
      "    accuracy                           0.72        97\n",
      "   macro avg       0.71      0.72      0.69        97\n",
      "weighted avg       0.73      0.72      0.71        97\n",
      "\n",
      "Epoch 7/100\n",
      "Train Loss: 0.9053, Train Accuracy: 0.6373\n",
      "Val Loss: 0.7244, Val Accuracy: 0.7216\n",
      "Epoch 8/100\n",
      "Train Loss: 0.8919, Train Accuracy: 0.6373\n",
      "Val Loss: 0.7502, Val Accuracy: 0.7010\n",
      "Epoch 9/100\n",
      "Train Loss: 0.9242, Train Accuracy: 0.6606\n",
      "Val Loss: 0.7045, Val Accuracy: 0.7320\n",
      "Epoch 10/100\n",
      "Train Loss: 0.8298, Train Accuracy: 0.6839\n",
      "Val Loss: 0.6645, Val Accuracy: 0.7423\n",
      "Epoch 11/100\n",
      "Train Loss: 0.8818, Train Accuracy: 0.6451\n",
      "Val Loss: 0.6916, Val Accuracy: 0.7216\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.29      0.40        17\n",
      "           1       0.84      0.73      0.78        22\n",
      "           2       0.90      0.72      0.80        25\n",
      "           3       0.64      0.95      0.77        19\n",
      "           4       0.59      0.93      0.72        14\n",
      "\n",
      "    accuracy                           0.72        97\n",
      "   macro avg       0.72      0.72      0.69        97\n",
      "weighted avg       0.74      0.72      0.71        97\n",
      "\n",
      "Epoch 12/100\n",
      "Train Loss: 0.9038, Train Accuracy: 0.6865\n",
      "Val Loss: 0.6632, Val Accuracy: 0.7526\n",
      "Epoch 13/100\n",
      "Train Loss: 0.8728, Train Accuracy: 0.6710\n",
      "Val Loss: 0.6517, Val Accuracy: 0.7629\n",
      "Saved new best model with validation accuracy: 0.7629\n",
      "Epoch 14/100\n",
      "Train Loss: 0.8790, Train Accuracy: 0.6839\n",
      "Val Loss: 0.6505, Val Accuracy: 0.7526\n",
      "Epoch 15/100\n",
      "Train Loss: 0.8519, Train Accuracy: 0.6813\n",
      "Val Loss: 0.6628, Val Accuracy: 0.7423\n",
      "Epoch 16/100\n",
      "Train Loss: 0.8026, Train Accuracy: 0.6969\n",
      "Val Loss: 0.6563, Val Accuracy: 0.7629\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.53      0.58        17\n",
      "           1       1.00      0.68      0.81        22\n",
      "           2       0.90      0.76      0.83        25\n",
      "           3       0.63      1.00      0.78        19\n",
      "           4       0.71      0.86      0.77        14\n",
      "\n",
      "    accuracy                           0.76        97\n",
      "   macro avg       0.78      0.77      0.75        97\n",
      "weighted avg       0.80      0.76      0.76        97\n",
      "\n",
      "Epoch 17/100\n",
      "Train Loss: 0.8520, Train Accuracy: 0.6658\n",
      "Val Loss: 0.6583, Val Accuracy: 0.7526\n",
      "Epoch 18/100\n",
      "Train Loss: 0.9024, Train Accuracy: 0.6451\n",
      "Val Loss: 0.6594, Val Accuracy: 0.7629\n",
      "Epoch 19/100\n",
      "Train Loss: 0.7913, Train Accuracy: 0.6943\n",
      "Val Loss: 0.6590, Val Accuracy: 0.7732\n",
      "Saved new best model with validation accuracy: 0.7732\n",
      "Epoch 20/100\n",
      "Train Loss: 0.8551, Train Accuracy: 0.6684\n",
      "Val Loss: 0.6727, Val Accuracy: 0.7423\n",
      "Epoch 21/100\n",
      "Train Loss: 0.8599, Train Accuracy: 0.6736\n",
      "Val Loss: 0.7072, Val Accuracy: 0.7423\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.35      0.46        17\n",
      "           1       0.94      0.73      0.82        22\n",
      "           2       0.90      0.72      0.80        25\n",
      "           3       0.63      1.00      0.78        19\n",
      "           4       0.62      0.93      0.74        14\n",
      "\n",
      "    accuracy                           0.74        97\n",
      "   macro avg       0.75      0.75      0.72        97\n",
      "weighted avg       0.78      0.74      0.73        97\n",
      "\n",
      "Epoch 22/100\n",
      "Train Loss: 0.8327, Train Accuracy: 0.6839\n",
      "Val Loss: 0.6220, Val Accuracy: 0.7835\n",
      "Saved new best model with validation accuracy: 0.7835\n",
      "Epoch 23/100\n",
      "Train Loss: 0.8324, Train Accuracy: 0.6865\n",
      "Val Loss: 0.6937, Val Accuracy: 0.7629\n",
      "Epoch 24/100\n",
      "Train Loss: 0.8106, Train Accuracy: 0.6762\n",
      "Val Loss: 0.6515, Val Accuracy: 0.7629\n",
      "Epoch 25/100\n",
      "Train Loss: 0.7981, Train Accuracy: 0.7047\n",
      "Val Loss: 0.6643, Val Accuracy: 0.7732\n",
      "Epoch 26/100\n",
      "Train Loss: 0.8195, Train Accuracy: 0.6632\n",
      "Val Loss: 0.7056, Val Accuracy: 0.7423\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.47      0.55        17\n",
      "           1       0.89      0.73      0.80        22\n",
      "           2       0.94      0.68      0.79        25\n",
      "           3       0.61      1.00      0.76        19\n",
      "           4       0.67      0.86      0.75        14\n",
      "\n",
      "    accuracy                           0.74        97\n",
      "   macro avg       0.76      0.75      0.73        97\n",
      "weighted avg       0.78      0.74      0.74        97\n",
      "\n",
      "Epoch 27/100\n",
      "Train Loss: 0.8548, Train Accuracy: 0.6554\n",
      "Val Loss: 0.6069, Val Accuracy: 0.7526\n",
      "Epoch 28/100\n",
      "Train Loss: 0.9008, Train Accuracy: 0.6839\n",
      "Val Loss: 0.6573, Val Accuracy: 0.7526\n",
      "Epoch 29/100\n",
      "Train Loss: 0.8555, Train Accuracy: 0.6658\n",
      "Val Loss: 0.6648, Val Accuracy: 0.7526\n",
      "Epoch 30/100\n",
      "Train Loss: 0.8130, Train Accuracy: 0.6632\n",
      "Val Loss: 0.6075, Val Accuracy: 0.7526\n",
      "Epoch 31/100\n",
      "Train Loss: 0.8000, Train Accuracy: 0.6943\n",
      "Val Loss: 0.6486, Val Accuracy: 0.7629\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.53      0.60        17\n",
      "           1       0.94      0.73      0.82        22\n",
      "           2       0.95      0.72      0.82        25\n",
      "           3       0.63      1.00      0.78        19\n",
      "           4       0.67      0.86      0.75        14\n",
      "\n",
      "    accuracy                           0.76        97\n",
      "   macro avg       0.78      0.77      0.75        97\n",
      "weighted avg       0.80      0.76      0.76        97\n",
      "\n",
      "Epoch 32/100\n",
      "Train Loss: 0.7059, Train Accuracy: 0.7176\n",
      "Val Loss: 0.6505, Val Accuracy: 0.7629\n",
      "Epoch 33/100\n",
      "Train Loss: 0.7973, Train Accuracy: 0.7124\n",
      "Val Loss: 0.6764, Val Accuracy: 0.7629\n",
      "Epoch 34/100\n",
      "Train Loss: 0.7552, Train Accuracy: 0.6762\n",
      "Val Loss: 0.6878, Val Accuracy: 0.7629\n",
      "Epoch 35/100\n",
      "Train Loss: 0.8597, Train Accuracy: 0.6917\n",
      "Val Loss: 0.6448, Val Accuracy: 0.7732\n",
      "Epoch 36/100\n",
      "Train Loss: 0.8120, Train Accuracy: 0.6839\n",
      "Val Loss: 0.6578, Val Accuracy: 0.7526\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.53      0.58        17\n",
      "           1       1.00      0.68      0.81        22\n",
      "           2       0.90      0.72      0.80        25\n",
      "           3       0.61      1.00      0.76        19\n",
      "           4       0.71      0.86      0.77        14\n",
      "\n",
      "    accuracy                           0.75        97\n",
      "   macro avg       0.77      0.76      0.75        97\n",
      "weighted avg       0.79      0.75      0.75        97\n",
      "\n",
      "Epoch 37/100\n",
      "Train Loss: 0.7915, Train Accuracy: 0.6710\n",
      "Val Loss: 0.6932, Val Accuracy: 0.7629\n",
      "Epoch 38/100\n",
      "Train Loss: 0.8442, Train Accuracy: 0.6762\n",
      "Val Loss: 0.6466, Val Accuracy: 0.7526\n",
      "Epoch 39/100\n",
      "Train Loss: 0.7439, Train Accuracy: 0.7176\n",
      "Val Loss: 0.6983, Val Accuracy: 0.7629\n",
      "Epoch 40/100\n",
      "Train Loss: 0.7982, Train Accuracy: 0.6943\n",
      "Val Loss: 0.6949, Val Accuracy: 0.7732\n",
      "Epoch 41/100\n",
      "Train Loss: 0.7353, Train Accuracy: 0.7306\n",
      "Val Loss: 0.6354, Val Accuracy: 0.7526\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.71      0.71        17\n",
      "           1       1.00      0.68      0.81        22\n",
      "           2       0.95      0.72      0.82        25\n",
      "           3       0.56      1.00      0.72        19\n",
      "           4       0.75      0.64      0.69        14\n",
      "\n",
      "    accuracy                           0.75        97\n",
      "   macro avg       0.79      0.75      0.75        97\n",
      "weighted avg       0.81      0.75      0.76        97\n",
      "\n",
      "Epoch 42/100\n",
      "Train Loss: 0.7568, Train Accuracy: 0.7098\n",
      "Val Loss: 0.6632, Val Accuracy: 0.7835\n",
      "Epoch 43/100\n",
      "Train Loss: 0.8114, Train Accuracy: 0.7098\n",
      "Val Loss: 0.6980, Val Accuracy: 0.7526\n",
      "Epoch 44/100\n",
      "Train Loss: 0.7491, Train Accuracy: 0.7228\n",
      "Val Loss: 0.6206, Val Accuracy: 0.7732\n",
      "Epoch 45/100\n",
      "Train Loss: 0.7654, Train Accuracy: 0.6865\n",
      "Val Loss: 0.7046, Val Accuracy: 0.7423\n",
      "Epoch 46/100\n",
      "Train Loss: 0.8136, Train Accuracy: 0.6762\n",
      "Val Loss: 0.6785, Val Accuracy: 0.7835\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.59      0.65        17\n",
      "           1       1.00      0.77      0.87        22\n",
      "           2       0.95      0.72      0.82        25\n",
      "           3       0.66      1.00      0.79        19\n",
      "           4       0.67      0.86      0.75        14\n",
      "\n",
      "    accuracy                           0.78        97\n",
      "   macro avg       0.80      0.79      0.78        97\n",
      "weighted avg       0.82      0.78      0.78        97\n",
      "\n",
      "Epoch 47/100\n",
      "Train Loss: 0.7585, Train Accuracy: 0.6995\n",
      "Val Loss: 0.6689, Val Accuracy: 0.7423\n",
      "Epoch 48/100\n",
      "Train Loss: 0.7844, Train Accuracy: 0.6865\n",
      "Val Loss: 0.6683, Val Accuracy: 0.7732\n",
      "Epoch 49/100\n",
      "Train Loss: 0.7361, Train Accuracy: 0.7098\n",
      "Val Loss: 0.6976, Val Accuracy: 0.7526\n",
      "Epoch 50/100\n",
      "Train Loss: 0.8275, Train Accuracy: 0.6762\n",
      "Val Loss: 0.6377, Val Accuracy: 0.7629\n",
      "Epoch 51/100\n",
      "Train Loss: 0.7728, Train Accuracy: 0.7073\n",
      "Val Loss: 0.7042, Val Accuracy: 0.7423\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.59      0.61        17\n",
      "           1       1.00      0.64      0.78        22\n",
      "           2       0.95      0.72      0.82        25\n",
      "           3       0.59      1.00      0.75        19\n",
      "           4       0.69      0.79      0.73        14\n",
      "\n",
      "    accuracy                           0.74        97\n",
      "   macro avg       0.77      0.75      0.74        97\n",
      "weighted avg       0.80      0.74      0.75        97\n",
      "\n",
      "Epoch 52/100\n",
      "Train Loss: 0.7446, Train Accuracy: 0.7280\n",
      "Val Loss: 0.6283, Val Accuracy: 0.7732\n",
      "Epoch 53/100\n",
      "Train Loss: 0.7627, Train Accuracy: 0.7176\n",
      "Val Loss: 0.7326, Val Accuracy: 0.7423\n",
      "Epoch 54/100\n",
      "Train Loss: 0.8128, Train Accuracy: 0.6762\n",
      "Val Loss: 0.6547, Val Accuracy: 0.7629\n",
      "Epoch 55/100\n",
      "Train Loss: 0.8487, Train Accuracy: 0.6788\n",
      "Val Loss: 0.6124, Val Accuracy: 0.7938\n",
      "Saved new best model with validation accuracy: 0.7938\n",
      "Epoch 56/100\n",
      "Train Loss: 0.8142, Train Accuracy: 0.6788\n",
      "Val Loss: 0.6329, Val Accuracy: 0.7732\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69        17\n",
      "           1       1.00      0.68      0.81        22\n",
      "           2       0.95      0.72      0.82        25\n",
      "           3       0.61      1.00      0.76        19\n",
      "           4       0.79      0.79      0.79        14\n",
      "\n",
      "    accuracy                           0.77        97\n",
      "   macro avg       0.80      0.78      0.77        97\n",
      "weighted avg       0.82      0.77      0.78        97\n",
      "\n",
      "Epoch 57/100\n",
      "Train Loss: 0.7218, Train Accuracy: 0.7098\n",
      "Val Loss: 0.6700, Val Accuracy: 0.7526\n",
      "Epoch 58/100\n",
      "Train Loss: 0.7330, Train Accuracy: 0.6943\n",
      "Val Loss: 0.6272, Val Accuracy: 0.7629\n",
      "Epoch 59/100\n",
      "Train Loss: 0.8146, Train Accuracy: 0.6917\n",
      "Val Loss: 0.6633, Val Accuracy: 0.7423\n",
      "Epoch 60/100\n",
      "Train Loss: 0.7945, Train Accuracy: 0.7021\n",
      "Val Loss: 0.6858, Val Accuracy: 0.7526\n",
      "Epoch 61/100\n",
      "Train Loss: 0.7493, Train Accuracy: 0.7047\n",
      "Val Loss: 0.6226, Val Accuracy: 0.7629\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.59      0.62        17\n",
      "           1       1.00      0.73      0.84        22\n",
      "           2       0.95      0.72      0.82        25\n",
      "           3       0.61      1.00      0.76        19\n",
      "           4       0.69      0.79      0.73        14\n",
      "\n",
      "    accuracy                           0.76        97\n",
      "   macro avg       0.78      0.76      0.76        97\n",
      "weighted avg       0.81      0.76      0.77        97\n",
      "\n",
      "Epoch 62/100\n",
      "Train Loss: 0.7145, Train Accuracy: 0.7073\n",
      "Val Loss: 0.6317, Val Accuracy: 0.7423\n",
      "Epoch 63/100\n",
      "Train Loss: 0.7600, Train Accuracy: 0.7332\n",
      "Val Loss: 0.6594, Val Accuracy: 0.7629\n",
      "Epoch 64/100\n",
      "Train Loss: 0.7282, Train Accuracy: 0.6943\n",
      "Val Loss: 0.6114, Val Accuracy: 0.7629\n",
      "Epoch 65/100\n",
      "Train Loss: 0.8095, Train Accuracy: 0.6969\n",
      "Val Loss: 0.6025, Val Accuracy: 0.7732\n",
      "Epoch 66/100\n",
      "Train Loss: 0.7265, Train Accuracy: 0.6839\n",
      "Val Loss: 0.6051, Val Accuracy: 0.7732\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.59      0.62        17\n",
      "           1       1.00      0.73      0.84        22\n",
      "           2       0.95      0.72      0.82        25\n",
      "           3       0.63      1.00      0.78        19\n",
      "           4       0.71      0.86      0.77        14\n",
      "\n",
      "    accuracy                           0.77        97\n",
      "   macro avg       0.79      0.78      0.77        97\n",
      "weighted avg       0.81      0.77      0.78        97\n",
      "\n",
      "Epoch 67/100\n",
      "Train Loss: 0.6827, Train Accuracy: 0.7435\n",
      "Val Loss: 0.6348, Val Accuracy: 0.7629\n",
      "Epoch 68/100\n",
      "Train Loss: 0.7357, Train Accuracy: 0.7124\n",
      "Val Loss: 0.6177, Val Accuracy: 0.7526\n",
      "Epoch 69/100\n",
      "Train Loss: 0.6922, Train Accuracy: 0.7306\n",
      "Val Loss: 0.6039, Val Accuracy: 0.7938\n",
      "Epoch 70/100\n",
      "Train Loss: 0.6937, Train Accuracy: 0.7409\n",
      "Val Loss: 0.6397, Val Accuracy: 0.7629\n",
      "Epoch 71/100\n",
      "Train Loss: 0.8043, Train Accuracy: 0.6813\n",
      "Val Loss: 0.6155, Val Accuracy: 0.7526\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.65      0.67        17\n",
      "           1       1.00      0.73      0.84        22\n",
      "           2       0.90      0.72      0.80        25\n",
      "           3       0.58      1.00      0.73        19\n",
      "           4       0.75      0.64      0.69        14\n",
      "\n",
      "    accuracy                           0.75        97\n",
      "   macro avg       0.78      0.75      0.75        97\n",
      "weighted avg       0.80      0.75      0.76        97\n",
      "\n",
      "Epoch 72/100\n",
      "Train Loss: 0.7141, Train Accuracy: 0.7021\n",
      "Val Loss: 0.6766, Val Accuracy: 0.7423\n",
      "Epoch 73/100\n",
      "Train Loss: 0.7177, Train Accuracy: 0.7358\n",
      "Val Loss: 0.6190, Val Accuracy: 0.7629\n",
      "Epoch 74/100\n",
      "Train Loss: 0.7423, Train Accuracy: 0.7254\n",
      "Val Loss: 0.5576, Val Accuracy: 0.7732\n",
      "Epoch 75/100\n",
      "Train Loss: 0.7063, Train Accuracy: 0.7306\n",
      "Val Loss: 0.6272, Val Accuracy: 0.7629\n",
      "Epoch 76/100\n",
      "Train Loss: 0.6771, Train Accuracy: 0.7228\n",
      "Val Loss: 0.5915, Val Accuracy: 0.7938\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.65      0.69        17\n",
      "           1       1.00      0.77      0.87        22\n",
      "           2       0.95      0.72      0.82        25\n",
      "           3       0.66      1.00      0.79        19\n",
      "           4       0.71      0.86      0.77        14\n",
      "\n",
      "    accuracy                           0.79        97\n",
      "   macro avg       0.81      0.80      0.79        97\n",
      "weighted avg       0.83      0.79      0.80        97\n",
      "\n",
      "Epoch 77/100\n",
      "Train Loss: 0.6928, Train Accuracy: 0.7487\n",
      "Val Loss: 0.6149, Val Accuracy: 0.7526\n",
      "Epoch 78/100\n",
      "Train Loss: 0.7471, Train Accuracy: 0.7098\n",
      "Val Loss: 0.6670, Val Accuracy: 0.7526\n",
      "Epoch 79/100\n",
      "Train Loss: 0.6879, Train Accuracy: 0.7306\n",
      "Val Loss: 0.6260, Val Accuracy: 0.7526\n",
      "Epoch 80/100\n",
      "Train Loss: 0.7099, Train Accuracy: 0.7254\n",
      "Val Loss: 0.6277, Val Accuracy: 0.7320\n",
      "Epoch 81/100\n",
      "Train Loss: 0.7441, Train Accuracy: 0.7073\n",
      "Val Loss: 0.5820, Val Accuracy: 0.7732\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.59      0.62        17\n",
      "           1       1.00      0.73      0.84        22\n",
      "           2       0.90      0.72      0.80        25\n",
      "           3       0.63      1.00      0.78        19\n",
      "           4       0.75      0.86      0.80        14\n",
      "\n",
      "    accuracy                           0.77        97\n",
      "   macro avg       0.79      0.78      0.77        97\n",
      "weighted avg       0.81      0.77      0.77        97\n",
      "\n",
      "Epoch 82/100\n",
      "Train Loss: 0.7052, Train Accuracy: 0.7306\n",
      "Val Loss: 0.6072, Val Accuracy: 0.7629\n",
      "Epoch 83/100\n",
      "Train Loss: 0.7212, Train Accuracy: 0.6969\n",
      "Val Loss: 0.6177, Val Accuracy: 0.7732\n",
      "Epoch 84/100\n",
      "Train Loss: 0.7129, Train Accuracy: 0.7202\n",
      "Val Loss: 0.6724, Val Accuracy: 0.7423\n",
      "Epoch 85/100\n",
      "Train Loss: 0.6556, Train Accuracy: 0.7254\n",
      "Val Loss: 0.6185, Val Accuracy: 0.7629\n",
      "Epoch 86/100\n",
      "Train Loss: 0.7197, Train Accuracy: 0.7176\n",
      "Val Loss: 0.6115, Val Accuracy: 0.7629\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.53      0.58        17\n",
      "           1       1.00      0.73      0.84        22\n",
      "           2       0.90      0.72      0.80        25\n",
      "           3       0.63      1.00      0.78        19\n",
      "           4       0.71      0.86      0.77        14\n",
      "\n",
      "    accuracy                           0.76        97\n",
      "   macro avg       0.78      0.77      0.75        97\n",
      "weighted avg       0.80      0.76      0.76        97\n",
      "\n",
      "Epoch 87/100\n",
      "Train Loss: 0.7500, Train Accuracy: 0.7228\n",
      "Val Loss: 0.6388, Val Accuracy: 0.7732\n",
      "Epoch 88/100\n",
      "Train Loss: 0.6610, Train Accuracy: 0.7435\n",
      "Val Loss: 0.6163, Val Accuracy: 0.7732\n",
      "Epoch 89/100\n",
      "Train Loss: 0.7146, Train Accuracy: 0.7461\n",
      "Val Loss: 0.6472, Val Accuracy: 0.7526\n",
      "Epoch 90/100\n",
      "Train Loss: 0.6946, Train Accuracy: 0.7073\n",
      "Val Loss: 0.5713, Val Accuracy: 0.7835\n",
      "Epoch 91/100\n",
      "Train Loss: 0.7389, Train Accuracy: 0.7098\n",
      "Val Loss: 0.6216, Val Accuracy: 0.7732\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.47      0.57        17\n",
      "           1       1.00      0.77      0.87        22\n",
      "           2       0.90      0.72      0.80        25\n",
      "           3       0.63      1.00      0.78        19\n",
      "           4       0.68      0.93      0.79        14\n",
      "\n",
      "    accuracy                           0.77        97\n",
      "   macro avg       0.79      0.78      0.76        97\n",
      "weighted avg       0.81      0.77      0.77        97\n",
      "\n",
      "Epoch 92/100\n",
      "Train Loss: 0.6195, Train Accuracy: 0.7383\n",
      "Val Loss: 0.6167, Val Accuracy: 0.7732\n",
      "Epoch 93/100\n",
      "Train Loss: 0.7525, Train Accuracy: 0.7124\n",
      "Val Loss: 0.6128, Val Accuracy: 0.7732\n",
      "Epoch 94/100\n",
      "Train Loss: 0.7125, Train Accuracy: 0.7280\n",
      "Val Loss: 0.6727, Val Accuracy: 0.7526\n",
      "Epoch 95/100\n",
      "Train Loss: 0.7016, Train Accuracy: 0.7461\n",
      "Val Loss: 0.5311, Val Accuracy: 0.7938\n",
      "Epoch 96/100\n",
      "Train Loss: 0.6514, Train Accuracy: 0.7332\n",
      "Val Loss: 0.6107, Val Accuracy: 0.7835\n",
      "\n",
      "Validation Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.71      0.69        17\n",
      "           1       1.00      0.68      0.81        22\n",
      "           2       0.95      0.72      0.82        25\n",
      "           3       0.63      1.00      0.78        19\n",
      "           4       0.80      0.86      0.83        14\n",
      "\n",
      "    accuracy                           0.78        97\n",
      "   macro avg       0.81      0.79      0.78        97\n",
      "weighted avg       0.83      0.78      0.79        97\n",
      "\n",
      "Epoch 97/100\n",
      "Train Loss: 0.6987, Train Accuracy: 0.7176\n",
      "Val Loss: 0.5958, Val Accuracy: 0.7526\n",
      "Epoch 98/100\n",
      "Train Loss: 0.6822, Train Accuracy: 0.7150\n",
      "Val Loss: 0.6064, Val Accuracy: 0.7835\n",
      "Epoch 99/100\n",
      "Train Loss: 0.6606, Train Accuracy: 0.7383\n",
      "Val Loss: 0.6359, Val Accuracy: 0.7423\n",
      "Epoch 100/100\n",
      "Train Loss: 0.6793, Train Accuracy: 0.7409\n",
      "Val Loss: 0.5732, Val Accuracy: 0.7938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\py311env\\Lib\\site-packages\\transformers\\configuration_utils.py:311: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   happiness       0.05      0.04      0.05        24\n",
      "       anger       0.00      0.00      0.00        25\n",
      "     disgust       0.00      0.00      0.00        24\n",
      "     sadness       0.07      0.12      0.09        24\n",
      "        fear       0.90      0.75      0.82        24\n",
      "\n",
      "    accuracy                           0.18       121\n",
      "   macro avg       0.20      0.18      0.19       121\n",
      "weighted avg       0.20      0.18      0.19       121\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import librosa\n",
    "import os\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, \n",
    "    Wav2Vec2Model, \n",
    "    BertModel, \n",
    "    BertTokenizer\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Custom Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultimodalEmotionDataset(Dataset):\n",
    "    def __init__(self, csv_path, processor, tokenizer, max_length=128, target_audio_length=16000):\n",
    "        # Read the CSV\n",
    "        self.data = pd.read_csv(csv_path, sep='\\t')\n",
    "        \n",
    "        # Add random text column if not exists\n",
    "        if 'text' not in self.data.columns:\n",
    "            self.data['text'] = [f\"Random text for {name}\" for name in self.data['name']]\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.target_audio_length = target_audio_length\n",
    "        \n",
    "        # Mapping emotions to indices\n",
    "        self.emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.data['emotion'].unique())}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        \n",
    "        # Process Audio\n",
    "        speech_array, sampling_rate = torchaudio.load(row['path'])\n",
    "        speech_array = speech_array.squeeze().numpy()\n",
    "        # speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, self.processor.feature_extractor.sampling_rate)\n",
    "        speech_array = librosa.resample(y=np.asarray(speech_array), orig_sr=sampling_rate, target_sr=self.processor.feature_extractor.sampling_rate)\n",
    "        # Pad or truncate audio\n",
    "        if len(speech_array) > self.target_audio_length:\n",
    "            speech_array = speech_array[:self.target_audio_length]\n",
    "        elif len(speech_array) < self.target_audio_length:\n",
    "            padding = self.target_audio_length - len(speech_array)\n",
    "            speech_array = np.pad(speech_array, (0, padding), mode='constant', constant_values=0)\n",
    "        \n",
    "        audio_inputs = self.processor(\n",
    "            speech_array, \n",
    "            sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Process Text\n",
    "        text_inputs = self.tokenizer(\n",
    "            row['text'], \n",
    "            max_length=self.max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get emotion label\n",
    "        label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "        return {\n",
    "            'audio_input': audio_inputs.input_values.squeeze(),\n",
    "            'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "            'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "            'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "            'label': label\n",
    "        }\n",
    "\n",
    "\n",
    "# Multimodal Fusion Model\n",
    "class MultimodalEmotionClassifier(nn.Module):\n",
    "    def __init__(self, num_labels, audio_model_path, text_model_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Audio Encoder (Wav2Vec2)\n",
    "        self.audio_encoder = Wav2Vec2Model.from_pretrained(audio_model_path)\n",
    "        \n",
    "        # Text Encoder (BERT)\n",
    "        self.text_encoder = BertModel.from_pretrained(text_model_path)\n",
    "        \n",
    "        # Freeze pretrained encoders (optional)\n",
    "        for param in self.audio_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.text_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Fusion Layer\n",
    "        audio_feature_dim = self.audio_encoder.config.hidden_size\n",
    "        text_feature_dim = self.text_encoder.config.hidden_size\n",
    "        fusion_dim = audio_feature_dim + text_feature_dim\n",
    "        \n",
    "        self.fusion_layers = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, fusion_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(fusion_dim // 2, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "        # Extract audio features\n",
    "        audio_outputs = self.audio_encoder(\n",
    "            audio_input, \n",
    "            attention_mask=audio_mask\n",
    "        )\n",
    "        audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)\n",
    "        \n",
    "        # Extract text features\n",
    "        text_outputs = self.text_encoder(\n",
    "            text_input_ids, \n",
    "            attention_mask=text_attention_mask\n",
    "        )\n",
    "        text_features = text_outputs.pooler_output\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat([audio_features, text_features], dim=1)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.fusion_layers(combined_features)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10, learning_rate=1e-4):\n",
    "    # Loss and Optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create models directory if it doesn't exist\n",
    "    os.makedirs('saved_models', exist_ok=True)\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        train_preds = []\n",
    "        train_true = []\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            # Move data to device\n",
    "            audio_input = batch['audio_input'].to(device)\n",
    "            audio_mask = batch['audio_mask'].to(device)\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                audio_input, \n",
    "                audio_mask, \n",
    "                text_input_ids, \n",
    "                text_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track training metrics\n",
    "            train_losses.append(loss.item())\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_preds.extend(preds.cpu().numpy())\n",
    "            train_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        val_preds = []\n",
    "        val_true = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                # Move data to device\n",
    "                audio_input = batch['audio_input'].to(device)\n",
    "                audio_mask = batch['audio_mask'].to(device)\n",
    "                text_input_ids = batch['text_input_ids'].to(device)\n",
    "                text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(\n",
    "                    audio_input, \n",
    "                    audio_mask, \n",
    "                    text_input_ids, \n",
    "                    text_attention_mask\n",
    "                )\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_losses.append(loss.item())\n",
    "                \n",
    "                # Track validation metrics\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_true.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Compute metrics\n",
    "        train_accuracy = accuracy_score(train_true, train_preds)\n",
    "        val_accuracy = accuracy_score(val_true, val_preds)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {np.mean(train_losses):.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "        print(f\"Val Loss: {np.mean(val_losses):.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_accuracy': best_val_accuracy,\n",
    "                'epoch': epoch\n",
    "            }, f'saved_models/best_multimodal_model.pth')\n",
    "            \n",
    "            print(f\"Saved new best model with validation accuracy: {best_val_accuracy:.4f}\")\n",
    "        \n",
    "        # Optional: Print classification report for validation set\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"\\nValidation Classification Report:\")\n",
    "            print(classification_report(val_true, val_preds))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Paths and Configurations\n",
    "    audio_model_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "    text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "    # Initialize Processors\n",
    "    audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "    text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "    \n",
    "    # Create Datasets\n",
    "    train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "    test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "    \n",
    "    # Split train into train and validation\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "        train_dataset, \n",
    "        [int(len(train_dataset)*0.8), len(train_dataset)-int(len(train_dataset)*0.8)]\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    # Get number of labels\n",
    "    num_labels = len(train_dataset.dataset.emotion_to_idx)\n",
    "    \n",
    "    # Initialize Model\n",
    "    model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model Initialized with {num_labels} emotion classes\")\n",
    "    print(\"Emotion to Index mapping:\", train_dataset.dataset.emotion_to_idx)\n",
    "    \n",
    "    # Train Model\n",
    "    trained_model = train_model(model, train_loader, val_loader, device, epochs=100)\n",
    "    \n",
    "    # Optional: Load and evaluate best saved model\n",
    "    best_model_path = 'saved_models/best_multimodal_model.pth'\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    \n",
    "    # Reinitialize model and load state dict\n",
    "    best_model = MultimodalEmotionClassifier(\n",
    "        num_labels=num_labels, \n",
    "        audio_model_path=audio_model_path, \n",
    "        text_model_path=text_model_path\n",
    "    ).to(device)\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Test model\n",
    "    best_model.eval()\n",
    "    test_preds = []\n",
    "    test_true = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            # Move data to device\n",
    "            audio_input = batch['audio_input'].to(device)\n",
    "            audio_mask = batch['audio_mask'].to(device)\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = best_model(\n",
    "                audio_input, \n",
    "                audio_mask, \n",
    "                text_input_ids, \n",
    "                text_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Track test metrics\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            test_preds.extend(preds.cpu().numpy())\n",
    "            test_true.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Print test classification report\n",
    "    print(\"\\nTest Classification Report:\")\n",
    "    print(classification_report(test_true, test_preds, \n",
    "        target_names=list(train_dataset.dataset.emotion_to_idx.keys())))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5542c8e4-68ae-4bdf-9357-0b8a163f08f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Paths and Configurations\u001b[39;00m\n\u001b[0;32m      4\u001b[0m audio_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths and Configurations\n",
    "audio_model_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "text_model_path = \"bert-base-uncased\"\n",
    "\n",
    "# Initialize Processors\n",
    "audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "\n",
    "# Create Datasets\n",
    "train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "\n",
    "# Split train into train and validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(\n",
    "    train_dataset, \n",
    "    [int(len(train_dataset)*0.8), len(train_dataset)-int(len(train_dataset)*0.8)]\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Get number of labels\n",
    "num_labels = len(train_dataset.dataset.emotion_to_idx)\n",
    "\n",
    "# Initialize Model\n",
    "model = MultimodalEmotionClassifier(\n",
    "    num_labels=num_labels, \n",
    "    audio_model_path=audio_model_path, \n",
    "    text_model_path=text_model_path\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model Initialized with {num_labels} emotion classes\")\n",
    "print(\"Emotion to Index mapping:\", train_dataset.dataset.emotion_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da4a7f16-dac9-41ec-b17e-f3cc47b034ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torchaudio\n",
    "# import librosa\n",
    "# from transformers import (\n",
    "#     Wav2Vec2Processor, \n",
    "#     Wav2Vec2Model, \n",
    "#     BertModel, \n",
    "#     BertTokenizer\n",
    "# )\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# # Custom Dataset (previous implementation remains the same)\n",
    "# class MultimodalEmotionDataset(Dataset):\n",
    "#     def __init__(self, csv_path, processor, tokenizer, max_length=128):\n",
    "#         # Read the CSV\n",
    "#         self.data = pd.read_csv(csv_path, sep='\\t')\n",
    "        \n",
    "#         # Add random text column if not exists\n",
    "#         if 'text' not in self.data.columns:\n",
    "#             self.data['text'] = [f\"Random text for {name}\" for name in self.data['name']]\n",
    "        \n",
    "#         self.processor = processor\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "        \n",
    "#         # Mapping emotions to indices\n",
    "#         self.emotion_to_idx = {emotion: idx for idx, emotion in enumerate(self.data['emotion'].unique())}\n",
    "#         self.idx_to_emotion = {idx: emotion for emotion, idx in self.emotion_to_idx.items()}\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.data.iloc[idx]\n",
    "        \n",
    "#         # Process Audio\n",
    "#         speech_array, sampling_rate = torchaudio.load(row['path'])\n",
    "#         speech_array = speech_array.squeeze().numpy()\n",
    "#         speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, self.processor.feature_extractor.sampling_rate)\n",
    "        \n",
    "#         audio_inputs = self.processor(\n",
    "#             speech_array, \n",
    "#             sampling_rate=self.processor.feature_extractor.sampling_rate, \n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "        \n",
    "#         # Process Text\n",
    "#         text_inputs = self.tokenizer(\n",
    "#             row['text'], \n",
    "#             max_length=self.max_length, \n",
    "#             padding='max_length', \n",
    "#             truncation=True, \n",
    "#             return_tensors=\"pt\"\n",
    "#         )\n",
    "        \n",
    "#         # Get emotion label\n",
    "#         label = self.emotion_to_idx[row['emotion']]\n",
    "        \n",
    "#         return {\n",
    "#             'audio_input': audio_inputs.input_values.squeeze(),\n",
    "#             'audio_mask': audio_inputs.attention_mask.squeeze(),\n",
    "#             'text_input_ids': text_inputs['input_ids'].squeeze(),\n",
    "#             'text_attention_mask': text_inputs['attention_mask'].squeeze(),\n",
    "#             'label': label\n",
    "#         }\n",
    "\n",
    "# # Multimodal Fusion Model (previous implementation remains the same)\n",
    "# class MultimodalEmotionClassifier(nn.Module):\n",
    "#     def __init__(self, num_labels, audio_model_path, text_model_path):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # Audio Encoder (Wav2Vec2)\n",
    "#         self.audio_encoder = Wav2Vec2Model.from_pretrained(audio_model_path)\n",
    "        \n",
    "#         # Text Encoder (BERT)\n",
    "#         self.text_encoder = BertModel.from_pretrained(text_model_path)\n",
    "        \n",
    "#         # Freeze pretrained encoders (optional)\n",
    "#         for param in self.audio_encoder.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         for param in self.text_encoder.parameters():\n",
    "#             param.requires_grad = False\n",
    "        \n",
    "#         # Fusion Layer\n",
    "#         audio_feature_dim = self.audio_encoder.config.hidden_size\n",
    "#         text_feature_dim = self.text_encoder.config.hidden_size\n",
    "#         fusion_dim = audio_feature_dim + text_feature_dim\n",
    "        \n",
    "#         self.fusion_layers = nn.Sequential(\n",
    "#             nn.Linear(fusion_dim, fusion_dim // 2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(fusion_dim // 2, num_labels)\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, audio_input, audio_mask, text_input_ids, text_attention_mask):\n",
    "#         # Extract audio features\n",
    "#         audio_outputs = self.audio_encoder(\n",
    "#             audio_input, \n",
    "#             attention_mask=audio_mask\n",
    "#         )\n",
    "#         audio_features = torch.mean(audio_outputs.last_hidden_state, dim=1)\n",
    "        \n",
    "#         # Extract text features\n",
    "#         text_outputs = self.text_encoder(\n",
    "#             text_input_ids, \n",
    "#             attention_mask=text_attention_mask\n",
    "#         )\n",
    "#         text_features = text_outputs.pooler_output\n",
    "        \n",
    "#         # Concatenate features\n",
    "#         combined_features = torch.cat([audio_features, text_features], dim=1)\n",
    "        \n",
    "#         # Classification\n",
    "#         logits = self.fusion_layers(combined_features)\n",
    "        \n",
    "#         return logits\n",
    "\n",
    "# # Training Function\n",
    "# def train_model(model, train_loader, val_loader, device, epochs=10, learning_rate=1e-3):\n",
    "#     # Loss and Optimizer\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(\n",
    "#         [\n",
    "#             {'params': model.fusion_layers.parameters(), 'lr': learning_rate},\n",
    "#             # Uncomment and adjust learning rates if unfreezing more layers\n",
    "#             # {'params': model.audio_encoder.parameters(), 'lr': learning_rate * 0.1},\n",
    "#             # {'params': model.text_encoder.parameters(), 'lr': learning_rate * 0.1}\n",
    "#         ]\n",
    "#     )\n",
    "    \n",
    "#     best_val_accuracy = 0\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "#         # Training Phase\n",
    "#         model.train()\n",
    "#         train_loss = 0\n",
    "#         train_correct = 0\n",
    "#         train_total = 0\n",
    "        \n",
    "#         for batch in train_loader:\n",
    "#             # Move data to device\n",
    "#             audio_input = batch['audio_input'].to(device)\n",
    "#             audio_mask = batch['audio_mask'].to(device)\n",
    "#             text_input_ids = batch['text_input_ids'].to(device)\n",
    "#             text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "#             labels = batch['label'].to(device)\n",
    "            \n",
    "#             # Zero the parameter gradients\n",
    "#             optimizer.zero_grad()\n",
    "            \n",
    "#             # Forward pass\n",
    "#             outputs = model(audio_input, audio_mask, text_input_ids, text_attention_mask)\n",
    "#             loss = criterion(outputs, labels)\n",
    "            \n",
    "#             # Backward pass and optimize\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             # Compute training metrics\n",
    "#             train_loss += loss.item()\n",
    "#             _, predicted = torch.max(outputs, 1)\n",
    "#             train_total += labels.size(0)\n",
    "#             train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "#         # Validation Phase\n",
    "#         model.eval()\n",
    "#         val_loss = 0\n",
    "#         val_correct = 0\n",
    "#         val_total = 0\n",
    "#         val_predictions = []\n",
    "#         val_true_labels = []\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 # Move data to device\n",
    "#                 audio_input = batch['audio_input'].to(device)\n",
    "#                 audio_mask = batch['audio_mask'].to(device)\n",
    "#                 text_input_ids = batch['text_input_ids'].to(device)\n",
    "#                 text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "#                 labels = batch['label'].to(device)\n",
    "                \n",
    "#                 # Forward pass\n",
    "#                 outputs = model(audio_input, audio_mask, text_input_ids, text_attention_mask)\n",
    "#                 loss = criterion(outputs, labels)\n",
    "                \n",
    "#                 # Compute validation metrics\n",
    "#                 val_loss += loss.item()\n",
    "#                 _, predicted = torch.max(outputs, 1)\n",
    "#                 val_total += labels.size(0)\n",
    "#                 val_correct += (predicted == labels).sum().item()\n",
    "                \n",
    "#                 val_predictions.extend(predicted.cpu().numpy())\n",
    "#                 val_true_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "#         # Print epoch summary\n",
    "#         train_accuracy = 100 * train_correct / train_total\n",
    "#         val_accuracy = 100 * val_correct / val_total\n",
    "        \n",
    "#         print(f'Epoch [{epoch+1}/{epochs}]')\n",
    "#         print(f'Train Loss: {train_loss/len(train_loader):.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "#         print(f'Val Loss: {val_loss/len(val_loader):.4f}, Val Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "#         # Classification Report\n",
    "#         print(\"\\nClassification Report:\")\n",
    "#         print(classification_report(\n",
    "#             val_true_labels, \n",
    "#             val_predictions, \n",
    "#             target_names=list(train_dataset.emotion_to_idx.keys())\n",
    "#         ))\n",
    "        \n",
    "#         # Save best model\n",
    "#         if val_accuracy > best_val_accuracy:\n",
    "#             best_val_accuracy = val_accuracy\n",
    "#             torch.save(model.state_dict(), 'best_multimodal_emotion_model.pth')\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Inference Function\n",
    "# def predict_emotion(model, audio_path, text, processor, tokenizer, device):\n",
    "#     model.eval()\n",
    "    \n",
    "#     # Process Audio\n",
    "#     speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "#     speech_array = speech_array.squeeze().numpy()\n",
    "#     speech_array = librosa.resample(np.asarray(speech_array), sampling_rate, processor.feature_extractor.sampling_rate)\n",
    "    \n",
    "#     audio_inputs = processor(\n",
    "#         speech_array, \n",
    "#         sampling_rate=processor.feature_extractor.sampling_rate, \n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "    \n",
    "#     # Process Text\n",
    "#     text_inputs = tokenizer(\n",
    "#         text, \n",
    "#         max_length=128, \n",
    "#         padding='max_length', \n",
    "#         truncation=True, \n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "    \n",
    "#     # Move to device\n",
    "#     audio_input = audio_inputs.input_values.to(device).squeeze()\n",
    "#     audio_mask = audio_inputs.attention_mask.to(device).squeeze()\n",
    "#     text_input_ids = text_inputs['input_ids'].to(device).squeeze()\n",
    "#     text_attention_mask = text_inputs['attention_mask'].to(device).squeeze()\n",
    "    \n",
    "#     # Predict\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(\n",
    "#             audio_input.unsqueeze(0), \n",
    "#             audio_mask.unsqueeze(0), \n",
    "#             text_input_ids.unsqueeze(0), \n",
    "#             text_attention_mask.unsqueeze(0)\n",
    "#         )\n",
    "#         probabilities = torch.softmax(outputs, dim=1)\n",
    "#         predicted_class = torch.argmax(probabilities, dim=1)\n",
    "    \n",
    "#     return {\n",
    "#         'emotion': train_dataset.idx_to_emotion[predicted_class.item()],\n",
    "#         'probabilities': {emotion: prob.item() for emotion, prob in zip(train_dataset.idx_to_emotion.values(), probabilities[0])}\n",
    "#     }\n",
    "\n",
    "# # Main Execution\n",
    "# def main():\n",
    "#     # Set device\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "#     # Paths and Configurations\n",
    "#     audio_model_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "#     text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "#     # Initialize Processors\n",
    "#     audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "#     text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "    \n",
    "#     # Create Datasets\n",
    "#     train_dataset = MultimodalEmotionDataset(\"dataset/train.csv\", audio_processor, text_tokenizer)\n",
    "#     test_dataset = MultimodalEmotionDataset(\"dataset/test.csv\", audio_processor, text_tokenizer)\n",
    "    \n",
    "#     # DataLoaders\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "#     val_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "#     # Get number of labels\n",
    "#     num_labels = len(train_dataset.emotion_to_idx)\n",
    "    \n",
    "#     # Initialize Model\n",
    "#     model = MultimodalEmotionClassifier(\n",
    "#         num_labels=num_labels, \n",
    "#         audio_model_path=audio_model_path, \n",
    "#         text_model_path=text_model_path\n",
    "#     ).to(device)\n",
    "    \n",
    "#     # Train Model\n",
    "#     trained_model = train_model(model, train_loader, val_loader, device)\n",
    "    \n",
    "#     # Example Inference\n",
    "#     # Assuming you have a sample audio file and want to predict its emotion\n",
    "#     sample_audio_path = test_dataset.data.iloc[0]['path']\n",
    "#     sample_text = \"Some random descriptive text\"\n",
    "    \n",
    "#     prediction = predict_emotion(\n",
    "#         trained_model, \n",
    "#         sample_audio_path, \n",
    "#         sample_text, \n",
    "#         audio_processor, \n",
    "#         text_tokenizer, \n",
    "#         device\n",
    "#     )\n",
    "    \n",
    "#     print(\"\\nSample Prediction:\")\n",
    "#     print(\"Predicted Emotion:\", prediction['emotion'])\n",
    "#     print(\"Emotion Probabilities:\")\n",
    "#     for emotion, prob in prediction['probabilities'].items():\n",
    "#         print(f\"{emotion}: {prob:.4f}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c906106-3719-41b3-bee3-0750928b04b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 150\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprobability\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 150\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 117\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m text_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Create inference object\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m inferencer \u001b[38;5;241m=\u001b[39m \u001b[43mMultimodalEmotionInference\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_model_path\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Example usage - Single prediction\u001b[39;00m\n\u001b[0;32m    124\u001b[0m audio_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrenzy/data/anger/a01 (2).wav\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[1;32mIn[10], line 18\u001b[0m, in \u001b[0;36mMultimodalEmotionInference.__init__\u001b[1;34m(self, model_path, audio_model_path, text_model_path)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load processors\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor, \n",
    "    BertTokenizer\n",
    ")\n",
    "\n",
    "class MultimodalEmotionInference:\n",
    "    def __init__(self, model_path, audio_model_path, text_model_path):\n",
    "        # Set device\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Load model\n",
    "        self.model = torch.load(model_path).to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Load processors\n",
    "        self.audio_processor = Wav2Vec2Processor.from_pretrained(audio_model_path)\n",
    "        self.text_tokenizer = BertTokenizer.from_pretrained(text_model_path)\n",
    "        \n",
    "        # Emotion mapping (reverse of what was used during training)\n",
    "        self.idx_to_emotion = {\n",
    "            0: 'happiness', \n",
    "            1: 'anger', \n",
    "            2: 'disgust', \n",
    "            3: 'sadness', \n",
    "            4: 'fear'\n",
    "        }\n",
    "    \n",
    "    def preprocess_audio(self, audio_path):\n",
    "        \"\"\"Preprocess audio file\"\"\"\n",
    "        speech_array, sampling_rate = torchaudio.load(audio_path)\n",
    "        speech_array = speech_array.squeeze().numpy()\n",
    "        speech_array = librosa.resample(\n",
    "            np.asarray(speech_array), \n",
    "            sampling_rate, \n",
    "            self.audio_processor.feature_extractor.sampling_rate\n",
    "        )\n",
    "        \n",
    "        audio_inputs = self.audio_processor(\n",
    "            speech_array, \n",
    "            sampling_rate=self.audio_processor.feature_extractor.sampling_rate, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return audio_inputs\n",
    "    \n",
    "    def preprocess_text(self, text, max_length=128):\n",
    "        \"\"\"Preprocess text input\"\"\"\n",
    "        text_inputs = self.text_tokenizer(\n",
    "            text, \n",
    "            max_length=max_length, \n",
    "            padding='max_length', \n",
    "            truncation=True, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return text_inputs\n",
    "    \n",
    "    def predict(self, audio_path, text):\n",
    "        \"\"\"Predict emotion for given audio and text\"\"\"\n",
    "        # Preprocess inputs\n",
    "        audio_inputs = self.preprocess_audio(audio_path)\n",
    "        text_inputs = self.preprocess_text(text)\n",
    "        \n",
    "        # Move to device\n",
    "        audio_input = audio_inputs.input_values.to(self.device)\n",
    "        audio_mask = audio_inputs.attention_mask.to(self.device)\n",
    "        text_input_ids = text_inputs['input_ids'].to(self.device)\n",
    "        text_attention_mask = text_inputs['attention_mask'].to(self.device)\n",
    "        \n",
    "        # Inference\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(\n",
    "                audio_input, \n",
    "                audio_mask, \n",
    "                text_input_ids, \n",
    "                text_attention_mask\n",
    "            )\n",
    "            \n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        # Prepare results\n",
    "        results = [\n",
    "            {\n",
    "                \"emotion\": self.idx_to_emotion[i], \n",
    "                \"probability\": float(prob)\n",
    "            } \n",
    "            for i, prob in enumerate(probs)\n",
    "        ]\n",
    "        \n",
    "        # Sort by probability in descending order\n",
    "        results.sort(key=lambda x: x['probability'], reverse=True)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def batch_predict(self, audio_paths, texts):\n",
    "        \"\"\"Batch prediction for multiple samples\"\"\"\n",
    "        results = []\n",
    "        for audio_path, text in zip(audio_paths, texts):\n",
    "            result = self.predict(audio_path, text)\n",
    "            results.append(result)\n",
    "        return results\n",
    "\n",
    "def main():\n",
    "    # Paths\n",
    "    model_path = \"saved_models/best_multimodal_model.pth\"  # Assuming you saved the model\n",
    "    audio_model_path = \"m3hrdadfi/wav2vec2-xlsr-greek-speech-emotion-recognition\"\n",
    "    text_model_path = \"bert-base-uncased\"\n",
    "    \n",
    "    # Create inference object\n",
    "    inferencer = MultimodalEmotionInference(\n",
    "        model_path, \n",
    "        audio_model_path, \n",
    "        text_model_path\n",
    "    )\n",
    "    \n",
    "    # Example usage - Single prediction\n",
    "    audio_path = \"Frenzy/data/anger/a01 (2).wav\"\n",
    "    text = \"hi im angry\"\n",
    "    \n",
    "    single_result = inferencer.predict(audio_path, text)\n",
    "    print(\"Single Prediction:\")\n",
    "    for pred in single_result:\n",
    "        print(f\"{pred['emotion']}: {pred['probability']:.4f}\")\n",
    "    \n",
    "    # Example usage - Batch prediction\n",
    "    audio_paths = [\n",
    "        \"Frenzy/data/anger/a01 (2).wav\", \n",
    "        \"Frenzy/data/anger/a01 (3).wav\"\n",
    "    ]\n",
    "    texts = [\n",
    "        \"im angry\", \n",
    "        \"im angry\"\n",
    "    ]\n",
    "    \n",
    "    batch_results = inferencer.batch_predict(audio_paths, texts)\n",
    "    print(\"\\nBatch Prediction:\")\n",
    "    for i, results in enumerate(batch_results):\n",
    "        print(f\"\\nAudio {i+1}:\")\n",
    "        for pred in results:\n",
    "            print(f\"{pred['emotion']}: {pred['probability']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff96e477-1be4-4792-907c-bc6698a4846d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
